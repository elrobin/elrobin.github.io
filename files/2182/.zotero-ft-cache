Skip to Main Content
Wiley Online Library
Log in / Register
Log In
E-Mail Address
Password

Forgotten Password?
Remember Me

    Register
    Institutional Login

Wiley Social Media Advertisement

    Home >
    Education >
    General & Introductory Education >
    European Journal of Education >
    Early View >
    Abstract

JOURNAL TOOLS

    Get New Content Alerts
    Get RSS feed
    Save to My Profile
    Get Sample Copy
    Recommend to Your Librarian

JOURNAL MENU

    Journal Home

FIND ISSUES

    Current Issue
    All Issues

FIND ARTICLES

    Early View

GET ACCESS

    Subscribe / Renew

FOR CONTRIBUTORS

    OnlineOpen
    Author Guidelines
    Submit an Article

ABOUT THIS JOURNAL

    Society Information
    News
    Overview
    Editorial Board
    Permissions
    Advertise
    Contact

SPECIAL FEATURES

    Guest Editor Guidelines
    Reasons to Read EJE
    Wiley Job Network
    Jobs

OnlineOpen Option Available Advertisement Advertisement

Original Article
You have full text access to this content
Where Are the Global Rankings Leading Us? An Analysis of Recent Methodological Changes and New Developments

    Andrejs Rauhvargers *

Article first published online: 27 JAN 2014

DOI: 10.1111/ejed.12066

© 2014 John Wiley & Sons Ltd

Issue
Cover image for Vol. 48 Issue 4
European Journal of Education

Early View (Online Version of Record published before inclusion in an issue)

Additional Information (Show All)

How to Cite Author Information Publication History
How to Cite

Rauhvargers, A. (2014), Where Are the Global Rankings Leading Us? An Analysis of Recent Methodological Changes and New Developments. European Journal of Education. doi: 10.1111/ejed.12066
Author Information

* Andrejs Rauhvargers, Professor, University of Latvia, Raina bulv.19, LV-1586 Riga, Latvia, andrejs.rauhvargers@lu.lv , http://www.lu.lv
Publication History

    Article first published online: 27 JAN 2014

SEARCH
Search Scope
Search String

    Advanced >
    Saved Searches >

SEARCH BY CITATION
Volume:
Issue:
Page:
ARTICLE TOOLS

    Get PDF (130K)
    Save to My Profile
    E-mail Link to this Article
    Export Citation for this Article
    Get Citation Alerts
    Request Permissions

Share |

    Abstract
    Article
    References
    Cited By

Get PDF (130K)
Keywords:

    university rankings;
    methodology changes;
    indicators;
    reputation survey;
    rankings' impact;
    university responses

Abstract

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

This article is based on the analysis of the changes in global university rankings and the new ‘products’ based on rankings data in the period since mid-2011. It is a summary and continuation of the European University Association (EUA)-commissioned report ‘Global University Rankings Their Impact, Report II’ which was launched in April 2013. It covers the changes in the ranking methodologies which have been the most visible in the CWTS Leiden Ranking and Webometrics and which have replaced some indicators with newly designed ones. Changes have been made in other rankings as well, but they are less visible. A new U21 ranking was launched in 2012. It is an attempt to rank national higher education systems rather than individual universities. New rankings by conventional ranking providers have demonstrated that in reputation rankings or reputation indicators the scores drop even more sharply than in the most élitist rankings and therefore can be used for even narrower groups of universities.

Several ranking providers have started their own data collections and combine ranking data with the data from the newly established data collections and use them for several multi-indicator classifications or profiling tools. QS has been most productive and has added not only classification and profiling tools, but has also launched a ranking of student cities, and ‘stars’ that universities can obtain.

Generally, the rankings’ impact is growing. Let us see where it will bring us.

At the same time, some rankings providers have changed language and explain the biases, flaws and misunderstandings created through misuse of rankings or using ranking indicators without proper knowledge.

New Developments in Rankings in the 2011–2013 Period

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

Since 2011 when EUA published its first survey of rankings, most have only slightly altered their methodology, while the CWTS Leiden Ranking (further CWTS) and the Webometrics Ranking of World Universities entirely replaced the indicators they used in 2012 and included a bibliometric indicator — the number of papers in the top 10% of cited papers (Rauhvargers, 2013 ). In 2013, both these rankings and the QS World University Ranking by subject made other changes in their methodology.

Several new rankings have come into being since 2011. For instance, the Shanghai Ranking Consultancy (SRC) also established a University Ranking in the Former Yugoslav Republic of Macedonia (FYROM) in 2011 and a Greater China Ranking in 2012.

In 2012, almost simultaneously, both QS (QS, 2012a ) and THE (THE, 2013a ) started new rankings of universities that were under 50 years of age. Despite widespread criticism of reputation indicators, THE started a reputation ranking in 2012 which continued in 2013, although the results clearly demonstrated that the score curve fell so steeply that THE could assign individual scores to only the first 50 universities (THE, 2013b ). Similarly, in 2012 and 2013, QS widened subject rankings, although reputation indicators predominated in those rankings even after the changes made in 2013. In the same year, QS started a new ranking of Best Student Cities in the World which uses only QS-ranked universities as the input source for indicators.
U21 Ranking of Higher Education Systems

In May 2012, the Universitas 21 (U21) group of 23 research universities published a ranking of higher education systems. It is a more thorough attempt to rank higher education systems rather than individual universities and is described in detail in the full project report (Williams et al ., 2012 ). In 2013, U21 (Williams et al ., 2013 ) ranking extended the number of higher education systems from 48 to 50 and made some methodology changes. Besides the overall ranking, the 22 indicators are also grouped into four sub-rankings (‘measures’) referring to: resources; weight (25%), environment (20%) (25% in 2012), connectivity (15%) (10% in 2012) and output (40%).

While the development of a systems' level ranking is an interesting new approach, there is room for methodological improvement. Thus, indicator E4.1 — the subdivision of higher education institutions into public, government-dependent private and independent private — says little about their true diversity. Indicator E4.2, which is derived from the World Economic Forum scores based on country responses regarding their higher education systems, may vary according to the national traditions or stereotypes which determine whether countries regard their higher education systems favourably or unfavourably (Milot, 2012 ). Use of this indicator for the purposes of global comparison is therefore questionable. As regards secondary use of SRC ARWU Ranking scores in the indicators, O4 and O5 strengthen the positions of large or rich countries with universities that are strong in medicine and natural sciences.
SCImago Research Group Institution Ranking

The SCImago ranking is particularly interesting because it covers over 3,000 universities as well as research. Unlike most global rankings, SCImago does not publish a league table, and thus does not apply weights to each indicator, which is required for an overall score. Instead, it publishes tables which position institutions with respect to their performance in a single indicator, giving their scores in relation to other indicators in separate table columns. In 2012, SCImago used seven indicators: output (O) — number of scientific papers published in scholarly journals; international collaboration (IC) — proportion of articles whose affiliations include more than one country address; normalised impact (NI) — normalised impact indicator values show the ratio between the average scientific impact of an institution and the world average impact of publications for the same period, document type and subject area; HIGH quality publications (Q1) — ratio of publications of an institution published in the journals ranked in the first quartile (25%) in the SCImago Journal Rank (SJR) indicator; specialisation index (SI) — calculated in the same way as the Gini index in economics, the value of the Gini index is between 0 and 1 and here, value 1 means that the institution has publications in one field only, i.e. it is specialised, and small index values mean that the institution is a comprehensive one; excellence rate (ER) — the indicator value is the proportion of an institution's journal publications included in the 10% most cited papers in the same scientific field; scientific leadership — the indicator value is the number of papers in which the corresponding author belongs to the institution ( www.scimagoir.com ). In 2013, SCImago added a new indicator: Excellence with Leadership (% EwL) — the number of documents in the Excellence rate in which the institution is the main contributor ( www.scimagoir.com ).

SCImago also provides a country ranking which uses the following indicators: total count of publications; count of citable documents; count of citations and self-citations; citations per document and h-index (ibid.). Country ranking users can customise rankings by narrowing the comparison to one of the world regions, subject areas and sub-categories. Using the same indicators as Country ranking, SCImago also offers Journal ranking. Allinall, Scimago has produced a series of useful tools for analysis ( www.scimagojr.com ).
University Ranking by Academic Performance (URAP)

URAP is a global ranking which is based on ISI bibliometric data. It uses the following indicators: count of articles published in previous year indexed by WoS (weight 21%); citations received in previous year (21%); various research papers — conference papers, reviews, letters, etc. (10%); sum of impact factors of journals — number of times the university has published articles in this journal (18%); and sum of the impact factors of the journals in which the cited articles are published ( www.urapcenter.org/2011/methodology.php?q=3 ). The main features of URAP are that it covers research only, it uses bibliometric data only and it is size-dependent because the indicators are calculated as absolute numbers. While it is less well-known than SRC ARWU, THE and QS, it is interesting because it publishes a list of 2,000 universities, while the above rankings cover a maximum of 700 universities.
The Shanghai Ranking Consultancy

The Shanghai Ranking Consultancy's (SRC) main product is the Academic Ranking of World Universities (ARWU). It is the most consolidated of the popular university-based global rankings and there have been no changes in its core methodology since 2010.

In addition to the data collected for the SRC World University Ranking, a Global Research Universities Project (GRUP) was launched in 2012. It serves as a benchmarking tool, an estimation tool and a ranking-by-indicator tool ( www.shanghairanking.com/grup/index.html ). The benchmarking tool allows users to view and compare statistics on all 40 indicators (Rauhvargers, 2013 ). Comparisons are made between the following groups of universities: ARWU Universities by Rank Range; ARWU Top 500 Universities by geographic location (e.g. US Top 500, Western Europe Top 500 etc.); ARWU Top 100 Universities by World Region (e.g. ARWU Asia and Oceania Top 100); ARWU Regional Best 20 Universities (e.g. ARWU East Asia Top 20) and National Leading Universities (e.g. Russell Universities in the UK; G10 Universities in Canada; best 10 French universities in ARWU).

In 2012, SRC became involved in two national rankings. The Macedonian University Rankings was released on 16 February 2012. It was funded by the Ministry of Education and Science of the Former Yugoslav Republic of Macedonia (FYROM) and carried out by ARWU. The FYROM authorities chose 19 indicators for the ranking, many of which use national or institutional data (SRC ARWU, 2012a ). These indicators seek to address teaching issues, including the much criticised staff/student ratio, as well as income per student, library expenditure and several nationally important issues. Research indicators include the number of articles in peer-reviewed journals and those indexed in the Thomson Reuters WoS database, doctorates awarded per academic staff member, and several forms of research funding. Service to society is measured using research funding from industry per academic staff member and patents per academic staff member. There is no information on how and whether ARWU monitors efforts to ensure the reliability of the data used.
The Greater China Ranking

The Greater China Ranking includes Mainland China, Taiwan, Hong Kong and Macau. Its purpose is to help students in Greater China select their universities, particularly if they are prepared to study at institutions in regions away from home (SRC ARWU, 2012b ). The selected universities are those that are willing and potentially able to position themselves internationally and authorised to recruit students from other administrative areas in Greater China (SRC ARWU, 2012b ). Indicators used for the Greater China Ranking are a 13-indicator subset from the 21 indicators formerly used for ARWU Rankings Lab.
Thomson Reuters

Thomson Reuters has been working on the Global Institutional Profiles Project (GPP) since 2009, using almost 100 indicators. Using the data collected, Thomson Reuters has developed several other services. It produces individual reports on élite institutions worldwide, combining Thomson Reuter's reputation survey results with self-submitted data on scholarly outputs, funding, faculty characteristics and other information incorporated in some 100 indicators (Thomson Reuters, 2012a , 2012b ). Thomson Reuters uses the GPP data to prepare profiling reports for individual universities based on 13 groups of six to seven indicators. All include research volume, research capacity and performance. Other indicators vary. Thomson Reuters now plans to use the GPP data for other services, such as preparing customised data sets for individual customer needs (Thomson Reuters, 2012c ). It is developing a platform that will combine different sets of key indicators with the results of reputation surveys and visualisation tools to identify the key strengths of institutions according to a wide variety of aspects and subjects.
National Taiwan University Ranking of Scientific Papers

The National Taiwan University (NTU) Ranking (formerly ‘HEEACT Taiwan Ranking of Scientific papers’) ranks performance in terms of the publications covering 500 universities worldwide. NTU draws data from SCI and SSCI. Since 2012, NTU Ranking also includes six fields: agriculture, clinical medicine, engineering, life sciences, natural sciences and social sciences. There are also 14 subject rankings: agricultural sciences, environment/ecology, plant and animal science, computer science, chemical engineering, electrical engineering, mechanical engineering, materials science, pharmacology/toxicology, chemistry, mathematics and physics. NTU publishes two versions of world rankings: the ‘original ranking’ where all eight indicators are absolute measures and a ‘reference ranking’ where four of the indicators are relative values, calculated per academic staff member and thus size-independent.

Compared to 2011, the NTU ranking in 2012 has given greater weights to indicators on publications and citations and accordingly cut the weight of the h-index.
Times Higher Education Ranking

In selecting universities for ranking, the Times Higher Education (THE) excludes the following: universities which do not teach undergraduates; highly specialised universities; and those that have published less than 1000 titles over a five-year period, and not less than 200 in any given year.

It is important to note that THE does not publish the scores of its 13 individual indicators. Only scores in the five following areas can be found: ‘Teaching — the learning environment’ consisting of five different indicators corresponding to 30% of the overall ranking score; ‘Research —volume, income and reputation’ — three indicators with a total weight of 30%; ‘Citations—research influence’ — one indicator, with yet a further weight of 30%; ‘Industry income — innovation’ — a weight of 2.5%; ‘International outlook — staff, students and research’ which corresponds to three indicators worth 7.5% in total. The constituent indicators in the ‘teaching’, ‘research’ and ‘international outlook’ categories are so different in nature that it would be more helpful to have separate scores for each. However, since 2010 when THE ended its collaboration with QS, only the overall scores of areas can be viewed.

The above narrows the usefulness of the rankings data and reduces the transparency of the ranking methodology — which should be qualified as non-compliance with the Berlin principles.

It is encouraging that THE draws attention to several negative impacts of rankings (Baty, 2012a , 2012b ). Warnings about biases or flaws caused by some indicators are included in the 2012 description of methodology (Baty, 2012c ), but this does not necessarily mean that the THE methodology will be modified because of criticisms. For instance, such a poor indicator as the staff/student ratio is still kept, research and academic reputation are still kept as two indicators, although the THE confesses that there is ‘greater confidence in respondents' ability to make accurate judgements regarding research quality’. Yet although the reputation ranking introduced in 2012 demonstrated that it was only possible to assign a score to the first 50 universities, the reputation ranking was repeated in 2013.The news for 2013 is that THE ranking is supplemented by a possibility to compare either UK or US universities according to their position in the THE world ranking, their average graduate salaries, their average entry tariff and their total number of students.
CWTS Leiden Ranking

The CWTS Leiden Ranking does not calculate an overall score, so no weights are applied to its indicators. Instead, universities are ranked according to the scores of one user-selected indicator, while the university scores of other indicators are also made visible. To qualify for the CWTS Leiden Ranking in 2013, universities had to have generated over 3,200 publications in the period 2007–2011, but citations were counted up to the end of 2012. The set of indicators greatly changed in 2012, but there are no changes in 2013.

The mean citation score indicator (MCS) is the average number of citations of the publications emanating from a given university (excluding self-citations). Essentially, this is the same indicator as the citations per publication (CPP) in 2010.

Mean normalised citation score indicator (MNCS): in the 2011–2012 edition, the MNCS indicator replaced the former indicator corresponding to the ‘field-normalised citations per publication’ (CPP/FCSm).

The proportion of top 10% publications indicator (PP top 10% ) was also included for the first time in 2011–2012. It shows the proportion of a university's publications output within the top 10% most frequently cited titles compared with other similar publications (Waltman et al ., 2012 , p. 8).

The 2010 ‘brute force’ indicator which was calculated as P*CPP/FCSm appears in 2011–2012 under the new name of TNCS indicator.
Collaboration Indicators

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

Besides the above impact indicators, CWTS launched a set of collaborations in 2012 which included four indicators: PP collab — proportion of a university's publications jointly authored with other institutions; PP collab_int — proportion of a university's publications jointly authored with other countries; MGCD — average geographical collaboration distance of a university's publications and PP >1000km — the proportion of long distance joint publications. In 2013, the latter was replaced by one on university-industry cooperation PP UI collab : proportion of collaborative publications with industry.

The CWTS Leiden Ranking has also developed: benchmark analyses are derived from the ranking, but provide much greater detail on the scholarly activities, impact and collaboration of the university. The trends analysis shows how the academic performance of a university has changed over time, while the performance analysis assesses performance with respect to academic disciplines, institutes, departments or research groups ( www.leidenranking.com/products.aspx ).
QS Rankings and Other Products

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

Quacqarelli-Symmonds (QS)

QS has developed a broad range of ranking products over the last few years. They include: QS World University Ranking, Ranking by Subject and Ranking by Faculty, QS Best Student Cities Ranking and QS top-50-under-50 Ranking, plus two products added to the QS rankings: the QS classification and QS stars.
The QS classification

In 2009, QS started a simple university classification using alphanumeric notation with a view to grouping institutions according to four criteria: Size that varies between ‘small’ (<5, 000 students) and ′extra-large′ (>30,000 students); Age is categorised between ‘new’ (<10 years old) and ′historic′ (>100 years of age); Subject Range goes from ‘fully comprehensive’ for universities with six faculties (arts and humanities, engineering and technology, life sciences, natural sciences, social sciences, medicine) to ‘specialised’ with one or two faculties; Research Intensity (number of publications) can vary between ‘very high’ and ‘low’ (QS, 2011a ). Category thresholds depend on both the size and subject range of the university. Since 2011, the classification data have been shown for each university, together with its score.
QS Stars

QS Stars is a rating of universities which are measured against 52 indicators covering four core criteria — teaching, employability, research and infrastructure; three advanced criteria — internationalisation, innovation and engagement; as well as specialist criteria (QS, 2011b , 2011c ). The outcome is that a university can be awarded from none to five stars and even ‘five stars plus’ (QS, 2012c ). The stars are posted alongside its score in the ranking table. QS Stars cost US$30,400 and are valid for three years (QS, 2011d ; QS, 2012d ). Despite the set fees, three universities in the UK, 16 in the US and 12 in Australia obtained stars without being audited. While universities are free to decide whether or not to take part in a QS Stars audit, when a good university does not appear to have stars, questions arise and university leaders are under pressure to take part in the QS Stars exercise.

It is also interesting to note that QS suddenly allows institutions that do not have a ranked subject but have had a subject accredited by an internationally recognised accreditation body to also highlight their specialist subject area (QS, 2012a ). It is, however, unclear what is meant by ‘internationally recognised accreditation body’ — does it mean the worldwide subject accreditation systems, e.g. ABET, or quality assurance bodies that are registered in the European Quality Assurance Register (EQAR)? Or both?
QS World University Rankings by Subject

According to QS ( 2012b ), 30 out of 52 subjects were ranked in 2013 (QS, 2013). In 2013, QS included another bibliometric indicator, the Hirsch — an index in which the value h is the publication count by a university which is cited no less than h times. Thus, in 2013, the set of indicators for QS subject rankings is: academic reputation, employer reputation, citations per paper and the H-index. In 2013, 2,858 universities were selected and 1,526 were ranked. The threshold to enter the ranking is the number of published papers in 2007–2011, which ranged from 190 in medicine to 10 in subjects such as accounting and finance and linguistics, and zero in English linguistics.

The weights applied vary depending on the subject which is being ranked. In the case of medicine, biology, earth sciences, environment sciences, material sciences, and pharmacy, the citation indicator has a weight of 25% (50% in 2012) and the H-index a weight of 25% in 2013 (no H-index in 2012). The employer reputation indicator has a weight of 10% and the academic reputation indicator a weight of 40%. At the other end of the spectrum, the introduction of the H-index reduced the weights of the reputation indicators. In 2013, the combined weight of the reputation indicators was 90% (for only two subjects (seven in 2012)). Thus, in 2013 the methodology was somewhat improved.

Comparisons between universities (QS, 2012b ) on a subject basis can be much more useful than global university league tables that try to encapsulate entire institutions in a single score. Furthermore, comparisons made within a single subject reduce the field bias caused by different publishing cultures and citation practices in different fields of research.

These rankings are strongly based on reputation surveys. The methodology used is not sufficiently transparent for users to repeat the calculations and various mathematical adjustments are made before the final score is reached. In relation to the academic reputation survey, QS admits that a university may occasionally be nominated as excellent and ranked in a subject in which it ‘neither operates programmes nor research’ (QS, 2011c , p. 11). While the measures taken by QS may help to eliminate inappropriate choices, they prevent academics from sometimes nominating universities which have programmes, but no capacity or strength in a given subject.
QS Best Student Cities Ranking

The QS Best Student Cities Ranking was launched in 2012. Cities are ranked according to the following four categories (O'Leary, 2012 ):

    The rankings category includes three indicators: number of QS-ranked institutions in the city; index calculated according to positions of QS-ranked institutions (Top 10, 10 points, Top 20, 9 points, Top 30, 8 points; Top 100, 7 points, a further 1 point less for each 100 positions); and Top indicator: score based on the position of the institution with the highest position.
    The student mix category has four indicators: number of students at QS-ranked institutions as a proportion of the city's population; number of international students attracted to the city and studying at QS-ranked institutions; number of international students as a proportion of all students studying at QS-ranked institutions in the city and a score is based on the Mercer Quality of Living Survey.
    The employer activity category includes two indicators: number of domestic employers who identified at least one institution in the city as producing excellent graduates; and number of foreign employers who identified at least one institution in the city as producing excellent graduates.
    Finally, the affordability category has three indicators: tuition fees; a score based on the Big Mac Index; Mercer cost of living index of over 200 items (housing, transport, food, clothing, household goods, entertainment, etc.) (ibid.).

It is certainly helpful to try to provide information that should improve international students' understanding of the cities in which their preferred universities are located. However, it still needs to be demonstrated that a city is a better student location if it has more QS-ranked universities. Half the indicators (six) of this ranking thus depend on whether universities are QS-ranked or not and on their positions in the QS ranking. Universities that are not in the QS ranking table are not considered, nor are their students (QS, 2012d ). It is worth noting that the overwhelming majority of students worldwide is enrolled in the some 97% of the universities that do not appear in the rankings but are still involved in student life, contribute to the community and to the environment of the cities in which they live, graduate and find jobs. It would make sense to also include these students in any serious student cities' ranking.
Webometrics

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

The Webometrics methodology was changed substantially in 2012 (Rauhvargers, 2013 ), but new changes have occurred in 2013 (Webometrics, 2013 ). As a result, Webometrics uses four indicators: impact — square root of the number of backlinks 1 plus the number of domains creating those backlinks from Majestic SEO, weight 50%; presence — number of web pages of the university domain from Google, weight 16.67% (20% in 2012); openness — number of papers in pdf, doc, docx, ppt format from Google Scholar (2008–12), 16.67% (15% in 2012); and finally the excellence indicator — number of papers belonging to the top 10% of cited papers (2004–11) from the SCImago database, weight 16.67% (15% in 2012).

The broader coverage of Webometrics to include over 20,000 higher education institutions allows nearly all higher education institutions worldwide to compare themselves with others. Apart from the addition of the ‘excellence’ indicator based on SCImago bibliometric data, all other indicators used by Webometrics are based on web analysis, and considerably less direct proxies than the indicators used by academic rankings. The focus of Webometrics thus remains on providing a rough indication of how an institution performs in comparison with others.

As the Webometrics team has noted (Aguillo et al ., 2008 ), it is strongly dependent on the functioning of global public search engines, the instabilities of which obliged Webometrics to make the substantial changes mentioned above in the indicators and their weights.
Observations on New Products and the Diversification of Services

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

These developments demonstrate that the providers are no longer engaged in rankings alone. Several have started data collection exercises, the scope of which goes far beyond the requirements of the original ranking, as is the case with the GRUP survey and the QS Stars audit. It allows them to offer new multi-indicator tools, profiling tools, or tailor-made benchmarking exercises.

The current trend is thus for providers to accumulate large amounts of peripheral data on universities. It is ironic that the data submitted by universities free of charge are often sold back to the universities in a processed form. Commenting on the Thomson Reuters GPP project, Kris Olds writes:

    Of course there is absolutely nothing wrong with providing services (for a charge) to enhance the management of universities, but would most universities (and their funding agencies) agree, from the start, to the establishment of a relationship where all data is provided for free to a centralized private authority headquartered in the US and UK, and then have this data both managed and monetized by the private authority? I'm not so sure (Olds, 2012 ).

Most Problems Persist

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

All the main global rankings pre-select universities according to top research universities: SRC ARWU starts its selection with universities that have Nobel Prize winners and highly cited researchers; the CWTS Leiden Ranking selects universities with at least 500 publications in the Web of Science (WoS) for five consecutive years, but deliberately excludes publications in the arts and humanities. NTU and QS Rankings and the Thomson Reuters GPP select institutions with the highest publications and citation counts. This pre-selection policy means that, even if a ranking uses teaching-related indicators, these indicators will show which of the research institutions are best in teaching rather than which universities are best in teaching among all universities.

Use of poor indicators also persists. Despite widespread criticism, reliance on reputation indicators is becoming more and more widespread. THE has started a reputation ranking and QS has continued to widen subject rankings in which reputation indicators predominate, and in some subjects they are the only ones used. This has occurred despite the strange results of THE Reputation ranking and the admission by QS that, in reputation surveys, universities can occasionally be nominated as excellent in subjects in which they neither offer programmes nor conduct research. Finally, despite the controversy surrounding staff/student ratio indicators, they are still widely used as a means of measuring teaching performance.

CWTS research has clearly demonstrated that publications in languages other than English are read by fewer researchers than those in English from the same universities (van Raan et al ., 2010; 2011 ). The result is that the non-English-language output of these universities has a lower citation impact and thus a lower position in the rankings. In order to solve the problem, a rational, not yet tried-out approach might be to count non-English-language publications in productivity indicators and exclude them from citation indicators.
Rankings Providers Become Somewhat Self-critical

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

Some ranking providers have recently started to warn about the controversial sides of rankings and their potential misuses. One example is provided by Phil Baty who is closely associated with THE ranking. He wrote:

    Those of us who rank need to work with governments and policy-makers to make sure that they are as aware of what rankings do not — and can never — capture, as much as what they can, and to encourage them to dig deeper than the composite scores that can mask real excellence in specific fields or areas of performance. […]Rankings can of course have a very useful role in allowing institutions […] to benchmark their performance, to help them plan their strategic direction. But [rankings] should inform decisions — never drive decisions (Baty, 2012 ) .

Such frankness is welcome, but the introduction of changes that would address these shortcomings would be more helpful.

In 2010, Thomson Reuters published critical statements regarding ranking methodologies resulting from their survey (Thomson Reuters, 2010), but also advised that bibliometric data should be processed and interpreted competently. Misinterpretation of data may have particularly adverse consequences in the cases of uninformed use of citation impact data, for example, and reliance on average citation data that mask huge differences in numbers counted over several years (Miyairi & Chang, 2012 ).

This growing trend among ranking providers to discuss openly the possible pitfalls of using their data is very welcome. It is all the more important, given the growing perception among policy makers, society at large and, in some world regions even higher education institutions, that rankings are the ultimate measurement of performance and quality in higher education. The growing willingness of providers to speak out is an encouraging first sign that progress may be possible.
The Impact of Rankings Keeps Growing

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

There is no doubt that the impact of rankings continues to grow. Rankings strongly influence the behaviour of universities, as their presence in ranking tables heightens their profile and reputation. Highly-ranked universities must invest enormous effort just to maintain their positions, and even more if they try to move up the ladder. The considerable attention paid to rankings also places increasing pressure on institutions that do not yet appear in league tables to make efforts to be included.

University rankings are potentially useful in helping students to choose an appropriate university, be it in their home country or abroad. However, fully exploiting this would require rankings to provide better explanations of what indicator scores actually mean. The use of a more ‘democratic indicator’ base for selecting universities would also be helpful, as this would mean that rankings would no longer be limited to the world's top research universities.

Rankings also help to encourage the collection and publication of reliable national data on higher education (Rauhvargers, 2011 ), as well as more informed policy making (IHEP, 2009 ).

Rankings have a strong impact on the management of higher education institutions. There are various examples of cases where the salary or positions of top university officials have been linked to their institution's position in rankings (Jaschik, 2007 ), or where better performance in the rankings is used to justify claims on resources (Espeland & Saunder, 2007 ; Hazelkorn, 2011 ).

As far as the system level is concerned, it has been observed that world-class institutions may be funded at the expense of institutions that pursue other national goals, with all the challenges that this represents for system-level development. There is a risk that they become more divided, segmented, and hierarchical, with the emergence of a second tier of more teaching-oriented universities (Chan, 2012 ). A move in this direction would mean that research would come to outweigh teaching activities and that there may also be an imbalance between academic fields (ibid.). Among the inherent dangers, it is of particular concern that, without specific policies and incentives to promote and protect institutional diversity, the premium placed on global research rankings may result in the development of more uniform and mainly vertically differentiated systems (van Vught & Ziegele, 2012 , p. 75). A strong warning is issued by Morphew & Swanson (Morphew & Swanson, 2011 ), ‘As institutions enter global competition for resources, they find themselves at the mercy of a cutthroat winner-takes-it-all campaign and the resulting inequalities can have devastating effects on academic institutions and their constituencies’.
Policy Implications of Rankings

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

Immigration

Since 2008, in the Netherlands, migrants who possess a degree from a higher education institution which is ranked in the top 200 in either the THE, the SRC ARWU or QS rankings (Netherlands Immigration and Naturalisation Office, 2012 , p. 1) have the privilege of obtaining the ‘highly-skilled migrant’ status. In fairness, it should be noted that the ranking-dependent requirement is only part of a broader scheme in which applicants go through a ‘Points Test’. In Denmark, obtaining the green card is ranking-dependent. Out of a total of 100 points for the educational level of applicants, up to 15 points may be awarded according to the ranking position of the university from which the applicant graduated ( www.nyidanmark.dk/en-us/coming_to_dk/work/greencard-scheme ).
Eligibility of Partner Institutions

In 2012, the University Grants Commission in India announced that foreign universities entering into bilateral programme agreements would have to be among the global top 500 in either the THE or SRC ARWU rankings (IBNLive, 2012 ; Olds & Robertson 2012 ). In 2011, Brazil started a scholarship programme ‘Science Without Borders’ in which 100,000 students would be able to go abroad. The intention appears to be to give preference for this ambitious programme to host institutions that are selected on the basis of success in the THE and QS rankings ( www.nature.com/news/2011/110804/full/news.2011.458.html ).
Recognition of Qualifications

On 25 April 2012, the government of the Russian Federation adopted Decision No. 389 which establishes an automatic recognition of qualifications issued by foreign HE institutions which are in the first 300 positions of the SRC's ARWU, QS and THE rankings. This recognition is questionable, given that ranking scores are based on research rather than on teaching performance and are very little influenced by activities in the arts, humanities or social sciences.
Mergers of Institutions

Mergers are planned or already under way in many European countries. Even where the purpose of institutional consolidation is not specifically to improve ranking positions, the growing importance of rankings and especially the debate on world-class universities have been important factors in national discussions. In Asia, and particularly in Japan, Taiwan, Singapore and Malaysia, rankings have nurtured a ‘collective anxiety’ among Asian countries about not being left behind. This has led these four countries to establish excellence schemes to support their top universities. Selected universities in all these countries except Singapore have been given extra funding to improve their research output and level of internationalisation, but to the detriment of the other higher education institutions and thus the higher education system at large (Chan, 2012 ; Yonezawa, 2012 ).
How Universities Respond to Rankings

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

University leaders and administrators are gaining experience by working with rankings, and this has been the subject of debate in many meetings and events held over the last few years. Some of the main points made by institutions engaged in these discussions are the following: universities establish an institutional policy for communicating with ranking providers and appoint a staff member who follows both trends in the ranking and the success of rival universities. Some hire well-paid ranking experts to work out strategies to improve ranking positions. A growing number of universities has started to use data from rankings for analysis, strategic planning and policy making (Yonezawa, 2012 ). One of the reasons for which universities report using ranking data is to establish comparisons with rival universities (Proulx, 2012 ; Hwung & Huey-Jen Su, 2012 ; Forslöw, 2012 ;). It is also a means of maintaining or improving a university's ranking position.
Main Conclusions

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

    There have been significant developments since the publication of the first EUA Report in 2011, including the emergence of a new venture, the Universitas 21 Ranking of National Higher Education Systems, methodological changes made in a number of existing rankings and, more importantly, a considerable diversification in the types of products offered by several rankings providers.
    Global university rankings continue to focus on the research function of the university and are still not able to do justice to research carried out in the arts, humanities and the social sciences. Moreover, even bibliometric indicators still have strong biases and flaws. The limitations of rankings remain most apparent in efforts to measure teaching performance.
    A welcome development is that the providers of the most popular global rankings have started to draw attention to the biases and flaws in the data underpinning rankings and thus to the dangers of abusing rankings.
    New multi-indicator tools for profiling, classifying or benchmarking higher education institutions offered by the rankings providers are proliferating. These increase the pressure on and the risk of overburdening universities, obliged to collect ever more data in order to maintain as high a profile as possible. The growing volume of information being gathered on universities, and the new ‘products’ on offer also strengthen both the influence of the ranking providers and their potential impact.
    Bibliometric indicators are being improved, with the progression from simple counts of papers and citations, and from field normalisation (CPP/FCSm) to mean normalisation (MNCS). This, in turn, shows that biases still remain, and that it is therefore safer to measure citation impact by using indicators measuring the proportion of articles in highly cited journals (Waltman et al ., 2012 ).
    At the same time, those improvements do not help those areas where researchers publish mainly in books.
    Rankings are beginning to impact on public policy making, as demonstrated by their influence in the development of immigration policies in some countries, in determining the choice of university partner institutions, or in recognising foreign qualifications. The attention paid to rankings is also reflected in discussions on university mergers in some countries.
    A growing number of universities have started to use data compiled from rankings for the purpose of benchmarking exercises that, in turn, feed institutional strategic planning.
    Rankings are here to stay. Even if academics are aware that the results of rankings are biased and cannot satisfactorily measure institutional quality, on a more pragmatic level they also recognise that an impressive position in the rankings can be a key factor in securing additional resources, recruiting more students and attracting strong partner institutions. Therefore those universities that are not represented in global rankings are tempted to calculate their likely scores in order to assess their chances of entering the rankings. Everyone should bear in mind that not all publication outputs consist of articles in journals, and many issues that are relevant to academic quality cannot be measured quantitatively.

Note

    1

    Backlink (also called internal link or inlink) is an external link to the website of a university thus revealing the extent to which the university is ‘interesting’ to others.

References

    Top of page
    Abstract
    New Developments in Rankings in the 2011–2013 Period
    Collaboration Indicators
    QS Rankings and Other Products
    Webometrics
    Observations on New Products and the Diversification of Services
    Most Problems Persist
    Rankings Providers Become Somewhat Self-critical
    The Impact of Rankings Keeps Growing
    Policy Implications of Rankings
    How Universities Respond to Rankings
    Main Conclusions
    References

    Aguillo, I. F. , Ortega, J. L. & Fernández, M. ( 2008 ) Webometric ranking of world universities: introduction, methodology, and future developments , Higher Education in Europe , 33 , pp. 234 – 244 .
        CrossRef
    Baty, P. ( 2012a ) Rankings without reason , Inside Higher Education , 31 May. www.insidehighered.com/views/2012/05/31/essay-danger-countries-setting-policy-based-university-rankings
    Baty, P. ( 2012b ) Rankings don't tell the whole story — Handle them with care , University World News Global Edition , Issue 227, 26 June. www.universityworldnews.com/article.php?story=20120626171938451
    Baty, P. ( 2012c ) The essential elements in our world-leading formula , Times Higher Education , 4 October. www.timeshighereducation.co.uk/world-university-rankings/2012-13/world-ranking/methodology
    Chan, S. ( 2012 ) Enhancing Global Competitiveness: University Ranking Movement in Asia , The Academic Rankings and Advancement of Higher Education: Lessons from Asia and Other Regions . Proceedings, IREG-6 Conference (Taipei, Taiwan, 19–20 April), Higher Education Evaluation and Accreditation Council of Taiwan, pp. 125 – 142 .
    Espeland, W. N. & Saunder, M. ( 2007 ) Rankings and reactivity: how public measures recreate social worlds , American Journal of Sociology , 113 , pp. 1 – 40 .
        CrossRef ,
        Web of Science® Times Cited: 134
    Forslöw, B. ( 2012 ), Rankings and competitive metrics from a university perspective . Presentation, NUAS Conference (Stockholm, Sweden, August 12–14, 2012 ) www.webforum.com/nuasconference/getfile.ashx?cid=331413&cc=3&refid=149
    Hazelkorn, E. ( 2011 ) Rankings and the Reshaping of Higher Education. The Battle for World-Class Excellence ( New York , Palgrave Macmillan)
        CrossRef ,
        Web of Science® Times Cited: 25
    Hwung, H. H. & Huey-Jen Su, J. ( 2012 ) How NCKU uses ranking as a benchmarking tool to promote its internal quality and international visibility: a case study , The Academic Rankings and Advancement of Higher Education: Lessons from Asia and Other Regions . Proceedings, IREG-6 Conference (Taipei, Taiwan, 19–20 April), Higher Education Evaluation and Accreditation Council of Taiwan, pp. 301 – 308 .
    IHEP ( 2009 ) Impact of College Rankings on Institutional Decision Making: Four Country Case Studies . (Institute for Higher Education Policy (IHEP)). www.ihep.org/publications/publications-detail.cfm?id=126
    IBNLive ( 2012 ) UGC to allow only top 500 foreign universities to enter India , IBN Live . http://content.ibnlive.in.com/article/03-Jun-2012india/ugc-to-allow-only-top-500-foreign-univs-in-india-263954-3.html
    Jaschik, S. ( 2007 ) The Mobile International Student , Inside Higher Ed , 10 October.
    Milot, B. ( 2012 ) University rankings and system benchmarking reach similar results , University World News , Issue No. 238, 9 September. http://test.universityworldnews.com/article.php?story=2012090414311572
    Miyairi, N. & Chang, H-W. ( 2012 ) Methods and Examples of Applying Data to Different Levels' Research Excellence, Proceedings , 2012 HEEACT pre-conference workshop ‘Using data in research excellence and institutional strategic plan’ , pp. 86 – 95
    Morphew, C. C. & Swanson, C. ( 2011 ) ‘ On the efficacy of raising your university's rankings , in: J. Shin , R. Toutkoushian & U. Teichler (Eds) University Rankings: theoretical basis, methodology and impacts on global higher education , pp. 185 – 199 ( Dordrecht, Heidelberg, London and New York ; Springer).
        CrossRef ,
        Web of Science®
    Netherlands Immigration and Naturalisation Office ( 2012 ) A highly educated migrant seeking employment. Netherlands Immigration and Naturalisation Office , pp. 1 .
    Olds, K. ( 2012 ) The Business Side of World University Rankings globalhighered.wordpress.com/2012/04/12/the-business-side-of-world-university-rankings/
    Olds, K. & Robertson, S. L. ( 2012 ) Towards a global common data set for world university rankers , Global Higher Ed Wordpress , 4 June.
    O'Leary, J. ( 2012 ) QS Best student cities: responding to global demand , Topuniversities . www.topuniversities.com/university-rankings-articles/qs-best-student-cities/qs-best-student-cities-responding-global-demand
    Proulx, R. ( 2012 ) Using World University Ranking Systems to Inform and Guide Strategic Policy and Decision Making How to Benchmark Rather Than Rank Performance with Application to Canadian Research Universities . The Academic Rankings and Advancement of Higher Education: Lessons from Asia and Other Regions , Proceedings, IREG-6 Conference (Taipei, Taiwan, 19–20 April), Higher Education Evaluation and Accreditation Council of Taiwan, pp. 187 – 209 .
    QS ( 2011a ) How do we classify institutions? www.iu.qs.com/university-rankings/qs-classifications/
    QS ( 2011b ) QS Stars Rating System: shining a light on excellence, recognizing diversity in higher education . http://qsiu.files.wordpress.com/2011/11/qs-stars_fall_11_foremail1.pdf
    QS ( 2011c ) World University Rankings. Subject Tables Methodology . http://image.guardian.co.uk/sys-files/Education/documents/2011/09/02/METHODOLOGY2011.pdf
    QS ( 2011d ), QS World University Rankings. Subject tables methodology , version 3. http://content.qs.com/wur/qs-world-university-rankings-by-subject-methodology.pdf
    QS ( 2012a ) QS University Rankings: Top 50 Under 50 . www.topuniversities.com/top-50-under-50
    QS ( 2012b ) QS Stars ratings system Shining a Light on Excellence. Recognizing Diversity in Higher Education . www.iu.qs.com/wp-content/uploads/2012/03/QS-Stars-Brochure_2012.pdf
    QS ( 2012c ) QS World University Rankings by Subject . www.iu.qs.com/university-rankings/subject-tables/
    QS ( 2012d ) What Makes a Top Student City? www.topuniversities.com/university-rankings-articles/qs-best-student-cities/what-makes-top-student-city
    QS ( 2013 ) QS World University Rankings by Subject 2013 www.topuniversities.com/subject-rankings
    Rauhvargers, A. ( 2011 ) Global University Rankings and their Impact ( Brussels , European University Association) www.eua.be/pubs/Global_University_Rankings_and_Their_Impact.pdf
        Web of Science®
    Rauhvargers, A. ( 2013 ) Global University Rankings and their Impact ( Brussels , European University Association). www.eua.be/pubs/Global_University_Rankings_and_Their_Impact.pdf
    SRC ARWU ( 2012a ) Macedonian HEIs Ranking . www.shanghairanking.com/Macedonian_HEIs_Ranking/index.html
    SRC ARWU ( 2012b ) Greater China University Ranking Methodology . www.shanghairanking.com/Greater_China_Ranking/Greater-China-ranking-Methodology-2011.html
    THE ( 2013a ) Times Higher Education 100 Under 50 . www.timeshighereducation.co.uk/world-university-rankings/2013/one-hundred-under-fifty
    THE ( 2013b ) Times Higher Education World Reputation Rankings . www.timeshighereducation.co.uk/world-university-rankings/2013/reputation-ranking/range/01-50
    Thomson Reuters ( 2010 ) Global opinion survey. New outlooks on institutional profiles . ip-science.thomsonreuters.com/m/pdfs/Global_Opinion_Survey.pdf
    Thomson Reuters ( 2012a ) Global Institutional Profiles Project . http://ip-science.thomsonreuters.com/globalprofilesproject/
    Thomson Reuters ( 2012b ) Institutional Profiles Indicator group descriptions . http://researchanalytics.thomsonreuters.com/researchanalytics/m/pdfs/ip_indicator_groups.pdf
    Thomson Reuters ( 2012c ) Global Institutional Profiles Project, Institutional Data Collection . http://ip-science.thomsonreuters.com/globalprofilesproject/gpp-datacollection/
    van Raan, T. , van Leeuwen, T. & Visser, M. ( 2011 ) Non-English papers decrease rankings , Nature , 469 , p. 34 .
        CrossRef ,
        CAS ,
        Web of Science® Times Cited: 10 ,
        ADS
    van Raan, T. , van Leeuwen, T. & Visser, M. ( 2010 ) Germany and France are wronged in citation-based ranking . www.cwts.nl/pdf/LanguageRanking22122010.pdf
    van Vught, F. & Ziegele, F. (Eds.) ( 2012 ) Multi-dimensional ranking: the design and development of U-Multiran Dordrecht, Heidelberg, London and New York ; Springer.
    Waltman, L. , Calero-Medina, C. , Kosten, J. , Noyons, N. C. , Tijssen, R. J. & Wouters, P. ( 2012 ) The Leiden Ranking 2011/2012: Data collection, indicators, and interpretation. http://arxiv.org/ftp/arxiv/papers/1202/1202.3941.pdf
    WEBOMETRICS ( 2013 ) Methodology . www.webometrics.info/en/Methodology
    Williams, R. , Rassenfosse, G. , Jensen, P. & Marginson, S. ( 2012 ) U21 Ranking of National Higher Education Systems . www.universitas21.com/RelatedFile/Download/279
    Williams, R. , Rassenfosse, G. , Jensen, P. & Marginson, S. ( 2013 ) U21 Ranking of National Higher Education Systems . www.universitas21.com/article/projects/details/152/u21-ranking-of-national-higher-education-systems
    Yonezawa, A. ( 2012 ) Rankings and Information on Japanese Universities . Proceedings, IREG-6 Conference (Taipei, Taiwan, 19–20 April) (Higher Education Evaluation and Accreditation Council of Taiwan, pp. 149 – 163 ).

Get PDF (130K)
More content like this
Find more content:

    like this article

Find more content written by:

    Andrejs Rauhvargers

    Publications
    Browse by Subject
    Resources

    About Us
    Help
    Contact Us
    Agents
    Advertisers
    Media
    Privacy
    Cookies
    Terms & Conditions
    Site Map

Copyright © 1999-2014 John Wiley & Sons, Inc. All Rights Reserved.

    About Wiley
    Wiley.com
    Wiley Job Network
    Wiley


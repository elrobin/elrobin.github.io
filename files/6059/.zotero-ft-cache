Assessing the citation impact of books: The role of Google Books, Google Scholar, and Scopus - Kousha - 2011 - Journal of the American Society for Information Science and Technology - Wiley Online Library

    Skip to Article Content
    Skip to Article Information

COVID-19 Impact: Information for print subscribers
By continuing to browse this site, you agree to its use of cookies as described in our Cookie Policy . ×
Access by Universidad De Granada
Association for Information Science &amp; Technology
Association for Information Science &amp; Technology
Access by Universidad De Granada
Search within

    Search term
    Advanced Search Citation Search
    Search term
    Advanced Search Citation Search
    Search term
    Advanced Search Citation Search

Login / Register

    Publications
        Journal of the Association for Information Science and Technology
        Proceedings of the Association for Information Science and Technology
        Bulletin of the Association for Information Science and Technology
        Annual Review of Information Science and Technology
    THESAURUS

Journal of the American Society for Information Science and Technology
Volume 62, Issue 11 Journal of the American Society for Information Science and Technology
Research Article
Full Access
Assessing the citation impact of books: The role of Google Books, Google Scholar, and Scopus
Kayvan Kousha

E-mail address: k.kousha@wlv.ac.uk

Statistical Cybermetrics Research Group, School of Technology, University of Wolverhampton, Wulfruna Street, Wolverhampton WV1 1LY, United Kingdom

Department of Library and Information Science, University of Tehran, Jalle‐Ahmad Highway, P.O. Box 14155–6456, Tehran, Iran
Search for more papers by this author
Mike Thelwall

E-mail address: m.thelwall@wlv.ac.uk

Statistical Cybermetrics Research Group, School of Technology, University of Wolverhampton, Wulfruna Street, Wolverhampton WV1 1LY, United Kingdom
Search for more papers by this author
Somayeh Rezaie

E-mail address: s_aries80@yahoo.com

Department of Library and Information Science, Shahid Beheshti University, Evin Street, Tehran, Iran
Search for more papers by this author
Kayvan Kousha

E-mail address: k.kousha@wlv.ac.uk

Statistical Cybermetrics Research Group, School of Technology, University of Wolverhampton, Wulfruna Street, Wolverhampton WV1 1LY, United Kingdom

Department of Library and Information Science, University of Tehran, Jalle‐Ahmad Highway, P.O. Box 14155–6456, Tehran, Iran
Search for more papers by this author
Mike Thelwall

E-mail address: m.thelwall@wlv.ac.uk

Statistical Cybermetrics Research Group, School of Technology, University of Wolverhampton, Wulfruna Street, Wolverhampton WV1 1LY, United Kingdom
Search for more papers by this author
Somayeh Rezaie

E-mail address: s_aries80@yahoo.com

Department of Library and Information Science, Shahid Beheshti University, Evin Street, Tehran, Iran
Search for more papers by this author
First published: 26 August 2011
https://doi.org/10.1002/asi.21608
Citations: 68
About

        Figures
        References
        Related
        Information
    PDF PDF

Sections

    Abstract
    Introduction
    Research Assessment: The United Kingdom
    Book Impact Assessment
    Research Questions
    Methods
    Results
    Further Analysis
    Conclusions
    Acknowledgements
    Appendix
    References
    Citing Literature

PDF PDF
Tools

    Request permission
    Export citation
    Add to favorites
    Track citation

Share Share

Give access
Share full text access
Close modal

Share full-text access
Please review our Terms and Conditions of Use and check box below to share full-text version of article.
I have read and accept the Wiley Online Library Terms and Conditions of Use
Shareable Link

Use the link below to share a full-text version of this article with your friends and colleagues. Learn more.
Copy URL

Share a link
Share on

    Email
    Facebook
    Twitter
    Linked In
    Reddit
    Wechat

Abstract

Citation indictors are increasingly used in some subject areas to support peer review in the evaluation of researchers and departments. Nevertheless, traditional journal‐based citation indexes may be inadequate for the citation impact assessment of book‐based disciplines. This article examines whether online citations from Google Books and Google Scholar can provide alternative sources of citation evidence. To investigate this, we compared the citation counts to 1,000 books submitted to the 2008 U.K. Research Assessment Exercise (RAE) from Google Books and Google Scholar with Scopus citations across seven book‐based disciplines (archaeology; law; politics and international studies; philosophy; sociology; history; and communication, cultural, and media studies). Google Books and Google Scholar citations to books were 1.4 and 3.2 times more common than were Scopus citations, and their medians were more than twice and three times as high as were Scopus median citations, respectively. This large number of citations is evidence that in book‐oriented disciplines in the social sciences, arts, and humanities, online book citations may be sufficiently numerous to support peer review for research evaluation, at least in the United Kingdom.
Introduction

Books and monographs are primary research outputs in the arts and humanities and in many social sciences (Glänzel & Schoepflin, 1999 ; Hicks, 2004 ; Huang & Chang, 2008 ; Nederhof, 2006 ), but it is difficult for subject experts to evaluate the quality of books on a large scale because books tend to be much longer than are journal articles. In the context of U.K. research evaluation, for example, Taylor and Walker ( 2009 ) argued that “Given the time constraints facing panel members, it is obvious that not all publications could be considered in detail, and certainly not by more than one panel member in the majority of cases”(p. 3). To support this, there were more than 14,000 monographs overall in the 2008 U.K. Research Assessment Exercise (RAE), 14 per reviewer, but in book‐oriented disciplines there were up to 100 books per reviewer (e.g., 1,665 monographs for history's 17 reviewers). While it could be argued that the selective reading of any individual text (which Taylor & Walker implied must occur) may be adequate for its overall quality assessment, this practice seems likely to increase the chance of errors and reviewer susceptibility to extraneous factors such as institutional reputation. Citation analysis also has been widely used for research evaluation, but has its own problems, errors, and biases (for an in‐depth review, see MacRoberts & MacRoberts, 1996 ). For instance, influential research can be uncited, and even types of influential research can remain uncited within a particular field (MacRoberts & MacRoberts, 2010 ). Consequently, it seems to be widely accepted in the field that bibliometric indicators should be used as supporting information for peer review rather than as a replacement for research‐quality assessment (e.g., van Raan, 2005 ; Warner, 2000 ; Weingart, 2005 ).

Is the problem of books significant when assessing national research outputs or is it only a minor issue? Our initial study of the United Kingdom showed that while 16.5% of the submissions to 67 units of assessment (UoAs) in the 2008 RAE were related to books (including monographs, edited books, and chapters), the proportion of book submissions in the 38 social sciences and arts & humanities subject areas was 31%. However, the percentage of book submissions varied from 1.3% in psychology to 68% in theology, divinity, and religious studies (Appendix Table A1 ). Furthermore, 12.4% of the submissions in the 38 social sciences and arts & humanities subject areas were “authored books” (excluding edited books and chapters), indicating that authored books (i.e., monographs) form a significant portion of the national research outputs (at least in the United Kingdom; see Table 1 ).
Table 1. Statistics for the main 2008 Research Assessment Exercise submission types for the 67 units of assessment (UoAs).
UoA 	Authored book n (%) 	Edited book n (%) 	Chapter in book n (%) 	All book types n (%) 	Journal article n (%) 	Conference output n (%) 	Other n (%) 	Total (%)
Social sciences and arts & humanities (38 UoAs) 	13,795 	2,915 	17,486 	34,196 	64,531 	1,510 	1,1216 	111,453
	(12.38) 	(2.62) 	(15.69) 	(30.68) 	(57.90) 	(1.35) 	(10.06) 	
Science (29 UoAs) 	410 	59 	785 	1,254 	96,732 	2,467 	2,383 	102,836
	(0.40) 	(0.06) 	(0.76) 	(1.22) 	(94.06) 	(2.40) 	(2.32) 	
Total (67 UoAs) 	14,205 	2974 	18,271 	35,450 	161,263 	3,977 	13,599 	214,289
	(6.63) 	(1.39) 	(8.53) 	(16.54) 	(75.25) 	(1.86) 	(6.35) 	(100)

Quantitative indicators may be helpful to aid the large‐scale evaluations of books. Citations are a logical choice due to their use in the sciences, but book citation counting seems to be rarely used for research assessment. This may be due to the almost‐complete absence of citation associations from monographs in the two key citation indexes, the Web of Science (WoS) and Scopus (Cronin, Snyder, & Atkins, 1997 ; Hicks, 1999 ; Moed, 2005 ; Oppenheim & Summers, 2008 ; Taylor, 2011 ). Although citations to books from indexed journal articles and conference papers are recorded in the major citation indexes and attempts have been made to extract citations to monographs based on cited reference search techniques from journal articles (e.g., Bar‐Ilan, 2010 ; Butler & Visser, 2006 ), it does not seem reasonable to rely on these for book‐based disciplines because the logical source of bibliometric‐impact evidence in such cases would be other books. In fact, the WoS does not include citations from most books and monographs and primarily restricts its indexing coverage to high‐impact journals and few selected serials and reviews (e.g., Lecture Notes in Computer Science ). To fill this gap, there have been calls for a “Book Citation Index” (Garfield, 1996 ) to make citation counting possible. Previous studies have also suggested that Google Scholar (e.g., Bar‐Ilan, 2008 ; Bornmann et al., 2009 ; Harzing & van derWal, 2009 ; Kousha & Thelwall, 2008 ) and Google Books (e.g., Kousha, Thelwall, & Rezaie, 2010 ) contains publication types outside of the WoS and Scopus, and therefore might be particularly useful for impact assessment outside of the hard sciences. Nevertheless, both Google databases do not publish a complete list of their sources; consequently, their coverage of academic information and the quality of the indexed sources are unknown (see Jacsó, 2005a ; Kousha & Thelwall, 2009 ). There also have been initiatives to develop noncitation indicators such as counting library holdings as a way of estimating the reach of books (White et al., 2009 ) or attempting to systematically classify publisher reputation (Giménez‐Toledo & Román, 2008 ) by analogy with journal reputation. Webometrics also has proposed new methods for extracting information for research evaluation. For instance, link analysis results have been shown to correlate significantly with U.K. RAE ratings in some subject areas (X. Li, Thelwall, Musgrove, & Wilkinson, 2003 ). The number of times an article is downloaded also has been used (Brody, Harnad,& Carr, 2006 ), and in the era of digital books, book downloads are a possible indicator, although download data might not be accessible for large‐scale studies from some academic publishers.

The main objective of the current study is to assess two new, web‐based sources of citation data for books: Google Books and Google Scholar. We used books submitted to the 2008 RAE in seven selected book‐oriented disciplines (described later) as a convenient and reasonably comprehensive source of lists of key academic outputs in a country. We compared citation counts to books from Google Books and Google Scholar with Scopus in archaeology; law; politics and international studies; philosophy; sociology; history; and communication, cultural, and media studies to assess whether citations in the two new sources were sufficiently numerous to be useful and to get some indication of whether they were potentially relevant for research assessment.
Research Assessment: The United Kingdom

The U.K. Research Excellence Framework (REF) is the modified successor to the RAE, the periodic national process to allocate public research funds for academic research. The main outcome is “quality profiles for each submission of research activity” (RAE 2008 Guidance on Submissions, 2005 , p. 5). In 2008, there were 67 UoAs or subject areas within which research was assessed by peer review, and more than 1,000 reviewers scored the submitted research outputs (up to four per researcher) against five quality levels from unclassified (below the standard of nationally recognized work) to 4* ( world‐leading ) (RAE 2008 Panels, n.d.).

The Higher Education Funding Council for England (HEFCE) is in charge of the new framework for assessing the quality of research, the REF, which was set to be first used in 2014. Although peer review is the main factor in the quality assessment of REF research outputs and has its own advantages and controversies (for an in‐depth review, see Bence & Oppenheim, 2004 ), citation information (from the Thomson Reuters WoS and Elsevier's Scopus) also will be used in some subject areas to assist the peer‐review process. As discussed later, previous investigations have reported significant correlations between citation measures and RAE scores in different subject areas, supporting the use of citation counts in this context as indicators of research quality. Furthermore, the significant correlations found between other types of expert review and citation metrics, for instance, in library and information science (J. Li, Sanderson, Willett, Norris, & Oppenheim, 2010 ), mathematics (Korevaar & Moed, 1996 ), chemistry (van Raan, 2006 ), and condensed matter physics (Rinia, van Leeuwen, van Vuren,& van Raan, 1998 ) and also between the research impact of successful postdoctoral applicants and the judgments of selection committees ( Bornmann & Daniel, 2006 ) suggest that citation data are relevant for research evaluation in many fields. Some even have argued that bibliometrics could replace peer review for monitoring research performance (Oppenheim, 1996 , p. 155).

Despite this evidence, there are criticisms about supplementing peer review with citation analysis and criticisms of citation analysis itself (e.g., MacRoberts & MacRoberts, 1996 ; Warner, 2000 ; also see van Raan, 2005 ). Moreover, a recent study has revealed that correlations between citations (from the WoS) and RAE peer‐review scores are not statistically significant in several social science and humanities UoAs, but are significant in most sciences (Mahdi, D'Este & Neely, 2008 , p. 16; see also Citation Indicators and the RAE section). One explanation might be the low WoS coverage of the journals in these disciplines (see Moed, 2005 , p. 119) or that other types of research outputs such as book chapters and monographs are significant in them (discussed later).

Using the WoS and Scopus citation databases, a pilot exercise commissioned by the HEFCE (2009a) to develop bibliometric indicators for the REF, reported that “bibliometrics are not sufficiently robust at this stage to be used formulaically or to replace expert review in the REF. However there is considerable scope for citation information to be used to inform expert review” (p. 3). This study selected 35 UoAs from the 2008 RAE in 22 U.K. institutions, and was conducted as evidence to be used for academic research planning and to decide whether and how bibliometrics should be used within the REF because the potential use of citation indicators for research evaluation varies across disciplines (discussed later) and because the main journal‐based citation indexes (WoS and Scopus) may not be adequate for citation information to inform peer review in some subject areas. This would be the case if the results were more misleading than informative because a broad spectrum of influential research was not recognized or because there was too little information to identify any influential research.

Although about “80% of journal articles submitted to the RAE 2001 could be found in the Web of Science” (Mahdi et al., 2008 , p. 9), this varies across disciplines and is much lower in many social sciences and arts& humanities UoAs (e.g., 24% in law; 29% in arts and design; ∼30% in theology, divinity, and religious studies; and 39% in education). One explanation for these disciplinary differences in publication behavior is that in the basic sciences, research tends to have an international audience, but many research topics in social sciences and arts & humanities disciplines have a more geographically limited audience because they are based on national or regional issues (e.g., national or regional law, social policy, economics and business considerations) and may appear in regional or national publications (e.g., monographs and reports) in national languages other than English, which are not well indexed by the WoS (see Nederhof, 2006 ). This confirms that broader types of publications and sources of citation data may be needed to identify research excellence in the social sciences and the arts & humanities.
Book Impact Assessment

Peer review is probably the optimal way to assess the value of books, but metrics also have been assessed as replacements or supplementary sources of information. Although books might be assessed in many different ways such as publisher quality (or even, in law, by length; Moed, Luwel, & Nederhof, 2001 ), for a long time, information scientists have sought appropriate citation data for their impact assessment (e.g., Garfield, 1996 ; Small & Crane, 1979 ). In particular, many investigations have suggested that conventional journal‐based citation databases (e.g., WoS) can sometimes be inadequate for the impact assessment of book‐based disciplines (e.g., Cronin et al., 1997 ; Glänzel & Schoepflin, 1999 ; Nederhof, 2006 ; Thompson, 2002 ), and attempts have been made to use online citations and informal scholarly indictors for social science research evaluation (Kousha & Thelwall, 2007b ).

The importance of book citations is supported by evidence that there are more citations to books and monographs than to journal articles in some social sciences and many arts & humanities subject areas (for a review, see Huang & Chang, 2008 ; Nederhof, 2006 ). For instance, “books account for 46 percent of the overall citations to U.K. social science literature, whereas only 12 percent of the citations in natural science were to books” (Earle & Vickery, 1969, as cited in Tang, 2008 , p. 357). Small and Crane ( 1979 ) also reported that the proportion of citations to books was about 40% in sociology and 25% in economics whereas it was about 1% in high‐energy physics. Nederhof and van Raan ( 1993 ) also found that the number of citations per publication was much higher for books ( n =3.15) than it was for ISI‐indexed articles ( n =0.95) in six research groups in economics. Similarly, sociological books seem to receive about three times more citations than do journal articles (Clemens, Powell, McIlwaine, & Okamoto, 1995, as cited in Nederhof, 2006 ). Within library and information science, Chung ( 1995 ) also examined over 5,000 references in 68 monographs and 352 journal articles during 1981 to 1990 and found that about 50% of the cited references were to books and book chapters and that 38% were to journal articles.
Online Indicators of Book Impact

Although several investigations have used Google (e.g., Vaughan & Shaw, 2005 ), Google Scholar (e.g., Bar‐Ilan, 2008 ; Bornmann et al., 2009 ; Franceschet, 2010 ; Jacsó, 2005b ; Kousha & Thelwall, 2007a ; Meho & Yang, 2007 ; Mingers & Lipitakis, 2010 ; Shaw & Vaughan, 2008 ), and Google Books (Kousha & Thelwall, 2009 ; Kousha et al., 2010 ) for the impact assessment of scientific research, it seems that no comprehensive study has directly used Google Books and Google Scholar for the citation impact of books across different disciplines. Most previous studies instead have focused on citations from Google Scholar to journal articles and compared the results with the WoS or Scopus. In contrast, Bar‐Ilan ( 2010 ) used Google Scholar, Scopus, and the WoS to assess citations to the book Introduction to Informetrics by Leo Egghe and Ronald Rousseau. For Google Scholar citations, three variations of the title (with and without the authors) were searched, and the results were manually checked to remove false and duplicate matches. Of the total 358 potential citing items, 86% were scientific sources that cited the book, indicating high accuracy for this specific case. Bar‐Ilan ( 2010 ) found that the WoS and Scopus databases covered about 90% of the citations found by each other. Google Scholar missed about 30% of the citing items covered by the WoS and Scopus, but 109 unique citations located by Google Scholar were not found by either Scopus or by the WoS. Bar‐Ilan ( 2010 ) concluded that the three databases supplement each other; in particular, the coverage of citing items in the WoS and Scopus was “quite comparable” and that Google Scholar's coverage was “surprisingly good, and its accuracy was also better than expected” (pp. 505–506).

It seems that only two studies have used Google Books for scientific impact assessment, and both assessed only citations to journal articles. For research in 10 sciences, social sciences, and humanities subject areas, Kousha and Thelwall ( 2009 ) compared citations from Google Books searches with WoS citations to selected journal articles. Google Book citations were 31 to 212% of the WoS citations in the social sciences and humanities, but only 3 to 5% in the sciences, except for computing (46%). There were relatively strong correlations between Google Books citations and ISI citations in all the studied disciplines (although weaker in chemistry and physics), suggesting that book citations measure a similar kind of impact to that of ISI citations. They concluded that Google Books citations were numerous enough to supplement WoS citations for impact assessment of articles in the social sciences and humanities. In another study, Kousha et al. ( 2010 ) used a combined Integrated Online Impact indicator for impact assessment of articles published in the Journal of the American Society for Information Science and Technology ( JASIST ) and Scientometrics in 2003. They compared citation counts from the WoS and Scopus with five online sources of citation data: Google Scholar, Google Books, Google Blogs, PowerPoint presentations, and course reading lists. Most notably, the Google Books mean citations per article were 2.23 and 1.08 times larger than the WoS citations, and Google Scholar mean citations per article were 22.2 and 14.3 larger than the WoS citations for JASIST and Scientometrics , respectively, confirming the numerical value of citations from books.
Citation Indictors and the RAE

A number of studies have investigated associations between bibliometric indicators and RAE ratings in different subject areas, with most of them finding significant correlations between the results of citation analysis and the peer‐review RAE ratings. Such correlations have been found between the 1992 RAE ratings for British library and information science (LIS) departments and total citations as well as citations per member of staff (Oppenheim, 1995 ; Seng & Willett, 1995 ); citations also have been used to predict future RAE performance in this area (Holmes & Oppenheim, 2001 ). Similar results have been found for anatomy, genetics, and archaeology (Oppenheim, 1997 ); psychology (Smith & Eysenck, 2002 ); archaeology (Norris & Oppenheim, 2003 ); and music (Oppenheim & Summers, 2008 ). Despite significant correlation between citations and RAE rankings for music, the WoS citation database was judged insufficient to assess music research (discussed later). A more recent study also reported a significant correlation between the peer ranking of LIS scholars and their h‐index at the individual level as well as between 2008 U.K. RAE rankings and h‐/g‐index scores in pharmacy and library and information management (excluding anthropology) at the departmental level (Norris & Oppenheim, 2010 ). Significant correlations are necessary to assess whether particular types of citation can be used as impact indicators, but it is important to understand the reason for any differences found to effectively interpret the results.

Oppenheim ( 1995 ) conducted one of the first investigations of this kind to assess the rank correlation between citation counts and RAE ratings. He found a strong, significant correlation between citation‐based rankings and the 1992 RAE ratings for British LIS university departments. Spearman correlation coefficients were 0.81 between RAE ratings and numbers of citations received by departments and 0.82 between RAE scores and numbers of citations per member of staff ( p s=0.01). Seng and Willett ( 1995 ) also found strongly significant rank correlations between the total number of citations and both the mean number of citations per member of staff and per publication with 1992 RAE rankings for seven U.K. LIS departments.

In a complementary study, Oppenheim ( 1997 ) tested the rank correlation between citation counts and the RAE ratings in multiple disciplines including genetics, anatomy, and archaeology. In all three subject areas, he again reported a statistically significant rank correlation between the total number of citations received, or the average number of citations per member of staff, and the RAE scores. ISI's Science Citation Index (for genetics and anatomy) or the Arts and Humanities Citation Index (for archaeology) were used to record citations counts for each of the academics. In anatomy, genetics, and archaeology, Spearman rank correlations between average citations per member of staff and the 1992 RAE rankings were 0.48, 0.68, and 0.73, respectively.

Smith and Eysenck ( 2002 ) examined the correlation between average ISI citation counts (excluding self‐citations) to academic staff in 38 university psychology departments in the United Kingdom in 1998 with RAE grades awarded in 1996 and 2001. They found highly significant rank correlations for both years (Spearman's α=0.91 and 0.85, respectively). Smith & Eysenck concluded that citation counts are “more cost‐efficient” and “less subjective” than is “time‐consuming RAE” for research evaluation of psychology departments (p. 8). One reason for finding high correlations between ISI citations and RAE scores might be that journal articles play a particularly significant role in research communication in psychology because its publication pattern is close to that of science. For instance, about 96% of the 2008 RAE submissions were journal articles whereas only 1.3% were books (including authored and edited books and chapters, see Appendix Table A1 ).

Using the ISI WoS ( Arts and Humanities Citation Index )‐cited reference searches, Norris and Oppenheim ( 2003 ) recorded citation counts by searching the names of 692 staff members in the 2001 RAE in the field of archaeology. Correlation analysis showed high statistically significant rank correlations, r =0.79–0.85, p ≤ 0.01, between the RAE results in archaeology departments and different citation measures of the academics (e.g., total and average staff citations). Norris and Oppenheim ( 2003 ) recommended that although citations analysis is not the ultimate assessment tool, “it should be adopted as the primary procedure for the initial ranking of university departments” (p. 728).

In a follow‐up study, Oppenheim and Summers ( 2008 ) examined citation‐count rankings and rankings based on the 2001 RAE scores in the field of music using the ISI Arts and Humanities Citation Index . At the departmental level, total and mean citation‐count rankings correlated strongly with RAE‐based rankings, r =0.80 and r =0.81, respectively; p ≤ 0.01. However, they found a weaker rank correlation between RAE scores and individual citation counts, r =0.46, p ≤ 0.01, and concluded that despite the strong, significant correlations between citation measures and RAE scores, “the coverage of the Arts and Humanities Citation Index is unrepresentative of music research,” and broader types of citation data from nonjournal material such as books are needed for evaluating the quality of music research.

On a large scale, Mahdi et al. ( 2008 ) assessed whether citation counts correlated with RAE scores for 141,789 journal articles submitted to 67 RAE 2001 UoAs. About 80% of the journal articles were found in the WoS. The study then selected (a) institutions that had 20 or more matched journal articles in the specific UoA and (b) UoAs with at least 20 institutions. The overall results gave statistically significant correlations for 19 of the 28 qualifying UoAs. Most important, for five of eight social sciences and humanities disciplines—politics and international studies; social policy and administration; sociology; history; and education—there were insignificant correlations between citation measures and RAE ratings. The correlation was statistically significant, but low, in geography (0.383) and economics and econometrics (0.677), but relatively strong in business and management (0.782). In nearly all biomedical fields (seven of eight, with the exception of nursing) and most engineering subject areas (9 of 13), research impact indicators based on citations to journal articles correlated with the RAE 2001 scores. The authors concluded that the WoS “offers only partial coverage in some subjects, and hence the use of bibliometrics becomes increasingly less valuable as we move from Biomedical, Physical Sciences and Engineering, to Socials Sciences, Literature and Arts and Humanities” (Mahdi et al., 2008 , p. 3). Taking this evidence in conjunction with the aforementioned studies, it seems that citation‐count rankings widely, but not universally, correlate with RAE rankings, particularly for science. Note, however, that the raw figures are more important than are the rankings, and it is the latter that has tended to be tested. Hence, the evidence is not clear that citations are useful to inform judgments about RAE (or REF) scores, although they should be reasonably successful at pointing to the best and worst performing departments.

Web indicators also have been assessed for other national research‐evaluation exercises. Smith ( 2008 ) used Google Scholar to generate citation counts to the web‐based research output of eight New Zealand universities and found that the total citations and hits from Google Scholar correlated with the New Zealand national research assessment exercise. Although there were methodological problems using Google Scholar for citation mining, it “provides a relatively simple way of assessing the Web based research output of institutions” based on different types of information available on the Web (Smith, 2008 , p. 315).

Despite evidence of statistically significant correlations between bibliometric indicators and RAE ratings in many subject areas and arguments that the RAE unwisely adds secondary subjective judgments to “articles that have already undergone peer review” (Bence & Oppenheim, 2004 , p. 347), there have been criticisms about early proposals to replace peer review with citation measures in the RAE. This is because in the peer‐review process, the quality of research is directly assessed through the specialized knowledge of informed experts, but citation counts depend on the complex citing behaviors of various authors (Bornmann & Daniel, 2008 ). Perhaps there also is a fear that citation counts could be manipulated and that citations or the standard sources of citations may ignore some relevant types of scholarly contributions. For instance, van Raan ( 2005 ) argued that the ranking of institutions by bibliometric indicators is an inappropriate tool for research evaluation, and that “peer review has to remain the primary process of research evaluation, bibliometric indicators must act as a support tool in order to make peer review more objective and transparent” (p. 141). Weingart ( 2005 ) also argued that bibliometric “raw data” should be cleaned and corrected when used for indicators that support science policy and funding decisions and warned against “the uncritical use of bibliometric measures independent of the peer review process” (p. 130). Warner ( 2000 ) also argued that replacing expert peer review with citation analysis in the RAE was “highly unrealistic” and claimed that “the future value of citation analysis could be to inform, but not to determine, judgements of research quality” (p. 453).
Research Questions
The objective of this study is to assess whether nontraditional online sources of citations are sufficiently numerous to help evaluate the scholarly contribution of books. In particular, we are interested to see whether our method gives significantly more useful results, based upon different sources of citation data, and to examine the potential uses of web citation to support peer judgments about the scholarly impact of books in the future REF. Correlation tests have been preformed for interpreting the extent of the agreement between journal‐based citation indicators and boarder types of online‐citation measures. The following questions drive this research:

    RQ1: Is the number of Google Scholar and Google Book citations to books in book‐based disciplines sufficiently numerous for research impact assessment? This is a heuristic evaluation, but if these sources yielded more citations than do traditional citation indexes such as Scopus, then they would earn consideration, even in contexts when traditional citation sources were insufficient.

    RQ2: Do Google Books and Google Scholar citations to authored books correlate with citations from traditional citation indexes such as Scopus in book‐based disciplines?

Methods

We compared Google Scholar and Google Books citations with Scopus citations to 1,000 sampled books submitted to the 2008 RAE in seven book‐based disciplines. The U.K. 2008 RAE is a convenient, large, and fairly comprehensive list of the best academic outputs of a single country over a fixed period (6 years 7 months). We did not assess the value of the citations because this needs a separate qualitative study and extensive manual checking in itself, but only whether they are sufficiently numerous to be an alternative or a compliment to traditional citation indexes. Google Scholar was used because it encompasses a wide range of nontraditional academic sources, and Google Books was used because it is logical to check books for citations to other books. We selected Scopus instead of the WoS to compare conventional against web‐extracted citations because (a) Scopus is sufficiently mature, especially from 1996 onward (see Ball & Tunger, 2006 ), that it will be one of the key sources of citation data for the future REF (see HEFCE, 2009a); (b) it is more comprehensive than is the WoS in terms of indexed peer‐reviewed journals (∼17,000 vs. 10,000) and other types of publications, especially in the social sciences and arts& humanities (see Scopus Content Coverage Guide, 2010 ); and (c) it has an effective search option to locate citations to books in the references of journals and other publications. The “Cited Reference Search” field in the WoS was problematic for locating citations, especially for books with very general titles. In fact, the WoS cited reference search does not display the full bibliographic information of cited works in the context of the reference section, and it thus was not possible to manually check the accuracy of citation counts (e.g., citations to different editions of books). Moreover, the large overlap in active journals between the two citation databases (e.g., 84% of the WoS is indexed by Scopus) (see Gavel & Iselid, 2008 , p. 17) and the strong correlation between impact indicators from the two databases (see Archambault, Campbell, Gingras, & Larivière, 2009 ) suggest that it would be largely redundant to check the WoS against Google Scholar and Google Books.
Research Population

To identify book‐based disciplines to be sampled, we downloaded all submission profiles from the 67 UoAs from the 2008 RAE website ( http://www.rae.ac.uk ). We then used the “output type” label for each submission and manually recorded the number of the main submission types: authored books, edited books, chapters in books, journal articles, conference contributions, and other types (see Appendix Table A1 ). As shown in Table 1 , 16.5% of the submissions to all 67 UoAs were book items (authored books, edited books, and chapters). There were broad disciplinary differences in the proportion of books in the 38 social sciences and arts& humanities compared to the 29 hard sciences. For instance, the proportion for all types of books was 31% in social sciences and arts & humanities subject areas, but was much lower (1.2%) in the hard sciences. In contrast, the proportion of journal article submissions was 94% in the 29 hard sciences, but this was lower in the 38 social sciences and arts & humanities subject areas (∼58%). These figures confirm previous findings that books and monographs are a major research communication platform in the arts & humanities and in many social science disciplines (e.g., Glänzel & Schoepflin, 1999 ; Hicks, 2004 ; Huang & Chang, 2008 ; Nederhof, 2006 ), but are less significant in many hard sciences.

To study the citation impact of book‐based disciplines, we selected seven major subject areas with a high percentage of “authored book” submissions (i.e., excluding edited books and chapters). The two factors considered when selecting the book‐based disciplines were that (a) at least 15% of the submissions were “authored books” and that (b) the selected areas were representative of major social sciences and arts & humanities subjects. We selected seven UoAs to make the project manageable. For each selected UoA, we took a random sample approximately proportional to the total number of authored book submissions in each discipline to make a total of 1,000. Table 2 shows that the average number of authored books (excluding edited books and chapters) per panel member in the seven selected fields is 52.
Table 2. Books per panel member in seven units of assessment (UoAs) submitted to the 2008 Research Assessment Exercise.
UoA 	Authored books 	Sampled books 	No. Panel members 	Authored books per panel member
Archaeology 	376 (17.5%) 	100 	11 	34
Law 	996 (15.9%) 	170 	14 	71
Politics and international studies 	1,028 (21.8%) 	170 	16 	64
Sociology 	619 (16.6%) 	160 	16 	38
Philosophy 	326 (15.7%) 	100 	16 	20
History 	1,665 (23.9%) 	200 	17 	97
Communication, cultural, and media studies 	410 (18.8%) 	100 	13 	31
Total 	5,420 	1,000 	103 	52
Google Books

Google Books ( http://books.google.com ) supports full text searching of its database and displays where the keywords occur in the matching texts. Hence, it is possible to search the bibliographic information of books and to locate citations in the full text of many digitized books.

To locate Google Books citations to authored books, we searched the titles of all 1,000 sampled books as phrase searches (e.g., “Political Leadership and the Northern Ireland Peace Process”). For books with very short or general titles, additional bibliographic information was added to the query, such as the first‐ author's name, the publisher's name, or the publication year, to reduce the number of false matches (e.g., “Political Constitutionalism” “Cambridge University Press” Bellamy 2007). Furthermore, sometimes we conducted several searches to get more accurate citation counts (e.g., omitting nonalphanumeric characters such as hyphens or slashes from book titles). Another important task was to manually check the results with the Google Books “preview” (of the whole book) or “snippet view” (a few sentences displaying the search terms in context) to check for false matches and to check whether a book had been mentioned as a citation, such as in a reference list or a footnote. Nevertheless, for “no preview” books within the Google Books search results, we could not find a practical method to manually check the citation motivations and therefore excluded all such results from the citation counts. Although the manual checking was time‐consuming, it was useful because in some cases we found bibliographic data that were not created for citation reasons, such as annotated bibliographies or advertisements for new or future publications—usually at the end of books.

In most cases, we could not find the first‐author names in the RAE book submissions to be used in the searches, so we used Google Books itself and also sometimes the Library of Congress Online Catalogue to locate first‐author names, checking bibliographic information (e.g., ISBN, pages, coauthors, etc.) against the original RAE submissions. In some cases, we found incorrect, incomplete, or modified bibliographic information in RAE submission records, such as misspellings or technical hitches (e.g., “Marsilius of Padua and 'the Truth of History',” where ' is the HTML code for quotes) or slight modifications (e.g., using “&” instead of “and” or “17th‐century” instead of “seventeenth‐century”). In such cases, we tried to identify the original bibliographic information and then used it for all searches in Google Books, Google Scholar, and Scopus. Another important issue in the manual‐checking process was the existence of various editions of the same book. For instance, the book Legal Problems of Credit and Security by Royston Miles Goode has three different editions published in 1982 (133 pages), 1988 (218 pages), and 2003 (343 pages). Consequently, we found citations in the Google Books search results to different editions. However, the 2003 edition was submitted to 2008 RAE, and as shown earlier, it seems that there are significant changes in the 2003 edition based on additional page numbers. Thus, we decided to ignore citation counts to other editions (both 1982 and 1988) as irrelevant to the intellectual impact of the submitted research in the time period of the 2008 RAE.
Google Scholar

For Google Scholar searches, we again searched the exact titles of books as phrase searches and combined them with other bibliographic data such as first‐author names, publishers, or publication years if necessary. We then recorded the number of Google Scholar citations by selecting the “cited by” option below each displayed record. We did not consider the citation counts reported by Google Scholar because they may include duplicate citing items or false matches. For this reason, we again manually checked the full text of open‐access citing documents through either the “view as HTML” and “cached” options below some retrieved records or preprint/postprint links (e.g., “[PDF] from cornell.edu”). Otherwise, we followed the link in Google Scholar to the full text of the citing documents through our institutional subscriptions to major journal publishers (e.g., Elsevier, Springer, Wiley, InformaWorld, Emerald, Sage, Oxford, JSTOR). In some cases, it was not possible to check the citations in the context of the retrieved documents through these methods. Therefore, the only practical method was to recognize the formal citation reasons for using books based on the brief textual information below each retrieved record in the Google Scholar search results. Our initial observations revealed that if the citation information of a book appeared in brief records in bold and looked like a cited reference (e.g., APA or Chicago citation styles), then it was likely to be a formal citation; otherwise, it was more likely to be a false match. Moreover, in rare cases, there were citations from books (usually with [BOOK] at the beginning of the record). We also checked these few cases with our Google Books search results to avoid counting duplicate citations in the two databases.

All Google Books, Google Scholar, and Scopus Searches (discussed later) were conducted during 2 months (October–November 2010) consecutively for each book to lessen the potential impact of the time window on the citation count and to make the project manageable, although this short period of time may still have a small impact on the citation changes over time for the studied disciplines.
Scopus Search

For Scopus citation searches, we searched the book titles as phrase searches in the reference source title field (REFTITLE). However, for general book titles, we again used additional bibliographic information such as the first author or the publisher to generate more effective searches.
Results

Table 3 compares the number, mean, and median of Google Books and Google Scholar with Scopus citations across seven book‐based disciplines based upon the sample of 1,000 authored books submitted to the 2008 RAE. It shows that Google Book and Google Scholar citations were 143 and 318% of Scopus citations, respectively, and therefore seem numerous enough to warrant consideration in the role of supporting the peer‐review process in book‐based disciplines, especially in conjunction with additional citation information from Scopus.
Table 3. Comparisons between Google Books, Google Scholar, and Scopus citation counts for authored books submitted to seven social sciences and humanities disciplines in the 2008 Research Assessment Exercise.
		Google Books citations 	Google Scholar citations 	Scopus citations
UoA 	No. of sampled books 	n M Mdn 	% of Scopus 	n M Mdn 	% of Scopus 	M Mdn
Law 	170 	1,687 	254.4% 	2,838 	428.1% 	663
		9.9 		16.7 		3.9
		6 		8 		2
History 	200 	3,723 	281% 	2,851 	215.2% 	1,325
		18.6 		14.3 		6.6
		11.5 		7 		4
Sociology 	160 	4,512 	91.7% 	15,648 	318% 	4,920
		28.2 		97.8 		30.8
		14 		37 		11
Philosophy 	100 	1,668 	115.1% 	4,553 	314.2% 	1,449
		16.7 		45.5 		14.5
		9 		17 		6.5
Archaeology 	100 	1,225 	174.3% 	2,028 	288.5% 	703
		12.3 		20.3 		7
		6.5 		8 		3
Politics and international studies 	170 	3,469 	143.5% 	8,267 	341.9% 	2,418
		20.4 		48.6 		14.2
		11 		20 		6
Communication, cultural, and media studies 	100 	1,621 	164.7% 	3,548 	360.6% 	984
		16.2 		35.5 		9.8
		11.5 		16 		3
Total 	1,000 	17,905 	143.7% 	39,733 	318.8% 	12,462
		17.9 		39.7 		12.5
		9 		13 		4

    UoA=unit of assessment.

Google Books Versus Scopus Citations

The results indicate that Google Books has relatively good coverage of books for citation impact in book‐based disciplines. Surprisingly, the median of the Google Books citations (not overlapping with Scopus citations) is more than double ( Mdn =9) the median of the Scopus citations ( Mdn =4), suggesting that Google Books could potentially be considered as a valuable source of citation impact for book‐based disciplines, although follow‐up investigations would be needed for the quality assessment of book citations. Moreover, the medians of the Google Books citations are much higher in three humanities: law; history (both about three times higher than that of Scopus); and communication, cultural, and media studies (about four times higher than that of Scopus).

Most significant, in history, the median number of Google Books citations ( Mdn =11.5) is higher than both Google Scholar ( Mdn =7) and Scopus ( Mdn =4) citations, suggesting that in some arts & humanities subject areas, book citations may be more significant than are other sources of citations (e.g., journal and conference papers).

Appendix Table A2 reports the top‐five highly cited books in the 2008 RAE in the seven selected book‐oriented disciplines based on Google Books.
Google Scholar Versus Scopus Citations

Table 3 reports Google Scholar and Scopus citations of authored books submitted to the 2008 RAE. It shows that Google Scholar citations were about 3.2 times more numerous than were Scopus citations ( Mdn s=13 and 4, respectively), confirming previous studies (reviewed earlier) that showed that Google Scholar is more comprehensive and includes broader types of document. Most notably, in communication, cultural, and media studies and law, the median citations for Google Scholar were four times higher than the median of Scopus, suggesting that Google Scholar can be a helpful source of citation data when nonjournal publications are important for evaluating social science and arts & humanities research.

Note that since in most cases the distributions of citations are highly skewed, the median is reported to compare Google Books, Google Scholar, and Scopus citations.
Patterns of Similarity

The correlation tests in Table 4 were performed for each UoA using the individual‐sampled authored books submitted to the 2008 RAE as the data points. Spearman correlation tests were applied instead of Pearson because in all cases the frequency distributions of citations were highly skewed. As shown in Table 4 , there is a significant correlation between Scopus citations and both the Google Books and the Google Scholar citation counts in all studied subject areas ( p < 0.01). The correlations are stronger between Google Scholar and Scopus citations and weaker between Google Books and Scopus citations. One explanation might be that both Google Scholar and Scopus measure a similar type of citation impact mostly based on journal citation counts whereas a weaker relationship is expected between Google Books and Scopus because they index completely different sources of citations: books versus journals and conference papers. Furthermore, the fact that books receiving more citations from journals in the Scopus database tend to receive more citations from books indexed by Google Books suggests that the intellectual influence of book‐based research often overlaps between book‐based and journal‐based research. For example, this may occur because citing journal articles are combined and republished as a monograph, because a book is influential in a field that publishes both books and journal articles, or because a book is influential in multiple fields, some of which tend to publish books and some of which tend to publish journal articles. From a research‐assessment perspective, it probably does not matter which of these applies most for any particular book.
Table 4. Correlations between Google Books, Google Scholar, and Scopus citation counts to authored books submitted to the 2008 Research Assessment Exercise for each studied discipline.
UoA 	Sampled books 	Google Books and Scopus 	Google Scholar and Scopus 	Google Books and Google Scholar
Law 	170 	0.616 ** 	0.740 ** 	0.746 **
History 	200 	0.683 ** 	0.778 ** 	0.744 **
Sociology 	160 	0.833 ** 	0.944 ** 	0.833 **
Philosophy 	100 	0.726 ** 	0.934 ** 	0.771 **
Archaeology 	100 	0.684 ** 	0.793 ** 	0.798 **
Politics and international studies 	170 	0.731 ** 	0.873 ** 	0.814 **
Communication, cultural, and media studies 	100 	0.732 ** 	0.791 ** 	0.773 **

    UoA = unit of assessment.
    ** ** p =0.01.

Further Analysis
Overlaps Between Scopus and Google Scholar Citations

We measured the overlap between Scopus citations and Google Scholar citations for a sample of 100 authored books submitted to the RAE in communication, cultural, and media studies to estimate how the coverage of these different sources may influence citation indicators. As shown in Table 5 , Google Scholar citations have a 16% relative overlap with Scopus (84% unique citations) whereas the relative overlap is about 55% for Scopus citations (45% unique citations). There are small disciplinary differences in the relative overlap of Google Scholar and Scopus for most areas, with two exceptions: law with about 91% Google Scholar unique citations and history with 67% Scopus unique citations.  
image
Figure 1
Open in figure viewer PowerPoint

Google Books citations against Research Assessment Exercise peer review scores in communication, cultural, and media studies.
Table 5. The relative overlap and unique citations for Google Scholar and Scopus to 100 sampled authored books submitted to the 2008 Research Assessment Exercise.
		Citation count 		Relative overlap 	Unique citations (%)
UoA 	Book sampled 	Google Scholar 	Scopus 	Overlapping citations 	Google Scholar 	Scopus 	Google Scholar 	Scopu
Law 	17 	326 	71 	32 	9.82% 	45.07% 	294 	39
							(90.18%) 	(54.93%)
History 	20 	185 	107 	35 	18.92% 	32.71% 	150 	72
							(81.08%) 	(67.29%)
Sociology 	16 	878 	232 	159 	18.11% 	68.53% 	719 	73
							(81.89%) 	(31.47%)
Philosophy 	10 	129 	41 	24 	18.60% 	58.54% 	105 	17
							(81.40%) 	(41.46%)
Archaeology 	10 	218 	88 	42 	19.27% 	47.73% 	176 	46
							(80.73%) 	(52.27%)
Politics and international studies 	17 	522 	183 	94 	18.01% 	51.37% 	428 	89
							(81.99%) 	(48.63%)
Communication, cultural, and media studies 	10 	341 	67 	45 	13.20% 	67.16% 	296 	22
							(86.80%) 	(32.84%)
Total 	100 	2,599 	789 	431 	16.58% 	54.63% 	2,168 	358
							(83.42%) 	(45.37%)

    UoA=unit of assessment.

Correlations Between Google Books Citation Means and RAE Scores

The significant number of books indexed in Google Books allows the use of an alternative web‐citation indicator for assessing the impact of research outputs in book‐based disciplines. To check this, correlation analysis can provide statistical evidence about the extent of the agreement between RAE scores and Google Books citation measures. For this reason, we recorded the 2008 RAE average ranking scores in communication, cultural, and media studies for 47 U.K. institutions (from http://www.guardian.co.uk/education/rae ). We also recorded the number of Google Books citations to the 407 authored books submitted (omitting three duplicate titles) by 45 U.K. institutions to the 2008 RAE after manual checking of citations and removing false matches. The 407 authored books received 6,235 citations from other books indexed by Google Books, with a mean and median of 138.5 and l08, respectively (Appendix Table A3 ).

A correlation test was performed for RAE average ranking scores and average Google Books citations, using institutions in communication, cultural, and media studies as the units of data analysis. There was a small, but significant, correlation, Spearman r =0.387, n =45, p =0.009, between the two variables. This weak, but significant, relationship between the average RAE ranking score and average Google Books citation suggests that book citations might be useful indicators to support the peer‐review process, especially in subject areas where books are important. The relatively weak correlation may be due to the differing importance of books in the field specialties of individual departments. The correlation between the total number of Google Books citations received by each department in communication, cultural, and media studies and their RAE score was much higher, Spearman r =0.648, n =45, p =0.000, but this only confirms that larger departments attracted more citations and gained higher RAE ratings.
Conclusions

Our study indicates that there are substantial numbers of citations to academic books from Google Books and Google Scholar, and it therefore may be possible to use these potential sources to help evaluate research in book‐oriented disciplines. Most notable, the possibility to locate cited references in many academic books through Google Books provides new opportunities to assess citations from books to books (but see the limitations discussed later) that were not traceable before through traditional article‐based citation indexes (e.g., WoS and Scopus). Due to the relatively moderate overlap between Google Scholar and Scopus citations, a combination of the two is recommended rather than just one of them. The moderate, but significant, correlation between average RAE scores and average Google Books citations in communication, cultural, and media studies is promising since a recent study has found no significant correlation between ISI citations and RAE peer‐review scores in several social science and humanities UoAs (e.g., history; sociology; education; social policy and administration; politics and international studies) whereas a significant relationship was found in most sciences (Mahdi et al., 2008 ). The discrepancy seems to be because in the aforementioned social science and humanities subject areas, books and monographs are one of the main platforms for research communication (e.g., Huang & Chang, 2008 ; Nederhof, 2006 ). While the significant correlations do not imply that citations have a causal relationship with research quality or can be used to measure research impact in the sense of the RAE, they do give evidence that citations can be used as indicators of research impact: evidence to present to human judges to help them to identify research quality.

A limitation of this study is that it focused on one country, and this is particularly important in humanities disciplines with a national focus (e.g., law, history). It may be that Google Books' coverage of other countries' literature is much larger or smaller due to local agreements between Google and libraries or publishers for book indexing. Similarly, other countries may not find as many Google Scholar citations because less of the national academic output is published online.

Another limitation is that this study only used quantitative methods to assess book citations, and follow‐up studies of motivations for book citations also are needed to validate their use for research‐quality assessment. A practical issue is that the data had to be collected manually, which makes large‐scale analyses time‐consuming, although it should still be cost‐effective for the U.K. REF. The challenging issue for automatic extraction is the same as that for bibliometrics: data cleaning. For instance, the mean and median Google Book search citations were 28.3 and 17, respectively, but after manual checking, they decreased to 17.9 and 9, respectively. Nevertheless, the unique content of Google Books (citations from books to books) for impact assessment of book‐based disciplines over conventional citations indexes and better coverage of citation information for research assessment (Google Books Mdn =9 vs. Scopus Mdn =4) might be a motivating factor in support of Garfield's ( 1996 ) Book Citation Index call. In fact, such a citation index already has been created for Spanish by the Dialnet Consortium ( http://dialnet.unirioja.es ). Another methodological issue is that RAE submissions contain a combination of books, journal articles, and other sources, and that it therefore would be instructive to examine the extent to which citations from books and from journal articles to books and to journal articles were complementary in the sense of giving different rather than similar evidence of impact. Presumably, there would be some cases of articles and books having very different citation impacts from books or from journal articles (e.g., Kousha & Thelwall, 2009 ), but it is not known how extensive this would be.

In terms of practical implications of this study for the UK, as discussed earlier, the absence of a plan to use citation information to inform expert reviewers about the impact of research outputs in the REF in “the arts, humanities and a number of other panels” (see HEFCE, 2009b, p. 3), may be a drawback in quality assessment of U.K. research because of the difficulty in assessing large numbers of books.

The challenge is that in arts & humanities and many social sciences, subject‐area books are a major research platform, and therefore broader sources of citation information (e.g., citations from books to books) also may be required to effectively identify research excellence. Our results suggest that HEFCE or REF panels in book‐based disciplines should urgently consider the possibility to generate new citation indicators to support peer review, using Google Scholar and Google Books in addition to Scopus and the WoS. Although indicators based on journal citations in these areas already have been rejected, the case is worth reconsidering because the addition of citations from books fills a significant gap that would otherwise have disadvantaged more book‐oriented RAE submissions within a specific UoA and because the combination of the different sources would give 143 to 318% more citation “evidence” than would Scopus citations alone.

Finally, a future practical step would be developing and assessing methods for the automatic submission and checking of most or all U.K. research outputs in the social sciences and arts & humanities with the Google Books application programming interface ( http://code.google.com/apis/books ). This would help to avoid the need for manual searching and checking huge numbers of submissions. This approach could systematically create citing and cited associations between books based on extracting both bibliographic information and the cited references of books indexed by Google Books, and would be a useful resource in itself.
Acknowledgements

We thank the Leverhulme Trust for funding this research. This article was adapted and extended from Kousha, K., & Thelwall, M. (2011, July). Assessing the Citation Impact of Book‐Based Disciplines: The Role of Google Books, Google Scholar and Scopus in the UK RAE. Paper presented at the 13th International Conference Society of Scientometrics and Informetrics Conference, Durban, South Africa.

Appendix
Table A1. Number and percentage for the main types of 2008 Research Assessment Exercise (RAE) submissions to the 67 units of assessment.
	Authored book 	Edited book 	Chapter in book 	All types of book sources a 	Journal article 	Conference 		
RAE 2008 UoA 	n 	% 	n 	% 	n 	% 	n 	% 	n 	% 	n 	% 	Total a 	Total no. of submissions b
1. Cardiovascular Medicine 	0 	0 	0 	0 	0 	0 	0 	0.00 	1,496 	98.42 	0 	0 	1,496 	1,520
2. Cancer Studies 	0 	0 	0 	0 	0 	0 	0 	0.00 	3,002 	98.56 	0 	0 	3,002 	3,046
3. Infection and Immunology 	1 	0.04 	0 	0 	1 	0.04 	2 	0.07 	2,671 	98.6 	0 	0 	2,673 	2,709
4. Other Hospital‐Based Clinical Subjects 	1 	0.01 	0 	0 	1 	0.01 	2 	0.03 	6,673 	98.74 	1 	0.01 	6,676 	6,758
5. Other Laboratory‐Based Clinical Subjects 	0 	0 	0 	0 	0 	0 	0 	0.00 	1,092 	98.73 	0 	0 	1,092 	1,106
6. Epidemiology and Public Health 	5 	0.21 	1 	0.04 	4 	0.17 	10 	0.41 	2,322 	95.99 	0 	0 	2,332 	2,419
7. Health Services Research 	11 	0.5 	2 	0.09 	20 	0.91 	33 	1.50 	2,060 	93.42 	2 	0.09 	2,095 	2,205
8. Primary Care and Other Community‐Based Clinical Subjects 	3 	0.45 	0 	0 	3 	0.45 	6 	0.90 	649 	96.87 	0 	0 	655 	670
9. Psychiatry, Neuroscience, and Clinical Psychology 	5 	0.15 	0 	0 	0 	0 	5 	0.15 	3,305 	98.54 	0 	0 	3,310 	3,354
10. Dentistry 	0 	0 	0 	0 	1 	0.06 	1 	0.06 	1,616 	97.12 	0 	0 	1,617 	1,664
11. Nursing and Midwifery 	38 	1.34 	9 	0.32 	51 	1.8 	98 	3.45 	2,582 	90.92 	3 	0.11 	2,683 	2,840
12. Allied Health Professions and Studies 	34 	0.55 	14 	0.23 	89 	1.44 	137 	2.21 	5,812 	93.82 	18 	0.29 	5,967 	6,195
13. Pharmacy 	2 	0.11 	0 	0 	3 	0.16 	5 	0.27 	1,780 	96.63 	1 	0.05 	1,786 	1,842
14. Biological Sciences 	15 	0.15 	1 	0.01 	9 	0.09 	25 	0.26 	9,587 	98 	4 	0.04 	9,616 	9,783
15. Preclinical and Human Biological Sciences 	9 	0.37 	1 	0.04 	19 	0.78 	29 	1.19 	2,374 	97.06 	0 	0 	2,403 	2,446
16. Agriculture, Veterinary, and Food Science 	6 	0.14 	1 	0.02 	16 	0.38 	23 	0.55 	4,081 	97.14 	4 	0.1 	4,108 	4,201
17. Earth Systems and Environmental Sciences 	29 	0.57 	0 	0 	118 	2.33 	147 	2.90 	4,857 	95.76 	8 	0.16 	5,012 	5,072
18. Chemistry 	4 	0.08 	0 	0 	4 	0.08 	8 	0.16 	4,858 	98.9 	0 	0 	4,866 	4,912
19. Physics 	9 	0.13 	0 	0 	10 	0.14 	19 	0.27 	7,022 	98.22 	22 	0.31 	7,063 	7,149
20. Pure Mathematics 	54 	1.96 	0 	0 	65 	2.35 	119 	4.31 	2,344 	84.87 	81 	2.93 	2,544 	2,762
21. Applied Mathematics 	24 	0.69 	0 	0 	23 	0.66 	47 	1.36 	3,315 	95.73 	9 	0.26 	3,371 	3,463
22. Statistics and Operational Research 	24 	1.62 	1 	0.07 	14 	0.95 	39 	2.63 	1,334 	90.07 	5 	0.34 	1,378 	1,481
23. Computer Science and Informatics 	75 	1 	9 	0.12 	199 	2.66 	283 	3.78 	4,966 	66.35 	1,989 	26.57 	7,238 	7,485
24. Electrical and Electronic Engineering 	8 	0.24 	0 	0 	9 	0.27 	17 	0.51 	3,244 	96.95 	53 	1.58 	3,314 	3,346
25. General Engineering and Mineral & Mining Engineering 	34 	0.58 	17 	0.29 	75 	1.28 	126 	2.15 	5,465 	93.29 	156 	2.66 	5,747 	5,858
26. Chemical Engineering 	3 	0.36 	0 	0 	3 	0.36 	6 	0.73 	800 	96.97 	8 	0.97 	814 	825
27. Civil Engineering 	12 	0.58 	0 	0 	30 	1.46 	42 	2.04 	1,928 	93.59 	47 	2.28 	2,017 	2,060
28. Mechanical, Aeronautical, and Manufacturing Engineering 	2 	0.05 	1 	0.02 	14 	0.34 	17 	0.42 	3,987 	97.65 	34 	0.83 	4,038 	4,083
29. Metallurgy and Materials 	2 	0.13 	2 	0.13 	4 	0.25 	8 	0.51 	1,510 	95.45 	22 	1.39 	1,540 	1,582
30. Architecture and the Built Environment 	174 	6.58 	51 	1.93 	240 	9.08 	465 	17.59 	1,702 	64.37 	220 	8.32 	2,387 	2,644
31. Town and Country Planning 	105 	6.15 	22 	1.29 	112 	6.56 	239 	14.00 	1,379 	80.79 	19 	1.11 	1,637 	1,707
32. Geography and Environmental Studies 	214 	4.67 	18 	0.39 	243 	5.3 	475 	10.36 	3,951 	86.19 	12 	0.26 	4,438 	4,584
33. Archaeology 	376 	17.55 	145 	6.77 	627 	29.27 	1,148 	53.59 	901 	42.06 	39 	1.82 	2,088 	2,142
34. Economics and Econometrics 	21 	0.7 	6 	0.2 	55 	1.82 	82 	2.71 	2,566 	84.94 	3 	0.1 	2,651 	3,021
35. Accounting and Finance 	16 	2.81 	0 	0 	14 	2.46 	30 	5.27 	490 	86.12 	2 	0.35 	522 	569
36. Business and Management Studies 	285 	2.27 	60 	0.48 	332 	2.64 	677 	5.38 	11,373 	90.44 	85 	0.68 	12,135 	12,575
37. Library and Information Management 	31 	2.57 	9 	0.75 	103 	8.53 	143 	11.85 	854 	70.75 	118 	9.78 	1,115 	1,207
38. Law 	996 	15.93 	23 	0.37 	1,330 	21.27 	2,349 	37.56 	3,691 	59.02 	4 	0.06 	6,044 	6,254
39. Politics and International Studies 	1,028 	21.84 	91 	1.93 	655 	13.91 	1,774 	37.68 	2,831 	60.13 	9 	0.19 	4,614 	4,708
40. Social Work and Social Policy & Administration 	618 	11.74 	141 	2.68 	624 	11.85 	1,383 	26.26 	3,515 	66.75 	7 	0.13 	4,905 	5,266
41. Sociology 	619 	16.6 	96 	2.57 	526 	14.11 	1,241 	33.28 	2,366 	63.45 	4 	0.11 	3,611 	3,729
42. Anthropology 	181 	14.45 	104 	8.3 	292 	23.3 	577 	46.05 	636 	50.76 	2 	0.16 	1,215 	1,253
43. Development Studies 	73 	8.95 	37 	4.53 	97 	11.89 	207 	25.37 	571 	69.98 	2 	0.25 	780 	816
44. Psychology 	28 	0.42 	6 	0.09 	56 	0.83 	90 	1.34 	6,472 	96.4 	9 	0.13 	6,571 	6,714
45. Education 	732 	10.25 	54 	0.76 	938 	13.14 	1,724 	24.15 	4,898 	68.6 	72 	1.01 	6,694 	7,140
46. Sports‐Related Studies 	38 	1.89 	3 	0.15 	60 	2.98 	101 	5.02 	1,831 	90.96 	6 	0.3 	1,938 	2,013
47. American Studies and Anglophone Area Studies 	71 	20.94 	20 	5.9 	94 	27.73 	185 	54.57 	139 	41 	1 	0.29 	325 	339
48. Middle Eastern and African Studies 	153 	23.29 	35 	5.33 	231 	35.16 	419 	63.77 	219 	33.33 	1 	0.15 	639 	657
49. Asian Studies 	111 	18.2 	24 	3.93 	162 	26.56 	297 	48.69 	292 	47.87 	5 	0.82 	594 	610
50. European Studies 	341 	18.8 	89 	4.91 	425 	23.43 	855 	47.13 	902 	49.72 	5 	0.28 	1,762 	1,814
51. Russian, Slavonic, and East European Languages 	75 	16.06 	33 	7.07 	118 	25.27 	226 	48.39 	219 	46.9 	1 	0.21 	446 	467
52. French 	295 	19.13 	82 	5.32 	450 	29.18 	827 	53.63 	623 	40.4 	3 	0.19 	1,453 	1,542
53. German, Dutch, and Scandinavian Languages 	167 	19.09 	65 	7.43 	326 	37.26 	558 	63.77 	281 	32.11 	3 	0.34 	842 	875
54. Italian 	73 	18.77 	18 	4.63 	116 	29.82 	207 	53.21 	156 	40.1 	4 	1.03 	367 	389
55. Iberian and Latin American Languages 	196 	21.4 	44 	4.8 	249 	27.18 	489 	53.38 	378 	41.27 	13 	1.42 	880 	916
56. Celtic Studies 	108 	25.9 	26 	6.24 	124 	29.74 	258 	61.87 	120 	28.78 	7 	1.68 	385 	417
57. English Language and Literature 	1,905 	25.64 	495 	6.66 	2,118 	28.5 	4,518 	60.80 	2,304 	31.01 	23 	0.31 	6,845 	7,431
58. Linguistics 	117 	9.98 	22 	1.88 	256 	21.84 	395 	33.70 	687 	58.62 	45 	3.84 	1,127 	1,172
59. Classics, Ancient History, Byzantine, and Modern Greek Studies 	329 	19.86 	92 	5.55 	668 	40.31 	1,089 	65.72 	460 	27.76 	12 	0.72 	1,561 	1,657
60. Philosophy 	326 	15.65 	31 	1.49 	464 	22.28 	821 	39.41 	1,132 	54.34 	2 	0.1 	1,955 	2,083
61. Theology, Divinity, and Religious Studies 	573 	28.82 	106 	5.33 	680 	34.21 	1,359 	68.36 	579 	29.12 	8 	0.4 	1,946 	1,988
62. History 	1,665 	23.93 	386 	5.55 	2,063 	29.65 	4,114 	59.12 	2,510 	36.07 	12 	0.17 	6,636 	6,959
63. Art and Design 	572 	7.21 	190 	2.4 	772 	9.74 	1,534 	19.35 	1,131 	14.26 	652 	8.22 	3,317 	7,929
64. History of Art, Architecture, and Design 	298 	21.83 	84 	6.15 	451 	33.04 	833 	61.03 	415 	30.4 	13 	0.95 	1,261 	1,365
65. Drama, Dance, and Performing Arts 	274 	15.35 	85 	4.76 	393 	22.02 	752 	42.13 	480 	26.89 	24 	1.34 	1,256 	1,785
66. Communication, Cultural, and Media Studies 	410 	18.83 	72 	3.31 	581 	26.69 	1,063 	48.83 	916 	42.08 	8 	0.37 	1,987 	2,177
67. Music 	201 	7.92 	50 	1.97 	441 	17.37 	692 	27.25 	561 	22.1 	55 	2.17 	1,308 	2,539
Total (%) 	14,205 	6.63 	2,974 	1.39 	18,271 	8.53 	35,450 	16.54 	161,263 	75.25 	3,977 	1.86 	200,690 	214,289

    Note . UoA = unit of assessment. Selected subject areas (UoAs) boldface.
    a a Total number of book sources including authored books, edited books, and chapter in books.
    b b Total number of all submitted types of research outputs, including patents, software, research reports, Internet publication, and so on in the 2008 RAE.

Table A2. Top‐five highly cited books in 2008 Research Assessment Exercise based on Google Books Searches (as of November 2010).
UoA 	First author 	Book titles 	Google Books n (% of Scopus) 	Google Scholar n (% of Scopus) 	Scopus n
Law 	Douglas‐Scott 	Constitutional Law of the 	64 	51 	6
		European Union 	(1066.7) 	(850) 	
	Craig 	EU Law, Text, Cases and Materials (4th ed.) 	51 	89 	29
			(175.9) 	(306.9) 	
	Kretzmer 	The Occupation of Justice: The Supreme Court of Israel and the Occupied Territories 	50 	110 	20
			(250.0) 	(550) 	
	Hood 	The Death Penalty: A World‐Wide Perspective (3rd ed.) 	47 	182 	20
			(235.0) 	(910) 	
	Honore 	Ulpian: Pioneer of Human Rights 	45 	11 	8
			(562.5) 	(137.5) 	
History 	Strachan 	The First World War: To Arms (Vol. 1) 	213 	104 	33
			(645.5) 	(315.2) 	
	Hobsbawm 	Interesting Times: A Twentieth‐Century Life 	117 	99 	32
			(365.6) 	(309.4) 	
	Malcolm 	Aspects of Hobbes 	102 	80 	42
			(242.9) 	(190.5) 	
	Rodger 	The Command of the Ocean: A Naval History of Britain, 1649–1815 	93 	70 	26
			(357.7) 	(269.2) 	
	MacMillan 	Peacemakers: The Paris Conference of 1919 and Its Attempt to End War 	89 	73 	23
			(387.0) 	(317.4) 	
Sociology 	Sassen 	Territory, Authority and Rights: From Medieval to Global Assemblages 	198 	437 	169
			(117.2) 	(258.6) 	
	Amin 	Cities: Reimagining the Urban 	194 	728 	273
			(71.1) 	(266.7) 	
	Sklair 	Globalization: Capitalism and Its Alternatives 	185 	354 	94
			(196.8) 	(376.6) 	
	Lash 	Critique of Information 	178 	460 	129
			(138.0) 	(356.6) 	
	Mann 	The Dark‐Side of Democracy: Explaining Ethnic Cleansing 	173 	326 	141
			(122.7) 	(231.2) 	
Philosophy 	Clark 	Natural‐Born Cyborgs: Minds, Technologies, and the Future of Human Intelligence 	132 	497 	123
			(107.3) 	(404.1) 	
	Dancy 	Ethics Without Principles 	100 	289 	98
			(102.0) 	(294.9) 	
	Papineau 	Thinking About Consciousness 	83 	249 	88
			(94.3) 	(283.0) 	
	Hallward 	Badiou: A Subject to Truth 	82 	159 	55
			(149.1) 	(289.1) 	
	Hallward 	Absolutely Postcolonial: Writing Between the Singular and the Specific 	64 	96 	29
			(220.7) 	(331.0) 	
Archeology 	Cummings 	Places of Special Virtue: Megaliths in the Neolithic Landscapes of Wales 	65 	36 	21
			(309.5) 	(171.4) 	
	Shennan 	Genes, Memes and Human History: Darwinian Archaeology and Cultural Evolution 	60 	234 	93
			(64.5) 	(251.6) 	
	Wheatley 	Spatial Technology and Archaeology: The Archaeological Applications of GIS 	49 	147 	36
			(136.1) 	(408.3) 	
	Gosden 	Archaeology and Colonialism: Cultural Contact From 5000 BC to the Present 	56 	103 	48
			(116.7) 	(214.6) 	
	Fowler 	The Archaeology of Personhood: An Anthropological Approach 	41 	100 	36
			(113.9) 	(277.8) 	
Politics and international studies 	Putnam 	Better Together: Restoring the American Community 	180 	470 	131
			(137.4) 	(358.8) 	
	Acharya 	Constructing a Security Community in Southeast Asia: ASEAN and the Problem of Regional Order 	170 	392 	93
			(182.8) 	(421.5) 	
	Buzan 	Regions and Powers: The Structure of International Security 	170 	620 	105
			(161.9) 	(590.5) 	
	Gray 	Al Qaeda and What it Means to Be Modern 	145 	189 	33
			(439.4) 	(572.7) 	
	Laclau 	On Populist Reason 	128 	410 	124
			(103.2) 	(330.6) 	
Communication, cultural, and media studies 	Ahmed 	The Cultural Politics of Emotion 	132 	501 	232
			(56.9) 	(215.9) 	
	Lister 	New Media: A Critical Introduction 	115 	270 	52
			(221.2) 	(519.2) 	
	Critcher 	Moral Panics and the Media 	72 	14 	25
			(288.0) 	(56) 	
	Newman 	Videogames 	59 	162 	39
			(151.3) 	(415.4) 	
	Tumber 	Media at War: The Iraq Crisis 	52 	126 	38
			(136.8) 	(331.6) 	
Total 			3,754 	8,238 	2,544
			(174.6) 	(323.8) 	

    UoA=unit of assessment.

Table A3. Statistics for Google Books citations and average 2008 Research Assessment Exercise (RAE) ranking score in communication, cultural, and media studies for 45 U.K. institutions.
University name 	Google Books citation M 	Google Books citation Mdn 	No. of books submitted 	Total citations 	RAE average ranking
University of Leicester 	8.5 	5.5 	10 	85 	3.600
University of Westminster 	21.75 	14.5 	20 	435 	3.500
University of East Anglia 	22.12 	15.5 	8 	177 	3.400
Goldsmiths College, University of London 	30.52 	21 	23 	702 	3.200
London School of Economics and Political Science 	38.66 	29.5 	6 	232 	3.200
Cardiff University 	24.76 	14 	17 	421 	3.150
University of East London 	21.53 	10 	13 	280 	2.950
Royal Holloway, University of London 	12 	12.5 	120 	95 	2.900
University of Sussex 	14.66 	11 	15 	220 	2.900
Nottingham Trent University 	10.25 	7.5 	16 	164 	2.800
University of Ulster 	11 	3 	9 	99 	2.800
University of Lincoln 	14 	12.5 	4 	56 	2.750
University of Sunderland 	7.67 	5.5 	16 	123 	2.750
University of Stirling 	15.92 	7 	13 	207 	2.750
De Montfort University 	6.4 	5 	9 	58 	2.700
University of the West of England, Bristol 	25.93 	13.5 	16 	415 	2.650
University of Leeds 	9.91 	7 	12 	119 	2.650
University of Oxford 	21.6 	13 	5 	108 	2.650
University of Bedfordshire 	21.62 	20 	8 	173 	2.550
Leeds Metropolitan University 	5.68 	4 	19 	108 	2.550
University of Salford 	10.23 	6 	13 	133 	2.550
Bournemouth University 	12.44 	7 	9 	122 	2.500
University of Derby 	9.77 	9 	9 	88 	2.450
Queen Margaret University Edinburgh 	30 	30 	2 	60 	2.450
Sheffield Hallam University 	3.42 	3 	7 	24 	2.400
London South Bank University 	5.25 	3 	4 	21 	2.400
Brunel University 	21.5 	16 	12 	258 	2.400
Roehampton University 	11.07 	3 	13 	144 	2.350
London Metropolitan University 	22.71 	20 	7 	159 	2.350
Glasgow Caledonian University 	5.5 	5.5 	2 	11 	2.300
University of Nottingham 	12.87 	5.5 	8 	103 	2.300
University of Winchester 	9.46 	3 	13 	123 	2.250
University of Brighton 	8.16 	8.5 	6 	49 	2.200
University of Central Lancashire 	10.14 	8 	7 	71 	2.200
University of Sheffield 	22.85 	20 	7 	160 	2.200
Middlesex University 	5 	6 	3 	15 	2.150
University of Glamorgan 	19.25 	5 	4 	77 	2.150
Swansea University 	16.5 	11.5 	8 	132 	2.050
University of the West of Scotland 	13.33 	14 	3 	40 	1.900
Staffordshire University 	14.14 	13 	7 	99 	1.850
Bath Spa University 	22.5 	22.5 	2 	45 	1.800
University of Greenwich 	6.66 	0 	3 	20 	1.650
Thames Valley University 	3.5 	3.5 	2 	7 	1.650
University of Bradford 	3 	3 	1 	3 	1.650
University of Huddersfield 	2 	0 	5 	10 	1.000
References

    Archambault, É. , Campbell, D. , Gingras, Y. , & Larivière, V. ( 2009 ). Comparing bibliometric statistics obtained from the Web of Science and Scopus . Journal of the American Society for Information Science and Technology , 60 ( 7 ), 1320 – 1326 .
    Wiley Online Library Web of Science® Google Scholar
    Ball, R. , & Tunger, D. ( 2006 ). Science indicators revisited—Science Citation Index versus SCOPUS: A bibliometric comparison of both citation databases . Information Services & Use , 26 ( 4 ), 293 – 301 .
    Crossref CAS Google Scholar
    Bar‐Ilan, J. ( 2008 ). Which h‐index? A comparison of WoS, Scopus and Google Scholar . Scientometrics , 74 ( 2 ), 257 – 271 .
    Crossref CAS Web of Science® Google Scholar
    Bar‐Ilan, J. ( 2010 ). Citations to the “Introduction to informetrics” indexed by WOS, Scopus and Google Scholar . Scientometrics , 82 ( 3 ), 495 – 506 .
    Crossref CAS Web of Science® Google Scholar
    Bence, V. , & Oppenheim, C. ( 2004 ). The influence of peer review on the research assessment exercise . Journal of Information Science , 30 ( 4 ), 347 – 368 .
    Crossref Web of Science® Google Scholar
    Bornmann, L. , & Daniel, H.‐D. ( 2006 ). Selecting scientific excellence through committee peer review—A citation analysis of publications previously published to approval or rejection of post‐doctoral research fellowship applicants . Scientometrics , 68 ( 3 ), 427 – 440 .
    Crossref CAS Web of Science® Google Scholar
    Bornmann, L. , & Daniel, H.‐D. ( 2008 ). What do citation counts measure? A review of studies on citing behavior . Journal of Documentation , 64 ( 1 ), 45 – 80 .
    Crossref Web of Science® Google Scholar
    Bornmann, L. , Marx, W. , Schier, H. , Rahm, E. , Thor, A. , & Daniel, H.‐D. ( 2009 ). Convergent validity of bibliometric Google Scholar data in the field of chemistry citation counts for papers that were accepted by Angewandte Chemie International Edition or rejected but published elsewhere, using Google Scholar, Science Citation Index, Scopus, and Chemical Abstracts . Journal of Informetrics , 3 ( 1 ), 27 – 35 .
    Crossref Web of Science® Google Scholar
    Brody, T. , Harnad, S. , & Carr, L. ( 2006 ). Earlier Web usage statistics as predictors of later citation impact . Journal of the American Association for Information Science and Technology , 57 ( 8 ), 1060 – 1072 .
    Wiley Online Library Web of Science® Google Scholar
    Butler, L. , & Visser, M. ( 2006 ). Extending citation analysis to non‐source items . Scientometrics , 66 ( 2 ), 327 – 343 .
    Crossref CAS Web of Science® Google Scholar
    Chung, Y. ( 1995 ). Characteristics of references in international classification systems literature . Library Quarterly , 65 ( 2 ), 200 – 215 .
    Crossref Web of Science® Google Scholar
    Cronin, B. , Snyder, H. , & Atkins, H. ( 1997 ). Comparative citation rankings of authors in monographic and journal literature: A study of sociology . Journal of Documentation , 53 ( 3 ), 263 – 273 .
    Crossref Web of Science® Google Scholar
    Franceschet, M. ( 2010 ). A comparison of bibliometric indicators for computer science scholars and journals on Web of Science and Google Scholar . Scientometrics , 83 ( 1 ), 243 – 258 .
    Crossref CAS Web of Science® Google Scholar
    Garfield, E. ( 1996, October ). Citation indexes for retrieval and research evaluation . Paper presented at the G7 Consensus Conference on the Theory and Practice of Research Assessment, Capri, Italy.
    Google Scholar
    Gavel, Y. , & Iselid, L. ( 2008 ). Web of Science and Scopus: A journal title overlap study . Online Information Review , 32 ( 1 ), 8 – 21 .
    Crossref Web of Science® Google Scholar
    Giménez‐Toledo, E. , & Román, A. ( 2008, September ). Peer review and in‐depth interviews with publishers as a means of assessing quality of research monographs . Poster presented at the 10th International Conference on Science and Technology Indicators, Vienna (Austria). Retrieved from http://eprints.rclis.org/handle/10760/12337
    Google Scholar
    Glänzel, W. , & Schoepflin, U. ( 1999 ). A bibliometric study of reference literature in the sciences and social sciences . Information Processing & Management , 35 ( 1 ), 31 – 44 .
    Crossref Web of Science® Google Scholar
    Harzing, A.‐W. , & van der Wal, R. ( 2009 ). A Google Scholar h‐index for journals: An alternative metric to measure journal impact in economics and business . Journal of the American Society for Information Science and Technology , 60 ( 1 ), 41 – 46 .
    Wiley Online Library Web of Science® Google Scholar
    Higher Education Funding Council for England . ( 2009a ). Report on the pilot exercise to develop bibliometric indicators for the Research Excellence Framework . Retrieved from http://www.hefce.ac.uk/pubs/hefce/2009/09_39/09_39.pdf
    Google Scholar
    Higher Education Funding Council for England . ( 2009b ). The Research Excellence Framework: A brief guide to the proposals . Retrieved from http://www.hefce.ac.uk/research/ref/resources/REFguide.pdf
    Google Scholar
    Hicks, D. ( 1999 ). The difficulty of achieving full coverage of international social science literature and the bibliometric consequences . Scientometrics , 44 ( 2 ), 193 – 215 .
    Crossref Web of Science® Google Scholar
    Hicks, D. ( 2004 ). The four literatures of social science . In H. Moed (Ed.), Handbook of quantitative science and technology research (pp. 476 – 496 ). Dordrecht, The Netherlands : Kluwer.
    Google Scholar
    Holmes, A. , & Oppenheim, C. ( 2001 ). Use of citation analysis to predict the outcome of the 2001 Research Assessment Exercise for Unit of Assessment (UoA) 61: Library and Information Management . Information Research , 6 ( 2 ). Retrieved from http://informationr.net/ir/6‐2/paper103.html
    Web of Science® Google Scholar
    Huang, M. , & Chang, Y. ( 2008 ). Characteristics of research output in social sciences and humanities: From a research evaluation perspective . Journal of the American Society for Information Science and Technology , 59 ( 11 ), 1819 – 1828 .
    Wiley Online Library Web of Science® Google Scholar
    Jacsó, P. ( 2005a ). Google Scholar: The pros and the cons . Online Information Review , 29 ( 2 ), 208 – 214 .
    Crossref Web of Science® Google Scholar
    Jacsó, P. ( 2005b ). As we may search: Comparison of major features of the Web of Science, Scopus, and Google Scholar citation‐based and citation‐enhanced databases . Current Science , 89 ( 9 ), 1537 – 1547 . Retrieved from http://www.ias.ac.in/currsci/nov102005/1537.pdf
    Web of Science® Google Scholar
    Korevaar, J.C. , & Moed, H.F. ( 1996 ). Validation of bibliometric indicators in the field of mathematics . Scientometrics , 37 ( 1 ), 117 – 130 .
    Crossref Web of Science® Google Scholar
    Kousha, K. , & Thelwall, M. ( 2007a ). Google Scholar citations and Google Web/URL citations: A multi‐discipline exploratory analysis . Journal of the American Society for Information Science and Technology , 58 ( 7 ), 1055 – 1065 .
    Wiley Online Library Web of Science® Google Scholar
    Kousha, K. , & Thelwall, M. ( 2007b ). The Web impact of open‐access social science research . Library and Information Science Research , 29 ( 4 ), 495 – 507 .
    Crossref Web of Science® Google Scholar
    Kousha, K. , & Thelwall, M. ( 2008 ). Sources of Google Scholar citations outside the Science Citation Index: A comparison between four science disciplines . Scientometrics , 74 ( 2 ), 273 – 294 .
    Crossref CAS Web of Science® Google Scholar
    Kousha, K. , & Thelwall, M. ( 2009 ). Google Book search: Citation analysis for social science and the humanities . Journal of the American Society for Information Science and Technology , 60 ( 8 ), 1537 – 1549 .
    Wiley Online Library Web of Science® Google Scholar
    Kousha, K. , Thelwall, M. , & Rezaie, S. ( 2010 ). Using the web for research evaluation: The Integrated Online Impact indicator . Journal of Informetrics , 4 ( 1 ), 124 – 135 .
    Crossref Web of Science® Google Scholar
    Li, J. , Sanderson, M. , Willett, P. , Norris, M. , & Oppenheim, C. ( 2010 ). Ranking of library and information science researchers: Comparison of data sources for correlating citation data, and expert judgments . Journal of Informetrics , 4 ( 4 ), 554 – 563 .
    Crossref Web of Science® Google Scholar
    Li, X. , Thelwall, M. , Musgrove, P. , & Wilkinson, D. ( 2003 ). The relationship between the links/Web Impact Factors of computer science departments in UK and their RAE (Research Assessment Exercise) ranking in 2001 . Scientometrics , 57 ( 2 ), 239 – 255 .
    Crossref Web of Science® Google Scholar
    MacRoberts, M. , & MacRoberts, B. ( 1996 ). Problems of citation analysis . Scientometrics , 36 ( 3 ), 435 – 444 .
    Crossref Web of Science® Google Scholar
    MacRoberts, M.H. , & MacRoberts, B.R. ( 2010 ). Problems of citation analysis: A study of uncited and seldom‐cited influences . Journal of the American Society for Information Science and Technology , 61 ( 1 ), 1 – 13 .
    Wiley Online Library Web of Science® Google Scholar
    Mahdi, S. , D'Este, P. , & Neely, A. ( 2008 ). Citation counts: Are they good predictors of RAE scores ? London, England : Advanced Institute of Management Research. Retrieved from http://ssrn.com/abstract=1154053
    Google Scholar
    Meho, L.I. , & Yang, K. ( 2007 ). Impact of data sources on citation counts and rankings of LIS faculty: Web of Science vs. Scopus and Google Scholar . Journal of the American Society for Information Science and Technology , 58 ( 13 ), 2105 – 2125 .
    Wiley Online Library Web of Science® Google Scholar
    Mingers, J. , & Lipitakis, E. ( 2010 ). Counting the citations: A comparison of Web of Science and Google Scholar in the field of business and management . Scientometrics , 85 ( 2 ), 613 – 625 .
    Crossref Web of Science® Google Scholar
    Moed, H. ( 2005 ). Citation analysis in research evaluation . New York : Springer.
    Google Scholar
    Moed, H. , Luwel, M. , & Nederhof, A. ( 2001 ). Towards research performance in the humanities . Library Trends , 50 ( 3 ), 498 – 520 .
    Web of Science® Google Scholar
    Nederhof, A. ( 2006 ). Bibliometric monitoring of research performance in the social sciences and the humanities: A review . Scientometrics , 66 ( 1 ), 81 – 100 .
    Crossref Web of Science® Google Scholar
    Nederhof, A. , & van Raan, A. ( 1993 ). A bibliometric analysis of six economics research groups: A comparison with peer review . Research Policy , 22 ( 4 ), 353 – 368 .
    Crossref Web of Science® Google Scholar
    Norris, M. , & Oppenheim, C. ( 2003 ). Citation counts and the Research Assessment Exercise V: Archaeology and the 2001 RAE . Journal of Documentation , 59 ( 6 ), 709 – 730 .
    Crossref Web of Science® Google Scholar
    Norris, M. , & Oppenheim, C. ( 2010 ). Peer review and the h‐index: Two studies . Journal of Informetrics , 4 ( 3 ), 221 – 232 .
    Crossref Web of Science® Google Scholar
    Oppenheim, C. ( 1995 ). The correlation between citation counts and the 1992 Research Assessment Exercise ratings for British library and information science university departments . Journal of Documentation , 51 ( 1 ), 18 – 27 .
    Crossref Web of Science® Google Scholar
    Oppenheim, C. ( 1996 ). Do citations count? Citation indexing and the Research Assessment Exercise (RAE) . Serials , 9 ( 2 ), 151 – 161 .
    Crossref Google Scholar
    Oppenheim, C. ( 1997 ). The correlation between citation counts and the 1992 Research Assessment Exercise ratings for British research in genetics, anatomy and archaeology . Journal of Documentation , 53 ( 5 ), 477 – 487 .
    Crossref Web of Science® Google Scholar
    Oppenheim, C. , & Summers, M. ( 2008 ). Citation counts and the Research Assessment Exercise, Part VI: Unit of assessment 67 (music) . Information Research , 13 ( 2 ). Retrieved from http://InformationR.net/ir/13hyphen;2/paper342.html
    Web of Science® Google Scholar
    RAE 2008 Guidance on Submissions . ( 2005 ). Retrieved from http://www.rae.ac.uk/pubs/2005/03/rae0305.pdf
    Google Scholar
    RAE 2008 Panels (n.d.). Retrieved from http://www.rae.ac.uk/aboutus/panels.asp
    Google Scholar
    Rinia, E.J. , van Leeuwen, T.N. , van Vuren, H.G. , & van Raan, A.F.J. ( 1998 ). Comparative analysis of a set of bibliometric indicators and central peer review criteria: Evaluation of condensed matter physics in the Netherlands . Research Policy , 27 ( 1 ), 95 – 107 .
    Crossref Web of Science® Google Scholar
    Scopus Content Coverage Guide . ( 2010 ). Retrieved from http://www.info.sciverse.com/documents/files/scopus‐training/resourcelibrary/pdf/sccg0510.pdf
    Google Scholar
    Seng, L.B. , & Willett, P. ( 1995 ). The citedness of publications by United Kingdom library schools . Journal of Information Science , 21 ( 1 ), 68 – 71 .
    Crossref Web of Science® Google Scholar
    Shaw, D. , & Vaughan, L. ( 2008 ). A new look at evidence of scholarly citation in citation indexes and from web sources . Scientometrics , 74 ( 2 ), 317 – 330 .
    Crossref Web of Science® Google Scholar
    Small, H. , & Crane, D. ( 1979 ). Specialties and disciplines in science and social science: An examination of their structure using citation indexes . Scientometrics , 1 ( 5–6 ), 445 – 461 .
    Crossref Web of Science® Google Scholar
    Smith, A. , & Eysenck, M. ( 2002, June ). The correlation between RAE ratings and citation counts in psychology [Unpublished Tech. Rep.] . University of London. Retrieved from: http://cogprints.org/2749/1/citations.pdf
    Google Scholar
    Smith, A.G. ( 2008 ). Benchmarking Google Scholar with the New Zealand PBRF research assessment exercise . Scientometrics , 74 ( 2 ), 309 – 316 .
    Crossref Web of Science® Google Scholar
    Tang, R. ( 2008 ). Citation characteristics and intellectual acceptance of scholarly monographs . College & Research Libraries , 69 ( 4 ), 356 – 369 .
    Crossref Web of Science® Google Scholar
    Taylor, J. ( 2011 ). The assessment of research quality: Peer review or metrics ? British Journal of Management , 22 ( 2 ), 202 – 217 .
    Wiley Online Library Web of Science® Google Scholar
    Taylor, J. , & Walker, I. ( 2009 ). Peer assessment of research: How many publications per staff ? Lancaster University Management School, Working Paper No. 2009/035. Retrieved from http://eprints.lancs.ac.uk/48977/1/Document.pdf
    Google Scholar
    Thompson, J. ( 2002 ). The death of the scholarly monograph in the humanities? Citation patterns in literary scholarship . Libri , 52 , 121 – 136 .
    Crossref Web of Science® Google Scholar
    van Raan, A.F.J. ( 2005 ). Fatal attraction: Conceptual and methodological problems in the ranking of universities by bibliometric methods . Scientometrics , 62 ( 1 ), 133 – 143 .
    Crossref CAS Web of Science® Google Scholar
    van Raan, A.F.J. ( 2006 ). Comparison of Hirsch‐index with standard bibliometric indicators and with peer judgment for 147 chemistry research groups . Scientometrics , 67 ( 3 ), 491 – 502 .
    Crossref CAS Web of Science® Google Scholar
    Vaughan, L. , & Shaw, D. ( 2005 ). Web citation data for impact assessment: A comparison of four science disciplines . Journal of the American Society for Information Science and Technology , 56 ( 10 ), 1075 – 1087 .
    Wiley Online Library Web of Science® Google Scholar
    Warner, J. ( 2000 ). A critical review of the application of citation studies to the Research Assessment Exercises . Journal of Information Science , 26 ( 6 ), 453 – 459 .
    Crossref Web of Science® Google Scholar
    Weingart, P. ( 2005 ). Impact of bibliometrics upon the science system: Inadvertent consequences ? Scientometrics , 62 ( 1 ), 117 – 131 .
    Crossref CAS Web of Science® Google Scholar
    White, H.D. , Boell, S.K. , Yu, H. , Davis, M. , Wilson, C.S. , & Cole, F.T. ( 2009 ). Libcitations: A measure for comparative assessment of book publications in the humanities and social sciences . Journal of the American Society for Information Science and Technology , 60 ( 6 ), 1083 – 1096 .
    Wiley Online Library Web of Science® Google Scholar

Citing Literature
Number of times cited according to CrossRef: 68

    Qingqing Zhou, Chengzhi Zhang, Measuring book impact via content-level academic review mining, The Electronic Library, 10.1108/EL-08-2019-0184, ahead-of-print , ahead-of-print, (2020).
    Crossref
    Elea Giménez Toledo, Why Books are Important in the Scholarly Communication System in Social Sciences and Humanities, Scholarly Assessment Reports, 10.29024/sar.14, 2 , 1, (2020).
    Crossref
    Juan Gorraiz, Martin Wieland, Ursula Ulrych, Christian Gumpenberger, De Profundis: A Decade of Bibliometric Services Under Scrutiny, Evaluative Informetrics: The Art of Metrics-Based Research Assessment, 10.1007/978-3-030-47665-6, (233-260), (2020).
    Crossref
    Cristina Figueroa-Domecq, Anna de Jong, Allan M. Williams, Gender, tourism & entrepreneurship: A critical review, Annals of Tourism Research, 10.1016/j.annals.2020.102980, 84 , (102980), (2020).
    Crossref
    Pei-Shan Chi, The field-specific citation and usage patterns of book literature in the Book Citation Index, Research Evaluation, 10.1093/reseval/rvz037, (2020).
    Crossref
    Kayvan Kousha, Mike Thelwall, Google Books, Scopus, Microsoft Academic and Mendeley for impact assessment of doctoral dissertations: A multidisciplinary analysis of the UK, Quantitative Science Studies, 10.1162/qss_a_00042, (1-29), (2020).
    Crossref
    Mike Thelwall, MEASURING SOCIETAL IMPACTS OF RESEARCH WITH ALTMETRICS? COMMON PROBLEMS AND MISTAKES, Journal of Economic Surveys, 10.1111/joes.12381, 0 , 0, (2020).
    Wiley Online Library
    Matthew S. Bickley, Kayvan Kousha, Michael Thelwall, Can the impact of grey literature be assessed? An investigation of UK government publications cited by articles and books, Scientometrics, 10.1007/s11192-020-03628-w, (2020).
    Crossref
    Mingkun Wei, Abdolreza Noroozi Chakoli, Evaluating the relationship between the academic and social impact of open access books based on citation behaviors and social media attention, Scientometrics, 10.1007/s11192-020-03678-0, (2020).
    Crossref
    Qingqing Zhou, Chengzhi Zhang, Evaluating wider impacts of books via fine-grained mining on citation literatures, Scientometrics, 10.1007/s11192-020-03676-2, (2020).
    Crossref
    Kayvan Kousha, Mike Thelwall, Can Google Scholar and Mendeley help to assess the scholarly impacts of dissertations?, Journal of Informetrics, 10.1016/j.joi.2019.02.009, 13 , 2, (467-484), (2019).
    Crossref
    Alesia Zuccala, Nicolas Robinson-García, Reviewing, Indicating, and Counting Books for Modern Research Evaluation Systems, Springer Handbook of Science and Technology Indicators, 10.1007/978-3-030-02511-3_27, (715-728), (2019).
    Crossref
    Yongjun Zhu, Erjia Yan, Silvio Peroni, Chao Che, Nine million book items and eleven million citations: a study of book-based scholarly communication using OpenCitations, Scientometrics, 10.1007/s11192-019-03311-9, (2019).
    Crossref
    Hanting Su, Zhuoya Fan, Chen Cao, Yi Zhang, Shuo Wang, Xiaofeng Meng, ScholarCitation: Chinese Scholar Citation Analysis Based on ScholarSpace in the Field of Computer Science, Frontiers in Big Data, 10.3389/fdata.2019.00041, 2 , (2019).
    Crossref
    Mohammadamin Erfanmanesh, A. Noorhidawati, A. Abrizah, What can Bookmetrix tell us about the impact of Springer Nature’s books, Scientometrics, 10.1007/s11192-019-03198-6, (2019).
    Crossref
    Cristina López-Duarte, Marta M. Vidal-Suárez, Belén González-Díaz, Cross-national distance and international business: an analysis of the most influential recent models, Scientometrics, 10.1007/s11192-019-03203-y, (2019).
    Crossref
    Alesia A. Zuccala, Elea Giménez-Toledo, Ginevra Peruginelli, Scholarly books and their evaluation context in the social sciences and humanities, Aslib Journal of Information Management, 10.1108/AJIM-11-2018-271, 70 , 6, (586-591), (2018).
    Crossref
    Wen-Lung Shiau, Chang-Ming Yan, Bang-Wen Lin, Exploration into the intellectual structure of mobile information systems, International Journal of Information Management, 10.1016/j.ijinfomgt.2018.10.025, (2018).
    Crossref
    Andrea Bonaccorsi, Peer Review in Social Sciences and Humanities. Addressing the Interpretation of Quality Criteria, The Evaluation of Research in Social Sciences and Humanities, 10.1007/978-3-319-68554-0, (71-101), (2018).
    Crossref
    Alesia Zuccala, Language, Culture and Traversing the Scholarly Evaluation Landscape, The Evaluation of Research in Social Sciences and Humanities, 10.1007/978-3-319-68554-0, (395-411), (2018).
    Crossref
    Kayvan Kousha, Mike Thelwall, Can Microsoft Academic help to assess the citation impact of academic books?, Journal of Informetrics, 10.1016/j.joi.2018.08.003, 12 , 3, (972-984), (2018).
    Crossref
    Bibliography, Becoming Metric-Wise, 10.1016/B978-0-08-102474-4.00023-6, (341-375), (2018).
    Crossref
    Rajesh Piryani, Vedika Gupta, Vivek Kumar Singh, David Pinto, Book impact assessment: A quantitative and text-based exploratory analysis, Journal of Intelligent & Fuzzy Systems, 10.3233/JIFS-169494, 34 , 5, (3101-3110), (2018).
    Crossref
    Raymond Tatalovich, John Frendreis, Winning awards and gaining recognition: An impact analysis of APSA section book prizes, The Social Science Journal, 10.1016/j.soscij.2018.07.006, (2018).
    Crossref
    Daniel Torres-Salinas, Juan Gorraiz, Nicolas Robinson-Garcia, The insoluble problems of books: what does Altmetric.com have to offer?, Aslib Journal of Information Management, 10.1108/AJIM-06-2018-0152, (2018).
    Crossref
    Elea Giménez-Toledo, Jorge Mañana-Rodríguez, Tim C. E. Engels, Raf Guns, Emanuel Kulczycki, Michael Ochsner, Janne Pölönen, Gunnar Sivertsen, Alesia A. Zuccala, Taking scholarly books into account, part II: a comparison of 19 European countries in evaluation and funding, Scientometrics, 10.1007/s11192-018-2956-7, (2018).
    Crossref
    Richard Scruggs, Paul A. McDermott, Xin Qiao, A Nationwide Study of Research Publication Impact of Faculty in U.S. Higher Education Doctoral Programs, Innovative Higher Education, 10.1007/s10755-018-9447-x, (2018).
    Crossref
    Jorge Mannana-Rodriguez, Elea Giménez-Toledo, Specialization and multidisciplinarity of scholarly book publishers: differences between Spanish University Presses and other scholarly publishers, Scientometrics, 10.1007/s11192-017-2563-z, 114 , 1, (19-30), (2017).
    Crossref
    Alicia Cavanaugh, Sébastien Breau, Locating geographies of inequality: publication trends across OECD countries, Regional Studies, 10.1080/00343404.2017.1371292, 52 , 9, (1225-1236), (2017).
    Crossref
    Alberto Martin-Martin, Enrique Orduna-Malea, Anne-Wil Harzing, Emilio Delgado López-Cózar, Can we use Google Scholar to identify highly-cited documents?, Journal of Informetrics, 10.1016/j.joi.2016.11.008, 11 , 1, (152-163), (2017).
    Crossref
    Frederik T. Verleysen, Truyken L. B. Ossenblok, Profiles of monograph authors in the social sciences and humanities: an analysis of productivity, career stage, co-authorship, disciplinary affiliation and gender, based on a regional bibliographic database, Scientometrics, 10.1007/s11192-017-2312-3, 111 , 3, (1673-1686), (2017).
    Crossref
    Gali Halevi, Henk Moed, Judit Bar-Ilan, Suitability of Google Scholar as a source of scientific information and as a source of data for scientific evaluation—Review of the Literature, Journal of Informetrics, 10.1016/j.joi.2017.06.005, 11 , 3, (823-834), (2017).
    Crossref
    Almudena Mangas-Vega, Raquel Goméz-Díaz, José Antonio Cordón-García, undefined, Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality - TEEM 2017, 10.1145/3144826.3145448, (1-6), (2017).
    Crossref
    Daniel Torres-Salinas, Nicolás Robinson-Garcia, Juan Gorraiz, Filling the citation gap: measuring the multidimensional impact of the academic book at institutional level with PlumX, Scientometrics, 10.1007/s11192-017-2539-z, 113 , 3, (1371-1384), (2017).
    Crossref
    Daniel Torres-Salinas, Christian Gumpenberger, Juan Gorraiz, PlumX As a Potential Tool to Assess the Macroscopic Multidimensional Impact of Books, Frontiers in Research Metrics and Analytics, 10.3389/frma.2017.00005, 2 , (2017).
    Crossref
    Farshid Danesh, Rahmatollah Fattahi, Mohammad Hossein Dayani, Stratification of Iranian LIS academics in terms of visibility, effectiveness and scientific and professional performance: Research report Part 1, Journal of Librarianship and Information Science, 10.1177/0961000616632866, 49 , 2, (191-198), (2016).
    Crossref
    Björn Hammarfelt, Beyond Coverage: Toward a Bibliometrics for the Humanities, Research Assessment in the Humanities, 10.1007/978-3-319-29016-4, (115-131), (2016).
    Crossref
    Alexander Dilger, Harry Müller, Outputanalyse betriebswirtschaftlicher Fachbereiche – Ein zitationsbasiertes Ranking deutschsprachiger Hochschulen, Nachhaltiges Entscheiden, 10.1007/978-3-658-12506-6, (405-425), (2016).
    Crossref
    Jooo Carvalho Santos, F. Vitorino Martins, Different Perspectives on Internationalization Research: A Bibliometric Review, SSRN Electronic Journal, 10.2139/ssrn.2757439, (2016).
    Crossref
    Elea Giménez-Toledo, Jorge Mañana-Rodríguez, Tim C. E. Engels, Peter Ingwersen, Janne Pölönen, Gunnar Sivertsen, Frederik T. Verleysen, Alesia A. Zuccala, Taking scholarly books into account: current developments in five European countries, Scientometrics, 10.1007/s11192-016-1886-5, 107 , 2, (685-699), (2016).
    Crossref
    Qingqing Zhou, Chengzhi Zhang, Star X. Zhao, Bikun Chen, Measuring book impact based on the multi-granularity online review mining, Scientometrics, 10.1007/s11192-016-1930-5, 107 , 3, (1435-1455), (2016).
    Crossref
    References, Altmetrics for Information Professionals, 10.1016/B978-0-08-100273-5.09986-5, (131-153), (2016).
    Crossref
    Pei-Shan Chi, Differing disciplinary citation concentration patterns of book and journal literature?, Journal of Informetrics, 10.1016/j.joi.2016.05.005, 10 , 3, (814-829), (2016).
    Crossref
    Ad A.M. Prins, Rodrigo Costas, Thed N. van Leeuwen, Paul F. Wouters, Using Google Scholar in research evaluation of humanities and social science programs: A comparison with Web of Science data, Research Evaluation, 10.1093/reseval/rvv049, 25 , 3, (264-270), (2016).
    Crossref
    Ronald Snijder, Revisiting an open access monograph experiment: measuring citations and tweets 5 years later, Scientometrics, 10.1007/s11192-016-2160-6, 109 , 3, (1855-1875), (2016).
    Crossref
    Alesia Zuccala, Roberto Cornacchia, Data matching, integration, and interoperability for a metric assessment of monographs, Scientometrics, 10.1007/s11192-016-1911-8, 108 , 1, (465-484), (2016).
    Crossref
    Kim Holmberg, The Past, Altmetrics for Information Professionals, 10.1016/B978-0-08-100273-5.00001-6, (9-54), (2016).
    Crossref
    Michael Thelwall, Web Indicators for Research Evaluation: A Practical Guide, Synthesis Lectures on Information Concepts, Retrieval, and Services, 10.2200/S00733ED1V01Y201609ICR052, 8 , 4, (i-155), (2016).
    Crossref
    Alberto Martín-Martín, Enrique Orduna-Malea, Juan M. Ayllón, Emilio Delgado López-Cózar, Un panorama académico de dos caras: retrato de los documentos altamente citados en Google Scholar (1950-2013), Revista española de Documentación Científica, 10.3989/redc.2016.4.1405, 39 , 4, (149), (2016).
    Crossref
    Teresa Bolger, Looking Back to Move Forward: Measuring the Impact of Existing Digital Resources Relevant to Irish Archaeology, New Review of Information Networking, 10.1080/13614576.2015.1113059, 20 , 1-2, (16-34), (2015).
    Crossref
    Matt Elbeck, Arne Baruca, A Journal-Neutral Ratio for Marketing Faculty Scholarship Assessment, Marketing Education Review, 10.1080/10528008.2015.1044853, 25 , 3, (193-204), (2015).
    Crossref
    Ximo Granell Zafra, Maria Pinto, Dora Sales, La evaluación de la investigación: criterios de evaluación en Humanidades y el caso de la Traducción e Interpretación, Investigación Bibliotecológica: Archivonomía, Bibliotecología e Información, 10.1016/j.ibbai.2016.02.025, 29 , 66, (57-78), (2015).
    Crossref
    Daniel Torres-Salinas, Nicolás Robinson-Garcia, Evaristo Jiménez-Contreras, Enrique Fuente-Gutiérrez, The BiPublishers ranking: Main results and methodological problems when constructing rankings of academic publishers, Revista española de Documentación Científica, 10.3989/redc.2015.4.1287b, 38 , 4, (e111), (2015).
    Crossref
    Mike Thelwall, Nabeil Maflahi, How important is computing technology for library and information science research?, Library & Information Science Research, 10.1016/j.lisr.2014.09.002, 37 , 1, (42-50), (2015).
    Crossref
    C. Michael Hall, Stephen J. Page, Following the impact factor: Utilitarianism or academic compliance?, Tourism Management, 10.1016/j.tourman.2015.05.013, 51 , (309-312), (2015).
    Crossref
    Alexander Botte, The relevance of the EERQI framework in the light of future perspectives: Enhancing the visibility and detection of European research publications, Assessing Quality in European Educational Research, 10.1007/978-3-658-05969-9, (184-196), (2014).
    Crossref
    Emilio Delgado-López-Cózar, Enrique Orduña-Malea, Evaristo Jiménez-Contreras, Rafael Ruiz-Pérez, H Index Scholar : el índice h de los profesores de las universidades públicas españolas en humanidades y ciencias sociales, El Profesional de la Informacion, 10.3145/epi.2014.ene.11, 23 , 1, (87-94), (2014).
    Crossref
    Kevin W. Boyack, Richard Klavans, Including cited non-source items in a large-scale map of science: What difference does it make?, Journal of Informetrics, 10.1016/j.joi.2014.04.001, 8 , 3, (569-580), (2014).
    Crossref
    Tina M. Neville, Deborah B. Henry, Evaluating Scholarly Book Publishers—A Case Study in the Field of Journalism, The Journal of Academic Librarianship, 10.1016/j.acalib.2014.05.005, 40 , 3-4, (379-387), (2014).
    Crossref
    Daniel Torres-Salinas, Nicolas Robinson-Garcia, Juan Miguel Campanario, Emilio Delgado López-Cózar, Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index, Online Information Review, 10.1108/OIR-10-2012-0169, 38 , 1, (24-42), (2014).
    Crossref
    Daniel Torres-Salinas, Nicolás Robinson-García, Álvaro Cabezas-Clavijo, Evaristo Jiménez-Contreras, Analyzing the citation characteristics of books: edited books, book series and publisher types in the book citation index, Scientometrics, 10.1007/s11192-013-1168-4, 98 , 3, (2113-2127), (2013).
    Crossref
    Loet Leydesdorff, Filippo Radicchi, Lutz Bornmann, Claudio Castellano, Wouter Nooy, Field‐normalized impact factors (IFs): A comparison of rescaling and fractionally counted IFs, Journal of the American Society for Information Science and Technology, 10.1002/asi.22911, 64 , 11, (2299-2309), (2013).
    Wiley Online Library
    Ehsan Mohammadi, Mike Thelwall, Assessing non-standard article impact using F1000 labels, Scientometrics, 10.1007/s11192-013-0993-9, 97 , 2, (383-395), (2013).
    Crossref
    Daniel Torres-Salinas, Rosa Rodríguez-Sánchez, Nicolás Robinson-García, J. Fdez-Valdivia, J.A. García, Mapping citation patterns of book chapters in the Book Citation Index, Journal of Informetrics, 10.1016/j.joi.2013.01.004, 7 , 2, (412-424), (2013).
    Crossref
    Loet Leydesdorff, Ulrike Felt, “Books” and “book chapters” in the book citation index (BKCI) and science citation index (SCI, SoSCI, A&HCI), Proceedings of the American Society for Information Science and Technology, 10.1002/meet.14504901027, 49 , 1, (1-7), (2013).
    Crossref
    Miquel Porta, Jan P. Vandenbroucke, John P. A. Ioannidis, Sergio Sanz, Esteve Fernandez, Raj Bhopal, Alfredo Morabia, Cesar Victora, Tomàs Lopez, Trends in Citations to Books on Epidemiological and Statistical Methods in the Biomedical Literature, PLoS ONE, 10.1371/journal.pone.0061837, 8 , 5, (e61837), (2013).
    Crossref
    Wen-Lung Shiau, Yogesh K. Dwivedi, Citation and co-citation analysis to identify core and emerging knowledge in electronic commerce research, Scientometrics, 10.1007/s11192-012-0807-5, 94 , 3, (1317-1337), (2012).
    Crossref
    Anne-Wil Harzing, A preliminary test of Google Scholar as a source for citation data: a longitudinal study of Nobel prize winners, Scientometrics, 10.1007/s11192-012-0777-7, 94 , 3, (1057-1075), (2012).
    Crossref

Volume 62 , Issue 11

November 2011

Pages 2147-2164

    Figures
    References
    Related
    Information

Close Figure Viewer
Return to Figure
Previous Figure Next Figure
Caption
Download PDF
back
Association for Information Science &amp; Technology Logo
© 2020 Association for Information Science & Technology

Join ASIS&T  |  Association News  |  Events Calendar  |  Career Center  |  iConnect  |  Webinars
© 2020 Association for Information Science & Technology
Additional links
About Wiley Online Library

    Privacy Policy
    Terms of Use
    Cookies
    Accessibility

Help & Support

    Contact Us

Opportunities

    Subscription Agents
    Advertisers & Corporate Partners

Connect with Wiley

    The Wiley Network
    Wiley Press Room

Copyright © 1999-2020 John Wiley & Sons, Inc . All rights reserved
Wiley Home Page
Log in to Wiley Online Library
Email or Customer ID
Password
Forgot password?
Log in with your ASIS&T membership
Go to Asist.org
NEW USER > INSTITUTIONAL LOGIN >
Change Password
Old Password
New Password
Too Short Weak Medium Strong Very Strong Too Long
Password Changed Successfully

Your password has been changed
Create a new account
Email or Customer ID
Returning user
Forgot your password?

Enter your email address below.
Email or Customer ID

Please check your email for instructions on resetting your password. If you do not receive an email within 10 minutes, your email address may not be registered, and you may need to create a new Wiley Online Library account.
Request Username

Can't sign in? Forgot your username?

Enter your email address below and we will send you your username
Email or Customer ID
Close

If the address matches an existing account you will receive an email with instructions to retrieve your username
Close crossmark popup

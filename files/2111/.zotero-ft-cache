nature.com

    Sitemap
    Register
    Login

Nature
International weekly journal of science
Show search Menu
Search Go Advanced search

    Home
    News & Comment
    Research
    Careers & Jobs
    Current Issue
    Archive
    Audio & Video
    For Authors

    Archive
    Volume 503
    Issue 7476
    Comment
    Article

Nature | Comment
Sharing

    Print
    Email
    Share/bookmark
        Facebook
        Twitter
        Delicious
        Digg
        Google+
        LinkedIn
        StumbleUpon
        Reddit

Policy: Twenty tips for interpreting scientific claims

    William J. Sutherland 1 ,
    David Spiegelhalter 2
    & Mark Burgman 3

20 November 2013

This list will help non-scientists to interrogate advisers and to grasp the limitations of evidence, say William J. Sutherland, David Spiegelhalter and Mark A. Burgman.
Article tools

    PDF
    Rights & Permissions

Subject terms:

    Government
    Policy
    Politics
    Society

Science and policy have collided on contentious issues such as bee declines, nuclear power and the role of badgers in bovine tuberculosis.

BADGER: ANDY ROUSE/NATURE PICTURE LIBRARY; NUCLEAR PLANT: MICHAEL KOHAUPT/FLICKR/GETTY; BEE: MICHAEL DURHAM/MINDEN/FLPA

Calls for the closer integration of science in political decision-making have been commonplace for decades. However, there are serious problems in the application of science to policy — from energy to health and environment to education.

    Google supercomputers tackle giant drug-interaction data crunch
    Twenty tips to help you interpret scientific claims
    How oral sex can cause throat cancer by transmitting HPV

One suggestion to improve matters is to encourage more scientists to get involved in politics. Although laudable, it is unrealistic to expect substantially increased political involvement from scientists. Another proposal is to expand the role of chief scientific advisers 1 , increasing their number, availability and participation in political processes. Neither approach deals with the core problem of scientific ignorance among many who vote in parliaments.

Perhaps we could teach science to politicians? It is an attractive idea, but which busy politician has sufficient time? In practice, policy-makers almost never read scientific papers or books. The research relevant to the topic of the day — for example, mitochondrial replacement, bovine tuberculosis or nuclear-waste disposal — is interpreted for them by advisers or external advocates. And there is rarely, if ever, a beautifully designed double-blind, randomized, replicated, controlled experiment with a large sample size and unambiguous conclusion that tackles the exact policy issue.

In this context, we suggest that the immediate priority is to improve policy-makers' understanding of the imperfect nature of science. The essential skills are to be able to intelligently interrogate experts and advisers, and to understand the quality, limitations and biases of evidence. We term these interpretive scientific skills. These skills are more accessible than those required to understand the fundamental science itself, and can form part of the broad skill set of most politicians.

To this end, we suggest 20 concepts that should be part of the education of civil servants, politicians, policy advisers and journalists — and anyone else who may have to interact with science or scientists. Politicians with a healthy scepticism of scientific advocates might simply prefer to arm themselves with this critical set of knowledge.

We are not so naive as to believe that improved policy decisions will automatically follow. We are fully aware that scientific judgement itself is value-laden, and that bias and context are integral to how data are collected and interpreted. What we offer is a simple list of ideas that could help decision-makers to parse how evidence can contribute to a decision, and potentially to avoid undue influence by those with vested interests. The harder part — the social acceptability of different policies — remains in the hands of politicians and the broader political process.

Of course, others will have slightly different lists. Our point is that a wider understanding of these 20 concepts by society would be a marked step forward.

Differences and chance cause variation. The real world varies unpredictably. Science is mostly about discovering what causes the patterns we see. Why is it hotter this decade than last? Why are there more birds in some areas than others? There are many explanations for such trends, so the main challenge of research is teasing apart the importance of the process of interest (for example, the effect of climate change on bird populations) from the innumerable other sources of variation (from widespread changes, such as agricultural intensification and spread of invasive species, to local-scale processes, such as the chance events that determine births and deaths).
Related stories

    Research: A standard for policy-relevant science
    Nature focus: Science and politics
    Nature focus: Meetings that changed the world

No measurement is exact. Practically all measurements have some error. If the measurement process were repeated, one might record a different result. In some cases, the measurement error might be large compared with real differences. Thus, if you are told that the economy grew by 0.13% last month, there is a moderate chance that it may actually have shrunk. Results should be presented with a precision that is appropriate for the associated error, to avoid implying an unjustified degree of accuracy.

Bias is rife. Experimental design or measuring devices may produce atypical results in a given direction. For example, determining voting behaviour by asking people on the street, at home or through the Internet will sample different proportions of the population, and all may give different results. Because studies that report 'statistically significant' results are more likely to be written up and published, the scientific literature tends to give an exaggerated picture of the magnitude of problems or the effectiveness of solutions. An experiment might be biased by expectations: participants provided with a treatment might assume that they will experience a difference and so might behave differently or report an effect. Researchers collecting the results can be influenced by knowing who received treatment. The ideal experiment is double-blind: neither the participants nor those collecting the data know who received what. This might be straightforward in drug trials, but it is impossible for many social studies. Confirmation bias arises when scientists find evidence for a favoured theory and then become insufficiently critical of their own results, or cease searching for contrary evidence.

Bigger is usually better for sample size. The average taken from a large number of observations will usually be more informative than the average taken from a smaller number of observations. That is, as we accumulate evidence, our knowledge improves. This is especially important when studies are clouded by substantial amounts of natural variation and measurement error. Thus, the effectiveness of a drug treatment will vary naturally between subjects. Its average efficacy can be more reliably and accurately estimated from a trial with tens of thousands of participants than from one with hundreds.

Correlation does not imply causation. It is tempting to assume that one pattern causes another. However, the correlation might be coincidental, or it might be a result of both patterns being caused by a third factor — a 'confounding' or 'lurking' variable. For example, ecologists at one time believed that poisonous algae were killing fish in estuaries; it turned out that the algae grew where fish died. The algae did not cause the deaths 2 .

Regression to the mean can mislead. Extreme patterns in data are likely to be, at least in part, anomalies attributable to chance or error. The next count is likely to be less extreme. For example, if speed cameras are placed where there has been a spate of accidents, any reduction in the accident rate cannot be attributed to the camera; a reduction would probably have happened anyway.

Extrapolating beyond the data is risky. Patterns found within a given range do not necessarily apply outside that range. Thus, it is very difficult to predict the response of ecological systems to climate change, when the rate of change is faster than has been experienced in the evolutionary history of existing species, and when the weather extremes may be entirely new.

Beware the base-rate fallacy. The ability of an imperfect test to identify a condition depends upon the likelihood of that condition occurring (the base rate). For example, a person might have a blood test that is '99% accurate' for a rare disease and test positive, yet they might be unlikely to have the disease. If 10,001 people have the test, of whom just one has the disease, that person will almost certainly have a positive test, but so too will a further 100 people (1%) even though they do not have the disease. This type of calculation is valuable when considering any screening procedure, say for terrorists at airports.

DAWID RYSKI

Controls are important. A control group is dealt with in exactly the same way as the experimental group, except that the treatment is not applied. Without a control, it is difficult to determine whether a given treatment really had an effect. The control helps researchers to be reasonably sure that there are no confounding variables affecting the results. Sometimes people in trials report positive outcomes because of the context or the person providing the treatment, or even the colour of a tablet 3 . This underlies the importance of comparing outcomes with a control, such as a tablet without the active ingredient (a placebo).

Randomization avoids bias. Experiments should, wherever possible, allocate individuals or groups to interventions randomly. Comparing the educational achievement of children whose parents adopt a health programme with that of children of parents who do not is likely to suffer from bias (for example, better-educated families might be more likely to join the programme). A well-designed experiment would randomly select some parents to receive the programme while others do not.

Seek replication, not pseudoreplication. Results consistent across many studies, replicated on independent populations, are more likely to be solid. The results of several such experiments may be combined in a systematic review or a meta-analysis to provide an overarching view of the topic with potentially much greater statistical power than any of the individual studies. Applying an intervention to several individuals in a group, say to a class of children, might be misleading because the children will have many features in common other than the intervention. The researchers might make the mistake of 'pseudoreplication' if they generalize from these children to a wider population that does not share the same commonalities. Pseudoreplication leads to unwarranted faith in the results. Pseudoreplication of studies on the abundance of cod in the Grand Banks in Newfoundland, Canada, for example, contributed to the collapse of what was once the largest cod fishery in the world 4 .

Scientists are human. Scientists have a vested interest in promoting their work, often for status and further research funding, although sometimes for direct financial gain. This can lead to selective reporting of results and occasionally, exaggeration. Peer review is not infallible: journal editors might favour positive findings and newsworthiness. Multiple, independent sources of evidence and replication are much more convincing.

Significance is significant. Expressed as P , statistical significance is a measure of how likely a result is to occur by chance. Thus P = 0.01 means there is a 1-in-100 probability that what looks like an effect of the treatment could have occurred randomly, and in truth there was no effect at all. Typically, scientists report results as significant when the P -value of the test is less than 0.05 (1 in 20).

Separate no effect from non-significance. The lack of a statistically significant result (say a P -value > 0.05) does not mean that there was no underlying effect: it means that no effect was detected. A small study may not have the power to detect a real difference. For example, tests of cotton and potato crops that were genetically modified to produce a toxin to protect them from damaging insects suggested that there were no adverse effects on beneficial insects such as pollinators. Yet none of the experiments had large enough sample sizes to detect impacts on beneficial species had there been any 5 .

Effect size matters. Small responses are less likely to be detected. A study with many replicates might result in a statistically significant result but have a small effect size (and so, perhaps, be unimportant). The importance of an effect size is a biological, physical or social question, and not a statistical one. In the 1990s, the editor of the US journal Epidemiology asked authors to stop using statistical significance in submitted manuscripts because authors were routinely misinterpreting the meaning of significance tests, resulting in ineffective or misguided recommendations for public-health policy 6 .

Study relevance limits generalizations. The relevance of a study depends on how much the conditions under which it is done resemble the conditions of the issue under consideration. For example, there are limits to the generalizations that one can make from animal or laboratory experiments to humans.

Feelings influence risk perception. Broadly, risk can be thought of as the likelihood of an event occurring in some time frame, multiplied by the consequences should the event occur. People's risk perception is influenced disproportionately by many things, including the rarity of the event, how much control they believe they have, the adverseness of the outcomes, and whether the risk is voluntarily or not. For example, people in the United States underestimate the risks associated with having a handgun at home by 100-fold, and overestimate the risks of living close to a nuclear reactor by 10-fold 7 .

Dependencies change the risks. It is possible to calculate the consequences of individual events, such as an extreme tide, heavy rainfall and key workers being absent. However, if the events are interrelated, (for example a storm causes a high tide, or heavy rain prevents workers from accessing the site) then the probability of their co-occurrence is much higher than might be expected 8 . The assurance by credit-rating agencies that groups of subprime mortgages had an exceedingly low risk of defaulting together was a major element in the 2008 collapse of the credit markets.

Data can be dredged or cherry picked. Evidence can be arranged to support one point of view. To interpret an apparent association between consumption of yoghurt during pregnancy and subsequent asthma in offspring 9 , one would need to know whether the authors set out to test this sole hypothesis, or happened across this finding in a huge data set. By contrast, the evidence for the Higgs boson specifically accounted for how hard researchers had to look for it — the 'look-elsewhere effect'. The question to ask is: 'What am I not being told?'

Extreme measurements may mislead. Any collation of measures (the effectiveness of a given school, say) will show variability owing to differences in innate ability (teacher competence), plus sampling (children might by chance be an atypical sample with complications), plus bias (the school might be in an area where people are unusually unhealthy), plus measurement error (outcomes might be measured in different ways for different schools). However, the resulting variation is typically interpreted only as differences in innate ability, ignoring the other sources. This becomes problematic with statements describing an extreme outcome ('the pass rate doubled') or comparing the magnitude of the extreme with the mean ('the pass rate in school x is three times the national average') or the range ('there is an x -fold difference between the highest- and lowest-performing schools'). League tables, in particular, are rarely reliable summaries of performance.

Journal name:
    Nature
Volume:
    503 , 
Pages:
    335–337
Date published:
    ( 21 November 2013 )
DOI:
    doi :10.1038/503335a

References

    Doubleday, R. & Wilsdon, J. Nature 485 , 301 – 302 ( 2012 ).
        Article
        PubMed
        ISI
        ChemPort
    Show context

    Borsuk, M. E. , Stow, C. A. & Reckhow, K. H. J. Water Res. Plan. Manage. 129 , 271 – 282 ( 2003 ).
        Article
    Show context

    Huskisson, E. C. Br. Med. J. 4 , 196 – 200 ( 1974 )
        Article
        PubMed
        ChemPort
    Show context

    Millar, R. B. & Anderson, M. J. Fish. Res. 70 , 397 – 407 ( 2004 ).
        Article
    Show context

    Marvier, M. Ecol. Appl. 12 , 1119 – 1124 ( 2002 ).
        Article
        ISI
    Show context

    Fidler, F. , Cumming, G. , Burgman, M. , Thomason, N. J. Socio-Economics 33 , 615 – 630 ( 2004 ).
        Article
    Show context

    Fischhoff, B. , Slovic, P. & Lichtenstein, S. Am. Stat. 36 , 240 – 255 ( 1982 ).
        ISI
    Show context

    Billinton, R. & Allan, R. N. Reliability Evaluation of Power Systems (Plenum, 1984).
    Show context

    Maslova, E. , Halldorsson, T. I. , Strøm, M. , Olsen, S. F. J. Nutr. Sci. 1 , e5 ( 2012 ).
        Article
        PubMed
    Show context

Related stories and links
From nature.com

    Research: A standard for policy-relevant science

    11 September 2013
    Nature focus: Science and politics
    Nature focus: Meetings that changed the world

Author information
Affiliations

    William J. Sutherland is professor of conservation biology in the Department of Zoology, University of Cambridge, UK.
    David Spiegelhalter is at the Centre for Mathematical Sciences, University of Cambridge.
    Mark Burgman is at the Centre of Excellence for Biosecurity Risk Analysis, School of Botany, University of Melbourne, Parkville, Australia.

Corresponding author

Correspondence to:

    William J. Sutherland

Author details

    William J. Sutherland

    Contact William J. Sutherland
    Search for this author in
        NPG journals
        PubMed
        Google Scholar
    David Spiegelhalter
    Search for this author in
        NPG journals
        PubMed
        Google Scholar
    Mark Burgman
    Search for this author in
        NPG journals
        PubMed
        Google Scholar

For the best commenting experience, please login or register as a user and agree to our Community Guidelines . You will be re-directed back to this page where you will see comments updating in real-time and have the ability to recommend comments to other users.
Comments
14 comments Subscribe to comments

    Avatar for Edward Powell
    Edward Powell • 2013-11-30 07:19 PM

        To these twenty, I would add two more: 21) The predictive ability of computer simulations is extremely limited, and should only be relied on when substantial and detailed agreement of the simulation results with experimental data is achieved, *and* when a detailed and thorough understanding of the elements of uncertainty in the models and simulations is achieved. A true scientist tries his best to falsify, disprove, or even "break" his models and simulations in any possible way, before relying on the results. John von Neuman said, "With four parameters I can fit an elephant, and with five I can make him wiggle his trunk." Any simulation with *any* free parameters is suspect, and detailed, conclusive, and scientifically justified causal relationships must be well understood for each and every free parameter before a simulation can even begin to be considered reliable. 22) No scientist shall, under any circumstances, refer to a simulation run as an "experiment". Simulations are not experiments, only data taken from actual reality-based experiments can be called real data.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Chris Atkins
    Chris Atkins • 2013-11-28 08:30 PM

        This is an excellent article and a subject close to my heart. It could (and should) be used with secondary (high) school students, particularly (but not exclusively) of science. For that purpose each 'tip' would benefit from a short (one page) article which describes the issue in more detail with one or two examples to bring it to life.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Lee De Cola
    Lee De Cola • 2013-11-27 05:48 PM

        this is a great list, but to be widely read it needs to be shortened...i've tried and come up with 3 groupings: psychology (of scientists), experiments, and statistics - could we get along with 3 or 4 tips in each group?

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for James Scanlan
    James Scanlan • 2013-11-27 05:29 PM

        The is a useful list in many respects. But it overlooks what may be the most serious problem in the interpretation of data on differences between outcome rates. Virtually all efforts to interpret data ongroup differences in outcome rates, including data on rates of treated and control subjects in clinical trials, are undermined by failure to recognize patterns by which standard measures of differences between outcome rates tend to be systematically affected by the prevalence of an outcome. The most notable of these patterns is that by which the rarer an outcome the greater tends to be the relative difference in experiencing it and the smaller tends to be the relative difference in avoiding it. By way of example pertinent to item 20 on the list, lowering test cutoffs (or generally improving test performance) tends to reduce relative differences between rates at which two groups pass a test while increasing relative differences rates at which they fail the test. Among other examples, reducing poverty tends to increase relative differences in poverty rates while reducing relative differences in rates of avoiding poverty; improving health tends to increase relative differences in mortality and other adverse health outcomes while reducing relative differences in survival and other favorable health outcomes; improving healthcare tends to reduce relative differences in receipt of appropriate care while increasing relative differences in rates of failing to receive appropriate care; reducing adverse lending outcomes or school suspension rates tends to increase relative difference in experiencing those outcomes while reducing relative differences in avoiding those outcomes. It is not possible to interpret data on group differences or advise policy makers on such issues without knowing these things. Yet very few people interpreting data on differences in outcome rates know that it is even possible for the two relative differences to change in opposite directions, much less that they tend to do so systematically. A number of references explaining these and related patterns and their implications in varied contexts are listed below.

        Reference 1 provides a fairly succinct explanation of the pattern of relative differences described above and does so in the course of explaining that, as a result of the failure to understand the pattern, the US government encourages lenders and schools to take actions that make it more likely that the government will sue them for discrimination. Reference 8 explains some of the clinical implications of the failure to understand the pattern, explaining as well that the rate ratio mentioned as a measure of effect in item 20 of the list is not merely a flawed measure of effect, but an illogical one. References 9 and 10 are discussions of the above-described pattern of relative differences by other persons. The latter reference observes that governments that ignore the pattern “run the risk of guaranteeing failure, largely for conceptual and methodological reasons rather than social welfare reasons.” The observation was focused on the meeting of health inequalities reduction goals cast in relative terms. But, as reflected in references 1 through 8, failure to understand the pattern, and relative patterns by which measures tend to be affected by the prevalence of an outcome, undermines society’s understanding of a great many things.

        1. “Misunderstanding of Statistics Leads to Misguided Law Enforcement Policies” (Amstat News, Dec. 2012):
        http://magazine.amstat.org/blog/2012/12/01/misguided-law-enforcement/

        2. “Measuring Health and Healthcare Disparities,” Federal Committee on Statistical Methodology 2013 Research Conference, Nov. 5-7, 2013.
        http://jpscanlan.com/images/2013_Fed_Comm_on_Stat_Meth_paper.pdf
        http://jpscanlan.com/images/2013_FCSM_Presentation.ppt

        3. “The Mismeasure of Discrimination,” Faculty Workshop at the University of Kansas School of Law, Sept. 20, 2013:
        http://jpscanlan.com/images/Univ_Kansas_School_of_Law_Faculty_Workshop_Paper.pdf
        http://jpscanlan.com/images/University_of_Kansas_School_of_Law_Workshop.pdf

        4. “The Mismeasure of Group Differences in the Law and the Social and Medical Sciences,” Applied Statistics Workshop at the Institute for Quantitative Social Science at Harvard University, Oct. 17, 2012:
        http://jpscanlan.com/images/Harvard_Applied_Statistic_Workshop.ppt

        5. “Can We Actually Measure Health Disparities?” (Chance, Spring 2006):
        http://www.jpscanlan.com/images/Can_We_Actually_Measure_Health_Disparities.pdf

        6. “The Misinterpretation of Health Inequalities in the United Kingdom,” British Society for Populations Studies Conference 2006, Sept. 18-20, 2006:
        http://www.jpscanlan.com/images/BSPS_2006_Complete_Paper.pdf

        7. “Race and Mortality” (Society, Jan./Feb. 2000):
        http://www.jpscanlan.com/images/Race_and_Mortality.pdf

        8. Subgroup Effects subpage of the Scanlan’s Rule page of jpscanlan.com
        http://www.jpscanlan.com/scanlansrule/subgroupeffects.html

        9. Lambert PJ, Subramanian S. Disparities in Socio-Economic outcomes: Some positive propositions and their normative implications. Society for the Study of Economic Inequality Working Paper Series, ECINEQ WP 2012 – 281:
        http://www.ecineq.org/milano/WP/ECINEQ2012-281.pdf

        10. Bauld L, Day P, Judge K. Off target: A critical review of setting goals for reducing health inequalities in the United Kingdom. Int J Health Serv 2008;38(3):439-454:
        http://baywood.metapress.com/app/home/contribution.asp?referrer=parent&backto=issue,4,11;journal,7,157;linkingpublicationresults,1:300313,1

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Benjamin Allen
    Benjamin Allen • 2013-11-26 11:04 PM

        An excellent article - well done! Translation from science into policy is made even more difficult when scientists themselves produce and promote unreliable science. Sutherland et al's 20 tips are well worth the read. For an ecological example that epitomises many of the 20 points they raise, see 'http://www.sciencedirect.com/science/article/pii/S0006320712005022'.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Douglas Duncan
    Douglas Duncan • 2013-11-26 06:38 PM

        Almost all those politicians have been to university, but these aspects of science are rarely discussed with non-science majors there. They need to be! I have been doing so for about 6 years, and have published the effects it has on students. Remember, these people have probably never been to a scientific meeting; they have little idea how scientists interact, and they sure don't learn that in a classroom, because most of us behave entirely differently there than we do with colleagues. The way scientists interact, how they arrive at consensus, what causes them to doubt (even the basic distinction between peer reviewed and non-peer reviewed publication) are things they have never encountered (according to our interviews of non-science majors). Anyone is welcome to use my curriculum, which is here (along with reference to the published results): http://casa.colorado.edu/~dduncan/pseudoscience/

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Bart Penders
    Bart Penders • 2013-11-26 09:44 AM

        The public credibility of science and the trust non-scientists place in scientific claims and the institution of science is not primarily derived from the methodological quality of scientific inquiry. The list above is worthwhile to determine that quality (or any threats to it), but not to help establish the public credibility (or trust). Let me quote Harvard historian of science Steven Shapin who wrote, eloquently: "When King Lear decided to take early retirement, he announced his intention to divide up the kingdom among his three daughters, each to get a share proportioned to the genuine love she bore him. Each is asked to testify to her love. For Goneril and Regan that presents no problem, and both use the oily art of rhetoric to good effect. Cordelia, however, trusts to the authenticity of her love and says nothing more than the simple truth. For Lear this will not do. Truth is her dower but credibility has she none. Cordelia, we should understand, was a modernist methodologist. The credibility and the validity of a proposition ought to be one and the same. Truth shines by its own light. And those claims that need lubrication by the oily art thereby give warning that they are not true". (Shapin 1995). Cordelia acts from the conviction that contained in her claims is the truth and that it can be accessed by those who hear it. The credibility of her claim is evident to herself - but only to herself. For a scientist to convince non-scientists, persuading them to see the truth as (s)he does, requires - next to that truth - a construction of credibility. Scientists have very strict and shared rules to establish the credibility of their claims, these include notions of peer review, citations, transparant methodologies and much more. They are part of the well-known academic credibilising strategy in which the work scientists do to make their data, arguments, theories or claims become stable and uncontested truths (i.e. to achieve absolute credibility). "The same work enables them to conduct further research by strengthening their reputation and attracting new funding. The process of gathering credibility is never-ending and cyclical: it drives the credibility cycle. Important in this process are moments of conversion, or translation. Latour and Woolgar show how time, money and effort are translated into data; data is translated into arguments, which are subsequently written down in publications. Peer scientists will read or cite them, resulting in recognition for their claim or argument. This recognition can be mobilised to support new funding requests. The new funding, translated into new personnel, projects or machineries, will produce new data, continuing the cycle. Every translation that takes place, contributes to the credibility of a given scientific claim, which extends to the researchers, laboratory or institute making the claim. If the ‘scientific wheel of credibility’ continues to turn, it will uphold or heighten the credibility of claims and claimants. Academic conventions, such as scientific publications, citations and peer review are part of this credibility cycle. The credibility cycle as a strategy to construct credibility is powerful only within the scientific community: outside the university walls, peer review and citation cultures are relatively unknown and cannot accrue similar credibility." (Penders 2013). Non-scientists (as well as scientists, when it comes to non-scientific claims) use other criteria, standards and strategies to evaluate the credibility of claims. What matters most, in real life, are the shape or form (visually, or rhetorically) of a message, the medium or location of a claim (e.g. quality newspaper vs. casual water-cooler chat), the degree in which a claim specifically addresses a problem (or question) the audience identifies with, the source of the claim (a trusted colleague or friend vs. an unknown online forum) and the relationship between the claim and the audience. The route a scientific claim takes before it reaches any given policy maker, may matter more than its content. Who whispers it into her ear may matter more than its content and the dominant political climate may matter more than the content of the claim. The authors wisely acknowledge that they "are not so naive as to believe that improved policy decisions will automatically follow". The reason for that is that any scientific claim wildly varies in its credibility over time, location and much more. That credibility is not derived from its content, but it is the reason why a claim is takes seriously, acted upon, insert into or excluded from policy.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Gavin Cawley
    Gavin Cawley • 2013-11-25 11:35 AM

        For policy related issues, the tip I would add would be: Always interpret a scientific finding in the way that provides the least support for ones current position . This helps to guard against the sort of confirmation biases etc. to which we are all susceptible. If the finding genuinely supports your position, it will still do so under its least favourable interpretation, and adopting this interpretation will demonstrate your self-skepticism.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Anand Ramanathan
    Anand Ramanathan • 2013-11-21 07:11 PM

        The article is good about warning policy makers about new scientific claims. However, it is a little worrisome that the same logic can be applied to any science to reject established results such as the warming over the last few decades or the effect of smoking. I would add one more point. Science typically does not change overnight and results that have been tested and confirmed over years (or decades) are unlikely to be wrong. Any new result that seeks to overthrow some bit of well-established science should be backed by a much higher standard of proof, such as independent replications, or carefully controlled studies with large sample sizes.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Gavin Cawley
    Gavin Cawley • 2013-11-25 11:42 AM

        In the case of the warming of the last few decades, I would say this is addressed by the points "Separate no effect from non-significance" and "Data can be dredged or cherry picked" as such arguments are normally based on the lack of a statistically significant warming trend since some cherry picked start (e.g. 1998 El-Nino) and ignore the statistical power of the test. A test for the existence of a change in the underlying rate of warming would likely also give a non-significant result.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Christopher Tong
    Christopher Tong • 2013-11-21 06:37 AM

        A few other tips might include an awareness of Simpson's Paradox, the Curse of Dimensionality, and the ease with which data models can overfit one data set and fail to generalize to others. The distinction between exploratory and confirmatory findings should also be emphasized. Exploratory research seeks to generate new hypotheses, and confirmatory research aims to evaluate pre-specified hypotheses. Sometimes a single study will have both confirmatory and exploratory findings, as follows. The confirmatory claims are pre-specified before the data was collected, and their consistency with the subsequent data are then evaluated. Exploratory findings are tentative findings of other, unanticipated patterns in the same data. The epistemological status of both kinds of findings must be kept distinct. Finally, data analysis methods must be chosen for fitness for purpose, and not by pattern matching, as is too often the case.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Brad Louis
    Brad Louis • 2013-11-21 03:40 AM

        Great list. Can I suggest a few more? We don't know everything (Science is never settled. Any scientist can be wrong.) - that's why we keep investing in scientific research. We will know more in another hundred years, by which time a lot of what we think we know now will be proven to be nonsense. Proper experiments produce the most robust scientific evidence - good experimental design can eliminate many of the problems mentioned in earlier tips. Observational studies and computer models are not a patch on proper experiments. Science funding supports a process not an outcome - well done science doesn't pretend to know the outcome of research, diversity of research perspectives is a good thing.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Ian Campbell
    Ian Campbell • 2013-11-20 11:00 PM

        This article provides a valuable summary of issues that need to be appreciated in interpreting science. However, in the environmental decision making arena, the science is never sole factor to be considered - there is always a value judgement as well. Science tells us the consequences of greenhouse gas emissions, or the likelihood that logging a particular area of forest will cause the extinction of a particular species, but then we have to make a value judgement. Do I value the profits I make from my power station more than the contribution it makes to global warming? Do I value the timber products I can get by logging this forest more than the possum that will become extinct? As value judgements these decisions should be informed by the science, but are made by politicians who are expected to reflect the values of their society. Politicians are always averse to making value judgement calls that will be unpopular with a substantial proportion of their community, or an influential group within the community, so they often try to deflect that responsibility. Two common deflection techniques are appeals to (often dodgy) short term economics (it would be too expensive, cost too many jobs), and raising doubts about the science either by citing poor science, or by suggesting that there is a lack of scientific consensus on the issue. It is difficult to tell whether a true lack of scientific understanding, as addressed in the article, or a misunderstanding of convenience to deflect criticism is a greater problem.

        Share to Twitter Share to Facebook Share link to this comment
    Avatar for Fred Sachs
    Fred Sachs • 2013-11-21 05:16 PM

        The article does emphasize some important issues but at the core is how one might evaluate the contributions of all these factors. The probability of success in a project cannot be calculated and a legislator would have to assign a weighting factor to each of the 20 criteria, let alone the societal consequences, to decide on the level of support and it can't be done. Real answers like this depend on numbers and we don't have the information to generate the numbers. I suspect that we are going to have to put up with intuition on the part of the policy makers. I have a few specific issues. One is that 5% significance is an entirely subjective choice by the researcher and has no significance if the distribution is not Gaussian. That should be the first test. Even at 5% there is a 1 in 20 chance that the hypothesis is wrong. Does this data represent that time? Especially in clinical data one should know the probability of getting better compared to the probability of getting worse. You should not do a one sided test to ask if subjects got better and ignore the times they got worse. The outcome should also be weighted by the payoff; a number not easily evaluated. Did the positive patients feel better while the negative ones died? Finally, for the cause and effect issue I like to bring up something seen a billion times a day: the roosters crowed and then the sun came up; clearly the roosters caused sunrise.

        Share to Twitter Share to Facebook Share link to this comment

See other News & Comment articles from Nature

    Humans are becoming more carnivorous
    Discovery of organ explains koalas' super-bass notes
    Fearful memories haunt mouse descendants
    Remnants suggest comet ISON still going
    Study linking GM maize to rat tumours is retracted
    Newlyweds' gut feelings predict marital happiness
    Stealth camera takes pictures virtually in the dark
    Downsized black hole is much brighter than it should be
    Security: Expand nuclear forensics
    Seven days: 22–28 November 2013
    Football fever could be a dose of dengue
    Personalized cancer treatments suffer setback
    Technology: Sharing data in materials science
    China battles army of invaders
    Research ethics: 3 ways to blow the whistle
    Immunology: The pursuit of happiness
    Nations fight back on ivory
    Listening to Africa’s elephants
    China aims for the Moon
    LHC plans for open data future

    Scroll left
    Scroll right

Top Story
newlyweds
Newlyweds' gut feelings predict marital happiness

Couples' visceral reactions to each other fortell future satisfaction.
Social Media Box - AML

    E-alert
    RSS
    Facebook
    Twitter

Close
Top Content - Article Page
Recent

    Humans are becoming more carnivorous

    Nature 02 December 2013
    Discovery of organ explains koalas' super-bass notes

    Nature 02 December 2013
    Fearful memories haunt mouse descendants

    Nature 01 December 2013
    Remnants suggest comet ISON still going

    Nature 29 November 2013
    Study linking GM maize to rat tumours is retracted

    Nature 28 November 2013

Read

    HPV: Sex, cancer and a virus

    Nature 20 Nov 2013
    Mystery humans spiced up ancients’ sex lives

    Nature 19 Nov 2013
    Policy: Twenty tips for interpreting scientific claims

    Nature 20 Nov 2013
    Study linking GM maize to rat tumours is retracted

    Nature 28 Nov 2013
    Graphene: The quest for supercarbon

    Nature 20 Nov 2013

View all
Commented

    Study linking GM maize to rat tumours is retracted

    Nature 28 Nov 2013 26 comments
    Mystery species spiced up ancients’ rampant sex lives

    Nature 19 Nov 2013 22 comments
    Who is the best scientist of them all?

    Nature 06 Nov 2013 16 comments
    Policy: Twenty tips for interpreting scientific claims

    Nature 20 Nov 2013 14 comments
    Reproducibility: The risks of the replication drive

    Nature 20 Nov 2013 14 comments

View all
Emailed

    Chilly lab mice skew cancer studies

    Nature 18 Nov 2013
    Immunology: The pursuit of happiness

    Nature 27 Nov 2013
    Policy: Twenty tips for interpreting scientific claims

    Nature 20 Nov 2013
    Football fever could be a dose of dengue

    Nature 27 Nov 2013
    Newlyweds' gut feelings predict marital happiness

    Nature 28 Nov 2013

View all
MSC webcast. Drug induced liver injury.
Science jobs from nature jobs

    Gastroenterologist

    Greenville Hospital System
    Regulatory Systems Biology - Faculty Position

    California Institute of Technology
    Vice Chancellor for Research

    Louisiana State University
    Director

    National Institute of Advanced Studies (NIAS)
    Faculty Positions in Environmental Science and Engineering

    Harvard University

    Post a Free Job
    More Science Jobs

    Nature
    ISSN : 0028-0836
    EISSN : 1476-4687

    About NPG
    Contact NPG
    Accessibility statement
    Help

    Privacy policy
    Use of cookies
    Legal notice
    Terms

    Nature jobs
    Nature Asia
    Nature Education
    RSS web feeds

    About Nature
    Contact Nature
    About the Editors
    Nature awards

Search Go

© 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. partner of AGORA, HINARI, OARE, INASP, CrossRef and COUNTER

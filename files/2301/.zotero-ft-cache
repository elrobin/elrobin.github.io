Skip to Main Content
Wiley Online Library
Log in / Register
Log In
E-Mail Address
Password

Forgotten Password?
Remember Me

    Register
    Institutional Login

Advertisement

    Home >
    Computer Science >
    General & Introductory Computer Science >
    Journal of the American Society for Information Science and Technology >
    Vol 64 Issue 11 >
    Abstract

JOURNAL TOOLS

    Get New Content Alerts
    Get RSS feed
    Save to My Profile

JOURNAL MENU

    Journal Home

FIND ISSUES

    All Issues
    Virtual Issues

FIND ARTICLES

    Early View
    Most Accessed
    Most Cited

GET ACCESS

    Subscribe / Renew

FOR CONTRIBUTORS

    OnlineOpen
    Author Guidelines
    Submit an Article

ABOUT THIS JOURNAL

    Overview
    Editorial Board
    Permissions
    Advertise
    Contact

SPECIAL FEATURES

    ASIS&T Digital Library
    Articles in the Advances in Information Science
    Bulletin of the American Society for Information Science and Technology
    Proceedings of the American Society for Information Science and Technology
    Annual Review of Information Science and Technology
    Virtual Issue on Knowledge Management
    Virtual Issue on Bibliometrics
    Jobs

Advertisement Advertisement

RESEARCH ARTICLE
You have full text access to this content
Do universities or research institutions with a specific subject profile have an advantage or a disadvantage in institutional rankings?
A Latent Class Analysis With Data From the SCImago Ranking

    Lutz Bornmann 1 ,
    Felix de Moya Anegón 2 and
    Rüdiger Mutz 3

Article first published online: 21 AUG 2013

DOI: 10.1002/asi.22923

© 2013 ASIS&T

Issue
Journal of the American Society for Information Science and Technology
Journal of the American Society for Information Science and Technology

Volume 64 , Issue 11 , pages 2310–2316 , November 2013

Additional Information (Show All)

How to Cite Author Information Publication History
How to Cite

Bornmann, L., de Moya Anegón, F. and Mutz, R. (2013), Do universities or research institutions with a specific subject profile have an advantage or a disadvantage in institutional rankings?. J. Am. Soc. Inf. Sci., 64: 2310–2316. doi: 10.1002/asi.22923
Author Information

    1

    Division for Science and Innovation Studies, Max Planck Society, Administrative Headquarters, Munich, Germany
    2

    CSIC/CCHS/IPP, Albasanz 26, Spain
    3

    Professorship for Social Psychology and Research on Higher Education, Zurich, Switzerland

Email: Lutz Bornmann (bornmann@gv.mpg.de), Felix de Moya Anegón (felix.moya@scimago.es), Rüdiger Mutz (ruediger.mutz@gess.ethz.ch)
Publication History

    Issue published online: 15 OCT 2013
    Article first published online: 21 AUG 2013
    Manuscript Accepted: 26 DEC 2012
    Manuscript Revised: 21 DEC 2012
    Manuscript Received: 12 NOV 2012

SEARCH
Search Scope
Search String

    Advanced >
    Saved Searches >

SEARCH BY CITATION
Volume:
Issue:
Page:
ARTICLE TOOLS

    Get PDF (80K)
    Save to My Profile
    E-mail Link to this Article
    Export Citation for this Article
    Get Citation Alerts
    Request Permissions

More Sharing Services Share | Share on citeulike Share on facebook Share on delicious Share on www.mendeley.com Share on twitter

    Abstract
    Article
    References
    Cited By

Enhanced Article (HTML) Get PDF (80K)
Keywords:

    bibliometrics

Abstract

    Top of page
    Abstract
    Introduction
    Methods
    Results
    Discussion
    References

Using data compiled for the SCImago Institutions Ranking, we look at whether the subject area type an institution (university or research-focused institution) belongs to (in terms of the fields researched) has an influence on its ranking position. We used latent class analysis to categorize institutions based on their publications in certain subject areas. Even though this categorization does not relate directly to scientific performance, our results show that it exercises an important influence on the outcome of a performance measurement: Certain subject area types of institutions have an advantage in the ranking positions when compared with others. This advantage manifests itself not only when performance is measured with an indicator that is not field-normalized but also for indicators that are field-normalized.

Introduction

    Top of page
    Abstract
    Introduction
    Methods
    Results
    Discussion
    References

With the increase of competition between universities (and research-focused institutions) and the dramatic growth of the higher education market, the end of the 1990s saw institutions worldwide starting to undergo evaluation and ranking (Shin & Toutkoushian, 2011 ). An overview of the (most important) rankings is provided in Chen and Liao ( 2012 ), van Vught and Ziegele ( 2012 ), and Waltman et al. ( 2012 ). For example, the Leiden Ranking 2011/2012 ( http://www.leidenranking.com/ ) measures the scientific performance of 500 major universities worldwide using a set of sophisticated bibliometric indicators (e.g., the universities' proportion of highly cited publications). The Academic Ranking of World Universities (ARWU) 2012 ( http://www.arwu.org/ ) uses six indicators to rank world universities, including the number of alumni and staff winning Nobel prizes and Field Medals, the number of highly cited researchers selected by Thomson Reuters, the number of articles published in Nature and Science , the number of articles indexed in the Science Citation Index–Expanded and the Social Sciences Citation Index , and per capita performance with respect to the size of an institution. More than 1,000 universities are actually ranked by ARWU every year, and the best 500 are published on the web ( http://www.arwu.org/aboutARWU.jsp ).

According to Shin and Toutkoushian ( 2011 ), “ranking universities is a challenging task because each institution has its own particular mission, focus and can offer different academic programs. Institutions can also differ in size and have varying amounts of resources at their disposal. In addition, each country has its own history and higher education system which can impact the structure of their colleges and universities and how they compare to others. It is therefore very difficult to rank entire universities, especially across national borders, according to the single criterion of ranking indicators” (p. 2). Federkeil, Vught, and Westerheijden ( 2012 ) specify the problem of diversity within institutions, writing that world university league tables “are primarily rankings of whole institutions, i.e. they compare whole institutions across all fields, ignoring internal variance in qualities of specific academic fields within an institution” (p. 41).

We want to use the figures in the SCImago Institutions Ranking (SIR; www.scimagoir.com/ ) to investigate how far the profile of subject areas researched at an institution (in the following, institution refers to both universities and research-focused institutions) can really impact on its position in a ranking (García, Rodríguez-Sánchez, Fdez-Valdivia, Robinson-García, & Torres-Salinas, 2012 ). SCImago publishes an annual World Report in which publication and citation data for the institutions are presented after a process of a very conscientious disambiguation of the names of the institutions (the World Report is similar to the Leiden Ranking). Using data compiled for the SIR, we look at whether the type an institution belongs to (in terms of the subject areas researched) has an influence on its ranking position. Subject area types can be seen as largely independent of specific scientific performance indicators.

The first step of the study is a latent class analysis (LCA) to classify types of institutions on the basis of their subject area profiles (i.e., in accordance with their representation in certain subject areas). In the second step, we investigate the proportion of the classes identified in step 1 that are represented in the 10% best institutions. The 10% best institutions are selected irrespective of the subject areas using the following three indicators: (a) proportion of their highly cited papers, (b) proportion of their papers in the best journals, and (c) proportion of their papers with international collaboration. If the field focus of an institution does not influence the rankings by the three indicators, it might be expected that the proportion of institutions in the individual classes will be approximately 10%. As the two indicators “proportion of papers in the best journals” and “proportion of highly cited papers” are field-normalized, we do not expect the subject area to have an impact on these two indicators.
Methods

    Top of page
    Abstract
    Introduction
    Methods
    Results
    Discussion
    References

Data Set

The study is based on Scopus (Elsevier) data that were prepared for the SIR. The study covers the publication years 2005–2009. The document types included are articles, reviews, and conference papers. SCImago uses full counting to assign publications to the institutions (for an overview on different counting methods, see Gauffriau, Larsen, Maye, Roulin-Perriard, & von Ins, 2007 ): If a publication has more than one coauthor (working at different institutions), each unique collaborating institution receives one full credit. Across all subject areas, the data set consists of 4,530 institutions. Only those institutions in a subject area for which there are at least 100 publications (in this subject area during the 5 years given above) are included in the study. If institutions with fewer publications had been included, there would have been a risk that an institution could not be assigned reliably to a subject area. It is only possible to conclude that an institution is genuinely active in a subject area if it has a reasonable number of publications in that area.
Variables and Indicators

A scientific field scheme to which the publications of a university have been assigned is required for the calculation of the LCA. Similar to the various Thomson Reuters citation indexes (used in the Web of Science), subject areas are defined in Scopus in terms of journal sets. There are 305 “specific subject areas” (e.g., “Biochemistry”) organized into 26 “subject areas” (e.g., “Biochemistry, Genetics & Molecular Biology”), plus a “general subject area” containing multidisciplinary journals such as Nature or Science (Bornmann, de Moya-Anegón & Leydesdorff, 2010 ). In this study, the 26 areas in Scopus are used to classify the publications into subjects. As this study only include institutions with a minimum of 100 publications, the publications are distributed over just 18 subject areas, which are grouped into 13 subject areas and an additional “Other subject areas” category:

    Agricultural and Biological Sciences
    Biochemistry, Genetics and Molecular Biology
    Chemistry/Chemical Engineering
    Maths/Computer Science
    Environmental Science/Earth and Planetary Sciences
    Engineering
    Immunology and Microbiology
    Materials Science
    Medicine
    Neuroscience
    Pharmacology, Toxicology and Pharmaceutics
    Physics and Astronomy
    Psychology/Social Science

The category “Other subject areas” covers areas for which there were very few publications for all of the institutions. Examples are “Arts and Humanities,” “Business, Management, and Accounting,” “Decision Sciences,” “Dentistry,” “Energy,” and “Nursing.” The categorical variable “subject area” was dummy-coded: For each subject area a binary or dummy variable (14 dummy variables for 14 subject areas) was created with 1 if an institution had any publications in the subject area in question and 0 if not. The sequence of 0s and 1s across the 14 dummy variables represents the specific research area profile of an institution (one row of the data matrix for each institution). The dummy variable for the remainder category (“Other subject areas”) was excluded from LCA for two reasons, redundancy and inhomogeneity: If an institution has no publications in any of the 13 subject areas, it must necessarily have publications in the remainder category. Furthermore, the remainder category covers very different subject areas (see previously), which does not contribute much to the specific profile of an institution.

The following three SCImago Reseach Group ( 2011 ) indicators were used in this study to rank the institutions and to determine the top 10% institutions:

    The excellence rate (ER) indicates which percentage of an institution's output belongs to the 10% of the most cited papers in their scientific fields. According to Tijssen, Visser, and van Leeuwen ( 2002 ) and Tijssen and van Leeuwen ( 2006 ), excellent or highly cited papers are those among the 10% most-cited papers in a field (papers in or greater than the 90 th percentile, designated class 10% papers in the following). According to Waltman et al. ( 2012 , p. 2425) the excellence rate (here designated PP top 10% ) is “the most important impact indicator” in the Leiden Ranking.
    The high-quality publications indicator (Q1) is the ratio of publications which an institution publishes in the most influential journals worldwide. The most influential journals are defined as the first quartile (25%) in their subject areas as sorted by the SCImago Journal Rank (SJR) indicator (Gonzalez-Pereira, Guerrero-Bote, & Moya-Anegon, 2010 ).
    International collaboration (IC) shows an institution's output ratio which has been produced in collaboration with foreign institutions. The ratios are computed by analyzing the institution's output. A paper produced in international collaboration has more than one country address. Note that IC is not as desirable in some countries as in others. Although collaboration with prestigious institutions is as a rule desirable, in the case of the United States, the most prestigious institutions are in the country itself, making it less desirable and influential.

Statistical Procedures
LCA

Following McCutcheon ( 1987 ), LCA can be defined as a statistical procedure that makes it possible to extract clusters of units (“latent classes”) that are homogenous with respect to the observed categorical indicators, where in our case the institutions provide for the units and the dummy-coded subject areas provide for the indicators (Mutz & Daniel, 2012 ). Similar to factors in a factor analysis, latent classes are extracted in such a way that the correlations between the observed indicators should vanish completely (“local statistical independence”). When compared with the requirements of ordinary cluster analysis, LCA directly models the data and does not require any specific similarity measure (e.g., measure of similarity). In addition to the lower number of procedure decisions required in LCA, LCA uses maximum-likelihood, a more efficient estimation algorithm than the estimation procedures used by ordinary cluster analysis. This also makes it possible to compare models and identify the number of latent classes with so-called information criteria (Bozdagon, 1993 ; Lukociene, Varriale, & Vermunt, 2010 ). One of them, the Bayes information criterion (BIC), penalizes the log-likelihood with increasing model complexity.

In empirical applications, it is frequently not possible to assume local stochastic independence. In our study, it can be expected that similarities between subject areas (such as “Engineering” and “Material Science”) will result in a strong association between two scientific subject areas that cannot be fully captured by the latent classes and that gives rise to high model residuals in the end. One solution to this problem is to add one or more direct effects from one variable to the other that account for the residual correlations between the observed variables (causing a violation of the local stochastic independence assumption). Another solution is to include a continuous latent variable, a so-called C-factor, comparable to a factor analysis. Not only residual correlations among the variables can be explained with this C-factor, but additional quantitative differences between the institutions can also be assessed (Mutz & Daniel, 2007 ). In other words, institutions differ not only in their qualitative profile of subject areas but also in the quantitative number of publications in different subject areas.

The LCA procedure as implemented in the software program Latent GOLD 4.5 was used for the statistical analysis of the data (Vermunt & Magidson, 2005 ).
Two-dimensional χ 2 test

A two-dimensional χ 2 test for r × c tables (χ 2 test of independence) is used to determine the strength of the relationship between latent class and ranking position of an institution (dichotomized as 0 = institution is not in the top 10% of the ranking and 1 = institution is in the top 10% of the ranking ). The test is used to evaluate the general hypothesis that the two variables (institution and ranking position) are independent of one another (Sheskin, 2007 ). As a measure of association for the r × c table, Cramér's V was calculated in addition (Sheskin, 2007 , pp. 660–661). Following Cohen ( 1988 ), Cramér's V is assessed to determine whether the degree of association between both variables is small, medium, or large.
Results

    Top of page
    Abstract
    Introduction
    Methods
    Results
    Discussion
    References

LCA

In the first step of an LCA, the number of latent classes or types of institutions must be determined (Table  1 ). Twelve different models with one C-factor differing in the number of latent classes were compared. Because low values in the BIC information criteria indicate an optimal model, we have decided on model 9 with nine latent classes due to the lowest BIC value. Strong associations between the pair “Agricultural and Biological Sciences” and “Environmental Science/Earth and Planetary Sciences” and between the pair “Engineering” and “Materials Science” make it necessary to include two direct effects between the two pairs of variables in the LCA in order to avoid high residuals. In the end, no residual value exceeds the threshold criterion of 3.84, which confirms that the model fits the data adequately. For one degree of freedom, bivariate residuals larger than χ 2 (1) = 3.84 indicate statistical significance at the .05 probability level (Vermunt & Magidson, 2005 ). Eventually, the observed probabilities are optimally predicted by the chosen latent class model (conditional probabilities, class size, C-factor).
Table 1.  Model comparison to determine the number of latent classes (latent class model with 1 C-factor) Model 	Number of clusters 	Log-likelihood 	Number of parameters 	Bayes information criterion

    Note . Final model in boldface.

 1 	1 	−25,747.8031 	28 	51,731.3235
 2 	2 	−23,089.5059 	42 	46,532.5879
 3 	3 	−22,373.3122 	56 	45,218.0591
 4 	4 	−21,895.4829 	70 	44,380.2591
 5 	5 	−21,620.771 	84 	43,948.6941
 6 	6 	−21,427.3619 	98 	43,679.7345
 7 	7 	−21,333.1771 	112 	43,609.2237
 8 	8 	−21,269.0172 	126 	43,598.7625
  9 	9 	− 21,173.2846 	140 	43,525.156
10 	10 	−21,155.5504 	154 	43,607.5462
11 	11 	−21,093.7976 	168 	43,601.8993
12 	12 	−21,024.9616 	182 	43,582.0861

The conditional probabilities (Table  2 ) are particularly interesting for the interpretation of the results of an LCA. These are the probabilities that an institution belonging to a certain latent class or type of institutions publishes in this subject area (here: at least 100 publications). These conditional probabilities can be used to describe the latent classes: The higher the value, the more the subject area is an indicator for the type of institution (figures > .30 are in boldface). For latent class 1, for instance, the conditional probability of institutions to publish (more than 100 publications) in “Medicine” is 1.0. Therefore, latent class 1 can be identified as a type of institution with “Medicine” as the predominant subject area. Given their conditional probabilities, the nine latent classes can be described in accordance with their subject area profile as follows: LC 1, Medicine; LC 2, All subject areas; LC 3, Natural Sciences (physics, chemistry, materials science); LC 4, Agriculture and Biological Sciences; LC 5, Engineering; LC 6, Engineering and Natural Sciences; LC 7, Several subject areas with a focus on medicine and social sciences; LC 8, Environmental Sciences, Earth and Planetary Sciences; and LC 9, Life Sciences (biochemistry, genetics, microbiology, chemistry, medicine). For each class, examples of institutions that have been assigned to the class on the basis of their subject area profile are given in Table  3 .
Table 2.  Conditional probabilities of a subject area being represented in an institution of a certain latent class (LC model with nine latent classes). For instance, “Medicine” is represented in institutions of LC1 with the probability of 1.0 Subject area 	Latent class
LC 1 	LC 2 	LC 3 	LC 4 	LC 5 	LC 6 	LC 7 	LC 8 	LC 9

    Note . Figures (conditional probabilities) > .30 in boldface.

Agricultural and Biological Sciences 	0.04 	0.96 	0.22 	1.00 	0.18 	0.43 	0.53 	0.17 	0.11
Biochemistry, Genetics and Molecular Biology 	0.39 	0.98 	0.33 	0.34 	0.13 	0.60 	0.50 	0.15 	1.00
Chemistry/Chemical Engineering 	0.00 	0.99 	0.63 	0.12 	0.12 	0.89 	0.25 	0.20 	0.50
Maths/Computer Science 	0.00 	0.99 	0.19 	0.01 	0.53 	0.91 	0.47 	0.00 	0.00
Environmental Sciences/Earth and Planetary Sciences 	0.01 	0.95 	0.15 	0.30 	0.15 	0.53 	0.45 	0.76 	0.00
Engineering 	0.01 	0.99 	0.39 	0.03 	0.76 	1.00 	0.49 	0.19 	0.13
Immunology and Microbiology 	0.17 	0.72 	0.06 	0.19 	0.03 	0.13 	0.18 	0.18 	0.25
Materials Science 	0.00 	0.94 	0.61 	0.01 	0.10 	0.95 	0.05 	0.05 	0.03
Medicine 	1.00 	0.99 	0.16 	0.32 	0.15 	0.30 	0.78 	0.14 	0.50
Neuroscience 	0.08 	0.62 	0.02 	0.02 	0.02 	0.00 	0.17 	0.04 	0.12
Pharmacology, Toxicology and Pharmaceutics 	0.05 	0.63 	0.07 	0.05 	0.01 	0.11 	0.16 	0.15 	0.50
Physics and Astronomy 	0.01 	0.99 	0.68 	0.03 	0.38 	1.00 	0.32 	0.22 	0.07
Psychology/Social Science 	0.03 	0.82 	0.00 	0.04 	0.00 	0.08 	0.98 	0.00 	0.01
Other areas (not included in LCA) 	0.04 	0.77 	0.12 	0.13 	0.10 	0.25 	0.49 	0.13 	0.08
Latent class size 	0.32 	0.14 	0.12 	0.11 	0.09 	0.09 	0.05 	0.04 	0.04
Table 3.  Distribution of institutions over the latent classes Latent class 	Examples of institutions 	Number of institutions in a class 	Proportion of institutions in the data set
LC 1 Medicine 	

University Hospital Antwerpen,

University Hospitals of Cleveland,

Texas Heart Institute
	1,453 	32.1
LC 2 All subject areas 	

University of Melbourne,

Catholic University of Leuven,

University of California, Berkeley
	631 	13.9
LC 3 Natural Sciences (physics, chemistry, materials science) 	

European Atomic Energy Community,

St. Petersburg State Polytechnic University,

Tokyo City University
	556 	12.3
LC 4 Agricultural and Biological Sciences 	

Agriculture and Agri-Food Canada,

Beijing Forestry University,

University of Agricultural Sciences, Bangalore
	503 	11.1
LC 5 Engineering 	

École des Mines de Nantes,

Free University of Bozen-Bolzano,

Moscow Institute of Physics and Technology
	414 	9.1
LC 6 Engineering and Natural Sciences 	

Ecole Polytechnique de Montreal,

Universidad Carlos III de Madrid,

Universität Stuttgart
	413 	9.1
LC 7 Several subject areas with a focus on medicine and social sciences 	

University of North Dakota,

University of Westminster,

Université de Lausanne
	205 	4.5
LC 8 Environmental Sciences/Earth and Planetary Sciences 	

Norwegian Institute for Water Research,

Dublin Institute For Advanced Studies,

Berkeley Geochronology Center
	196 	4.3
LC 9 Life Sciences (biochemistry, genetics, microbiology, chemistry, medicine) 	

Institut Pasteur de Paris,

Kobe Pharmaceutical University,

Boston Biomedical Research Institute
	159 	3.5
Total 	  	4,530 	100

As the distribution of institutions over the classes shows (Table  2 , last row), the classes and proportions of the classes in the data set, respectively, vary greatly. Institutions with a focus on medicine ( n = 1,453) make up the largest proportion at around a third. They are followed by institutions that offer all subject areas ( n = 631) and institutions in the natural sciences (physics, chemistry, materials science; n = 556), each at around 12%. There are fewer institutions focusing on life sciences ( n = 159, or 3.5%).
Proportion of Institutions in a Class Among the 10% Best Institutions Ranked by Three Different Indicators

Table  4 shows the proportion of institutions in a latent class among the 10% best institutions in which the institutions are ranked (a) by the ER, (b) by the Q1, and (c) by the IC. Where the ranking by ER is concerned, the table shows that 17.3% of the institutions assigned to the latent class “Medicine” are among the 10% best institutions. This means that with 7.3 percentage points above the expected value of 10%, this class is represented disproportionately among the 10% best institutions. In contrast, only 5.1% of the institutions which cover all the subject areas are represented among the 10% best institutions. These institutions are therefore underrepresented among the 10% best institutions. Institutions in the area of life sciences have the highest percentage: At 17.8%, there are slightly more of them among the 10% best institutions than there are institutions in the latent class “Medicine.” As the results of the χ 2 test in the notes for the table show, the difference between the classes in terms of their inclusion in the 10% best institutions is statistically significant, but the effect size is only small to medium (Cramér's V = .20).
Table 4.  Proportion of the 10% best institutions of all 4,530 institutions for the indicators “Excellence rate” (ER), “High-quality publications” (Q1), and “International collaboration” (IC), separately for each latent class. Sample reading: 17.3% of the institutions in the latent class “Medicine” belong to the 10% best institutions worldwide for ER; 10% would be expected   	Proportion of institutions among the 10% best institutions in the ranking [%]
Latent class 	Excellence rate (ER) a 	High-quality publications (Q1) b 	International collaboration (IC) c

    Note. a χ 2 (8) = 174.2 p  < .05, Cramér's V = .20 (small to medium effect size); b χ 2 (8) = 266.0 p  < .05, Cramér's V = .24 (small to medium effect size); c χ 2 (8) = 345.6 p  < .05, Cramér's V = .28 (medium effect size).

Medicine 	17.3 	15.8 	5.3
All subject areas 	5.1 	2.2 	3.5
Natural Sciences (physics, chemistry, materials science) 	6.4 	8.2 	14.2
Agricultural and Biological Sciences 	7.9 	11.0 	21.8
Engineering 	6.7 	3.8 	12.9
Engineering and Natural Sciences 	1.1 	0.4 	3.5
Several subject areas with a focus on medicine and social sciences 	5.8 	3.2 	3.2
Environmental Sciences/Earth and Planetary Sciences 	13.0 	30.9 	38.9
Life Sciences (biochemistry, genetics, microbiology, chemistry, medicine) 	17.8 	19.5 	12.4
Expected value 	10.0 	10.0 	10.0

There are slightly bigger differences in the percentages if the institutions are ranked by Q1 and IC rather than ER, as shown in Table  4 . As the results for Q1 reveal, Cramér's V (Cramér's V = .24) is slightly higher than for the results ranked by ER. The result of the χ 2 is also statistically significant. Around a third (30.9%) of the institutions in the “Environmental Sciences/Earth and Planetary Sciences” class are among the 10% best institutions; there are hardly any institutions in the “Engineering and Natural Sciences” class in the best 10% (0.4%). The percentages for the institutions in “Medicine” (15.8%) and “Life Sciences” (19.5%) are significantly above the expected value of 10%, as they are for the institutions in the “Environmental Sciences/Earth and Planetary Sciences.”

The largest percentage differences between the latent classes are shown in Table  4 in the ranking by IC: The differences between the latent classes are statistically significant and the effect size is medium (Cramér's V = .28). It is very likely that this result reflects the fact that the ER and Q1 indicators are normalised for subject area, but IC is not. More than a third of the institutions in the “Environmental Sciences/Earth and Planetary Sciences” (38.9%) are among the 10% best institutions; with the institutions in the “Agricultural and Biological Sciences,” this is more than a fifth of the institutions (21.8%). The latent classes “Engineering and Natural Sciences,” “All subject areas” (both 3.5%), and “Several subject areas with a focus on medicine and social sciences” (3.2%) are not highly represented among the 10% best institutions.
Discussion

    Top of page
    Abstract
    Introduction
    Methods
    Results
    Discussion
    References

In this study, we have used the LCA to categorize institutions covered in the SCImago ranking (SCImago Reseach Group, 2012 ) based on their publications in certain subject areas. Even though this categorization is not directly related to scientific performance, our results show that it exercises significant influence on the outcome of a performance measurement: Certain subject-specific types of institutions have an advantage in the ranking positions when compared with others. This advantage manifests itself not only when performance is measured with one indicator (IC) that is not field-normalized but also with the indicators that are field-normalized (ER and Q1). These results are consistent with the results of analyses that have already been carried out by the SCImago group. In the SIR World Report 2012 (SCImago Reseach Group, 2012 ), it is noted that highly specialized institutions in health research have a particular advantage in the measurement of citation impact with a field-normalized indicator. Furthermore, the recently published study of van Raan (2012) using data collected for the Leiden Ranking 2011–2012 shows that fewer citations than expected are characteristic for universities of technology and universities with large engineering departments; the opposite (more citations than expected) are typical for universities with large medical schools.

The International Ranking Expert Group ( 2006 ) published a set of principles of quality and good practice in higher education rankings. One principle is as follows: “ Recognize the diversity of institutions and take the different missions and goals of institutions into account . Quality measures for research-oriented institutions, for example, are quite different from those that are appropriate for institutions that provide broad access to underserved communities.” Our results support the necessity of these principles. Even if indicators are used which imply a normalization of publications from the various institutions, we can assume that the different types of institutions influence the ranking outcomes. In our view, it is therefore necessary to take account of the various types in a ranking (only like should be compared with like) even if normalized impact indicators are used. The LCA's statistical procedure could therefore be used by the ranking suppliers to classify institutions into types and only to compare institutions within these types.

In this study, we have used the SCImago Institutions Ranking as an example of a worldwide ranking. Our analysis could have been carried out with data from many other rankings (such as the Leiden Ranking 2011/2012). In future studies it would be interesting to explore whether the definition of the subject areas and the threshold of 100 papers could have an effect on the results (in this study, only those institutions in a subject area were included for which there are at least 100 publications). For example, a threshold of 100 papers is probably easily reached by a research group in medicine but not by authors in mathematics due to field-specific differences in publication behavior (Bornmann & Daniel, 2008 ). Also, these differences can occur within the subject areas, for example, high-energy physics versus theoretical physics. Would the results be different with a different subject classification system? Furthermore, it would be interesting to include in future studies not only indicators for measuring research performance but also for activities in education and teaching.
References

    Top of page
    Abstract
    Introduction
    Methods
    Results
    Discussion
    References

    Bornmann, L. , & Daniel, H.-D. ( 2008 ). What do citation counts measure? A review of studies on citing behavior . Journal of Documentation , 64 ( 1 ), 45 – 80 . doi: 10.1108/00220410810844150
        CrossRef ,
        Web of Science® Times Cited: 144
    Bornmann, L. , de Moya-Anegón, F. , & Leydesdorff, L. ( 2010 ). Do scientific advancements lean on the shoulders of giants? A bibliometric investigation of the Ortega hypothesis . PLoS ONE , 5 ( 10 ), e11344 .
        CrossRef ,
        CAS ,
        Web of Science® Times Cited: 15 ,
        ADS
    Bozdagon, H. ( 1993 ). Choosing the number of component clusters in the mixture-model using a new informational complexity criterion of the inverse-fisher information matrix . In O. Opitz , B. Lausen , & R. Klar (Eds.), Information and classification (pp. 218 – 234 ). Heidelberg : Springer.
    Chen, K.-H. , & Liao, P.-Y . ( 2012 ). A comparative study on world university rankings: a bibliometric survey . Scientometrics , 92 ( 1 ), 89 – 103 . doi: 10.1007/s11192-012-0724-7
        CrossRef ,
        Web of Science® Times Cited: 1
    Cohen, J. ( 1988 ). Statistical power analysis for the behavioral sciences ( 2nd ed .). Hillsdale, NJ : Lawrence Erlbaum.
    Federkeil, G. , Vught, F.A. , & Westerheijden, D.F. ( 2012 ). An evaluation and critique of current rankings . In F.A. van Vught & F. Ziegele (Eds.), Multidimensional ranking: The design and development of U-Multirank (Vol. 37 , pp. 39 – 70 ). Amsterdam : Springer.
        CrossRef
    García, J.A. , Rodríguez-Sánchez, R. , Fdez-Valdivia, J. , Robinson-García, N. , & Torres-Salinas, D. ( 2012 ). Mapping academic institutions according to their journal publication profile: Spanish universities as a case study . Journal of the American Society for Information Science and Technology , 53 ( 11 ), 2328 – 2340 . doi: 10.1002/asi.22735
    Direct Link:
        Abstract
        Full Article (HTML)
        PDF(2506K)
        References
        Web of Science®
    Gauffriau, M. , Larsen, P.O. , Maye, I. , Roulin-Perriard, A., & von Ins, M. ( 2007 ). Publication, cooperation and productivity measures in scientific research . Scientometrics , 73 ( 2 ), 175 – 214 . doi: 10.1007/s11192-007-1800-2
        CrossRef ,
        Web of Science® Times Cited: 21
    Gonzalez-Pereira, B. , Guerrero-Bote, V.P. , & Moya-Anegon, F. ( 2010 ). A new approach to the metric of journals' scientific prestige: The SJR indicator . Journal of Informetrics , 4 ( 3 ), 379 – 391 . doi: 10.1016/j.joi.2010.03.002
        CrossRef ,
        Web of Science® Times Cited: 39
    International Ranking Expert Group (IREG) . ( 2006 ). Berlin principles on ranking of higher education institutions . Berlin : Author.
    Lukociene, O. , Varriale, R. , & Vermunt, J.K. ( 2010 ). The simultaneous decision about the number of lower- and higher-level classes in multilevel latent class analysis . Sociological Methodology , 40 , 247 – 283 .
    Direct Link:
        Abstract
        Full Article (HTML)
        PDF(219K)
        References
        Web of Science® Times Cited: 5
    McCutcheon, A.L. ( 1987 ). Latent class analysis . London : Sage.
    Mutz, R. , & Daniel, H.D. ( 2007 ). Development of a ranking procedure by mixed Rasch model and multilevel analysis—Psychology as an example . Diagnostica , 53 ( 1 ), 3 – 16 . doi: 10.1026/0012-1924.53.1.3
        CrossRef ,
        Web of Science® Times Cited: 6
    Mutz, R. , & Daniel, H.-D. ( 2012 ). University and student segmentation: Multilevel latent class analysis of students' attitudes toward research methods and statistics . British Journal of Educational Psychology . doi: 10.1111/j.2044-8279.2011.02062.x
    SCImago Reseach Group . ( 2011 ). SIR World Report 2011 . Granada, Spain : University of Granada.
    SCImago Reseach Group . ( 2012 ). SIR World Report 2012 . Granada, Spain : University of Granada .
    Sheskin, D. ( 2007 ). Handbook of parametric and nonparametric statistical procedures ( 4th ed .). Boca Raton, FL: Chapman & Hall/CRC .
    Shin, J.C. , & Toutkoushian, R.K. ( 2011 ). The past, present, and future of university rankings . In J.C. Shin , R.K. Toutkoushian , & U. Teichler (Eds.), University rankings: Theoretical basis, methodology and impacts on global higher education (Vol. 3 , pp. 1 – 16 ). Dordrecht, The Netherlands : Springer.
        Web of Science® Times Cited: 2
    Tijssen, R. , & van Leeuwen, T. ( 2006 ). Centres of research excellence and science indicators. Can “excellence” be captured in numbers? In W. Glänzel (Ed.), Ninth International Conference on Science and Technology Indicators (pp. 146 – 147 ). Leuven, Belgium : Katholieke Universiteit Leuven.
    Tijssen, R. , Visser, M., & van Leeuwen, T. ( 2002 ). Benchmarking international scientific excellence: are highly cited research papers an appropriate frame of reference? Scientometrics , 54 ( 3 ), 381 – 397 .
        CrossRef ,
        Web of Science® Times Cited: 56
    van Raan, A.F.J. ( 2012 ). Universities scale like cities . Retrieved from http://arxiv.org/abs/1211.5124
    van Vught, F.A. , & Ziegele, F. (Eds.). ( 2012 ). Multidimensional ranking: The design and development of U-Multirank . Dordrecht, the Netherlands : Springer.
    Vermunt, J.K. , & Magidson, J. ( 2005 ). Latent GOLD 4.0 user's guide . Belmont, MA : Statistical Innovations Inc.
    Waltman, L. , Calero-Medina, C. , Kosten, J. , Noyons, E.C.M. , Tijssen, R.J.W., van Eck, N.J. , … Wouters, P. ( 2012 ). The Leiden Ranking 2011/2012: Data collection, indicators, and interpretation . Journal of the American Society for Information Science and Technology , 63 ( 12 ), 2419 – 2432 .
    Direct Link:
        Abstract
        Full Article (HTML)
        PDF(564K)
        References
        Web of Science® Times Cited: 8

Enhanced Article (HTML) Get PDF (80K)
More content like this
Find more content:

    like this article

Find more content written by:

    Lutz Bornmann
    Felix de Moya Anegón
    Rüdiger Mutz
    All Authors

    Publications
    Browse by Subject
    Resources

    About Us
    Help
    Contact Us
    Agents
    Advertisers
    Media
    Privacy
    Cookies
    Terms & Conditions
    Site Map

Copyright © 1999-2014 John Wiley & Sons, Inc. All Rights Reserved.

    About Wiley
    Wiley.com
    Wiley Job Network
    Wiley

